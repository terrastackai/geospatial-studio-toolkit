{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to the IBM Geospatial Exploration and Orchestration Studio SDK documentation!","text":""},{"location":"#installation","title":"Installation","text":"<p>First, set up your virtual development environment: <pre><code>python -m venv venv/\n\nsource venv/bin/activate\n</code></pre></p> <p>Install the SDK through pypi:</p> <pre><code>pip install geostudio\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<ul> <li>After installing the SDK, obtain an API key.  Navigate to the UI front page and create an api key.  Click on the <code>Manage your API keys</code> link. This should pop-up a window where you can generate, access and delete your api keys.</li> </ul> <ul> <li> <p>Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:     <pre><code>echo \"GEOSTUDIO_API_KEY=&lt;paste_api_key_here&gt;\" &gt; .geostudio_config_file &amp;&amp; echo \"BASE_STUDIO_UI_URL=&lt;paste_ui_base_url_here&gt;\" &gt;&gt; .geostudio_config_file\n</code></pre></p> </li> <li> <p>Connect to the Geospatial Studio SDK Client</p> <pre><code>from geostudio import Client\n\ngfm_client = Client(geostudio_config_file=\".geostudio_config_file\")\n</code></pre> </li> </ul>"},{"location":"#test-connection","title":"Test connection","text":"<p>Next, you can now use the gfm_client to make requests to the Geospatial studio api sevrice. The example below, uses the SDK client to list available models that we can run inference jobs against.</p> <pre><code>models = gfm_client.list_models()\n</code></pre> <p>For detailed examples on how to use the Geospatial studio SDK, please see the examples page</p>"},{"location":"api/","title":"API reference","text":""},{"location":"base_client/","title":"BaseClient","text":""},{"location":"base_client/#geostudio.backends.base_client","title":"geostudio.backends.base_client","text":""},{"location":"base_client/#geostudio.backends.base_client.BaseClient","title":"BaseClient","text":"<pre><code>BaseClient(\n    api_config: GeoFmSettings = None,\n    session: Session = None,\n    api_token: str = None,\n    api_key: str = None,\n    api_key_file: str = None,\n    geostudio_config_file: str = None,\n    *args,\n    **kwargs\n)\n</code></pre> <p>This class provides methods for making HTTP requests to a Geospatial studio APIs.</p> <p>Parameters:</p> Name Type Description Default <code>api_config</code> <code>GeoFmSettings</code> <p>The configuration settings for the GeoFm API. Defaults to None.</p> <code>None</code> <code>session</code> <code>Session</code> <p>A pre-configured requests session. Defaults to None.</p> <code>None</code> <code>api_token</code> <code>str</code> <p>The API token for authentication. Defaults to None.</p> <code>None</code> <code>api_key</code> <code>str</code> <p>The API key for authentication. Defaults to None.</p> <code>None</code> <code>api_key_file</code> <code>str</code> <p>The path to the file containing the API key. Defaults to None.</p> <code>None</code> <code>geostudio_config_file</code> <code>str</code> <p>The file path to the geostudio config path containing api_key + base_urls.</p> <code>None</code> <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>GeoFMException</code> <p>If no API token, API key, or API key file is provided.</p> <p>Attributes:</p> Name Type Description <code>api_config</code> <code>GeoFmSettings</code> <p>The configuration settings for the GeoFm API.</p> <code>session</code> <code>Session</code> <p>A pre-configured requests session.</p> <code>logger</code> <code>Logger</code> <p>The logger instance for logging messages.</p> Subclassed by: <ul> <li> SDK Client <ul> <li> Client main Client </li> <li> Fine-tuning client Client </li> <li> Inference client Client </li> </ul> </li> </ul> Source code in <code>geostudio/backends/base_client.py</code> <pre><code>def __init__(\n    self,\n    api_config: GeoFmSettings = None,\n    session: requests.Session = None,\n    api_token: str = None,\n    api_key: str = None,\n    api_key_file: str = None,\n    geostudio_config_file: str = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the GeoFmClient with the provided configuration.\n\n    Args:\n        api_config (GeoFmSettings, optional): The configuration settings for the GeoFm API. Defaults to None.\n        session (requests.Session, optional): A pre-configured requests session. Defaults to None.\n        api_token (str, optional): The API token for authentication. Defaults to None.\n        api_key (str, optional): The API key for authentication. Defaults to None.\n        api_key_file (str, optional): The path to the file containing the API key. Defaults to None.\n        geostudio_config_file (str): The file path to the geostudio config path containing api_key + base_urls.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Raises:\n        GeoFMException: If no API token, API key, or API key file is provided.\n\n    Attributes:\n        api_config (GeoFmSettings): The configuration settings for the GeoFm API.\n        session (requests.Session): A pre-configured requests session.\n        logger (logging.Logger): The logger instance for logging messages.\n    \"\"\"\n    self.api_config = api_config or GeoFmSettings()\n\n    if api_token:\n        print(\"Using api_token\")\n        api_token = api_token or GeoFmSettings.GEOFM_API_TOKEN\n        self.session = gfm_session(access_token=api_token)\n    elif api_key:\n        print(\"Using api_key from sdk command\")\n        self.session = gfm_session(api_key=api_key)\n    elif api_key_file:\n        if not os.path.isfile(api_key_file):\n            raise GeoFMException(\"Config file does not exist, Please provide a valid config file.\")\n        print(\"Using api_key from file\")\n        self.session = gfm_session(api_key_file=api_key_file)\n    elif geostudio_config_file:\n        if not os.path.isfile(geostudio_config_file):\n            raise GeoFMException(\"Config file does not exist, Please provide a valid config file.\")\n        print(\"Using api key and base urls from geostudio config file\")\n        geostudio_config_file_values = dotenv_values(geostudio_config_file)\n        settings.BASE_GATEWAY_API_URL = geostudio_config_file_values.get(\"BASE_GATEWAY_API_URL\", \"\")\n        settings.BASE_STUDIO_UI_URL = geostudio_config_file_values.get(\"BASE_STUDIO_UI_URL\", \"\")\n        settings.GEOSTUDIO_API_KEY = geostudio_config_file_values.get(\"GEOSTUDIO_API_KEY\", None)\n        self.session = gfm_session(api_key=settings.GEOSTUDIO_API_KEY)\n    else:\n        raise GeoFMException(\"Missing APIToken. Add `GEOFM_API_TOKEN` to env variables.\")\n\n    # else:\n    #     self.session = session or gfm_session(\n    #         client_id=self.api_config.ISV_CLIENT_ID,\n    #         client_secret=self.api_config.ISV_CLIENT_SECRET,\n    #         well_known_url=self.api_config.ISV_WELL_KNOWN,\n    #         userinfo_endpoint=self.api_config.ISV_USER_ENDPOINT,\n    #     )\n    self.logger = logging.getLogger()\n</code></pre>"},{"location":"base_client/#geostudio.backends.base_client.BaseClient.api_url","title":"api_url  <code>property</code>","text":"<pre><code>api_url\n</code></pre> <p>Process both settings.BASE_GATEWAY_API_URL and settings.BASE_STUDIO_UI_URL 1. For both ensure they end with / 2. For settings.BASE_STUDIO_UI_URL it should only have one / and if there are others clip the rest of    the url after the first / e.g. https//myui.com/ 3. For settings.BASE_GATEWAY_API_URL it should have only one / if it does not have 2 / enclosing poxy    keyword. e.g. /proxy/ e.g. https//myapi.com/ or https//myapi.com/proxy/ otherwise clip the unwanted    parts of the url 4. In the function if settings.BASE_STUDIO_UI_URL is present return the processed UI_URL from 2 above    and only if settings.BASE_GATEWAY_API_URL is present return it after step 3</p>"},{"location":"base_client/#geostudio.backends.base_client.BaseClient.http_get","title":"http_get","text":"<pre><code>http_get(\n    endpoint,\n    owner=None,\n    params=None,\n    output=None,\n    data_field=None,\n)\n</code></pre> <p>Sends an HTTP GET request to the specified endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The endpoint to send the GET request to.</p> required <code>params</code> <code>dict</code> <p>Query parameters to include in the GET request.</p> <code>None</code> <code>output</code> <code>str</code> <p>The desired output format.</p> <code>None</code> <code>data_field</code> <code>str</code> <p>The name of the data field to extract from the response.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>object</code> <p>The response data in the specified format.</p> Source code in <code>geostudio/backends/base_client.py</code> <pre><code>def http_get(self, endpoint, owner=None, params=None, output=None, data_field=None):\n    \"\"\"\n    Sends an HTTP GET request to the specified endpoint.\n\n    Parameters:\n        endpoint (str): The endpoint to send the GET request to.\n        params (dict, optional): Query parameters to include in the GET request.\n        output (str, optional): The desired output format.\n        data_field (str, optional): The name of the data field to extract from the response.\n\n    Returns:\n        object: The response data in the specified format.\n    \"\"\"\n    endpoint = parse.urljoin(self.api_url, endpoint)\n    # print(endpoint)\n    response = self.session.get(endpoint, params=params)\n    _check_auth_error(response=response)\n    if output == \"raw\":\n        return response\n    else:\n        return formated_output(response=response, output_fmt=output, data_field=data_field)\n</code></pre>"},{"location":"base_client/#geostudio.backends.base_client.BaseClient.http_post","title":"http_post","text":"<pre><code>http_post(\n    endpoint,\n    data,\n    files: dict = None,\n    output=None,\n    data_field=None,\n)\n</code></pre> <p>Sends an HTTP POST request to the specified endpoint with the given data.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The API endpoint to send the POST request to.</p> required <code>data</code> <code>dict</code> <p>The data to be sent in the POST request body.</p> required <code>output</code> <code>str</code> <p>The desired output format.</p> <code>None</code> <code>data_field</code> <code>str</code> <p>The key in the response JSON that contains the desired data.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>object</code> <p>The response data in the specified format.</p> Source code in <code>geostudio/backends/base_client.py</code> <pre><code>def http_post(self, endpoint, data, files: dict = None, output=None, data_field=None):\n    \"\"\"\n    Sends an HTTP POST request to the specified endpoint with the given data.\n\n    Parameters:\n        endpoint (str): The API endpoint to send the POST request to.\n        data (dict): The data to be sent in the POST request body.\n        output (str, optional): The desired output format.\n        data_field (str, optional): The key in the response JSON that contains the desired data.\n\n    Returns:\n        object: The response data in the specified format.\n    \"\"\"\n    endpoint = parse.urljoin(self.api_url, endpoint)\n    if files:\n        self.session.headers.pop(\"Content-Type\")\n        response = self.session.post(endpoint, data=data, files=files)\n    else:\n        response = self.session.post(\n            endpoint,\n            data=json.dumps(data),\n        )\n    _check_auth_error(response=response)\n    return formated_output(response=response, output_fmt=output, data_field=data_field)\n</code></pre>"},{"location":"base_client/#geostudio.backends.base_client.BaseClient.http_put_file","title":"http_put_file","text":"<pre><code>http_put_file(\n    endpoint, file_path, output=None, data_field=None\n)\n</code></pre> <p>Uploads a file to a specified endpoint using a PUT request.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The URL endpoint to send the PUT request to.</p> required <code>file_path</code> <code>str</code> <p>The path to the file to be uploaded.</p> required <code>output</code> <code>str</code> <p>The format of the response output. Default is None.</p> <code>None</code> <code>data_field</code> <code>str</code> <p>The field in the response to extract.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The response from the server in the specified output format.</p> Source code in <code>geostudio/backends/base_client.py</code> <pre><code>def http_put_file(self, endpoint, file_path, output=None, data_field=None):\n    \"\"\"\n    Uploads a file to a specified endpoint using a PUT request.\n\n    Parameters:\n        endpoint (str): The URL endpoint to send the PUT request to.\n        file_path (str): The path to the file to be uploaded.\n        output (str, optional): The format of the response output. Default is None.\n        data_field (str, optional): The field in the response to extract.\n\n    Returns:\n        dict: The response from the server in the specified output format.\n    \"\"\"\n    endpoint = parse.urljoin(self.api_url, endpoint)\n\n    with open(file_path, \"rb\") as file:\n        files = {\"file\": (file_path, file, \"application/x-yaml\")}\n        # Update file_headers to upload multipart/form-data\n        # file_headers = {\n        #     \"User-Agent\": \"python-requests/2.32.3\",\n        #     \"Accept-Encoding\": \"gzip, deflate\",\n        #     \"Accept\": \"*/*\",\n        #     \"Connection\": \"keep-alive\",\n        #     # \"Content-Type\": \"application/json\",\n        #     \"X-API-Key\": \"pak-M3B7oM4wtrC8lCxStRewwrewG3eayFI2\",\n        #     \"x-request-origin\": \"python-sdk/\",\n        # }\n\n        if \"Content-Type\" in self.session.headers:\n            self.session.headers.pop(\"Content-Type\")\n        response = self.session.put(url=endpoint, files=files)\n        # After updating, Return the headers to before adjusting them\n        self.session.headers[\"Content-Type\"] = \"application/json\"\n\n        return formated_output(response=response, output_fmt=output, data_field=data_field)\n</code></pre>"},{"location":"base_client/#geostudio.backends.base_client.BaseClient.http_put","title":"http_put","text":"<pre><code>http_put(\n    endpoint,\n    data,\n    output=None,\n    data_field=None,\n    file_path=None,\n)\n</code></pre> <p>Sends an HTTP PUT request to the specified endpoint with the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The URL endpoint to send the PUT request to.</p> required <code>data</code> <code>dict or str</code> <p>The data to be sent in the PUT request body. If a dictionary, it will be converted to JSON.</p> required <code>output</code> <code>str</code> <p>The desired output format.</p> <code>None</code> <code>data_field</code> <code>str</code> <p>The field in the response to extract.</p> <code>None</code> <code>file_path</code> <code>str</code> <p>If the data is a file path, the file will be read and sent as the request body.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>The formatted response data based on the provided output format.</p> Source code in <code>geostudio/backends/base_client.py</code> <pre><code>def http_put(self, endpoint, data, output=None, data_field=None, file_path=None):\n    \"\"\"\n    Sends an HTTP PUT request to the specified endpoint with the provided data.\n\n    Parameters:\n        endpoint (str): The URL endpoint to send the PUT request to.\n        data (dict or str): The data to be sent in the PUT request body. If a dictionary, it will be converted to JSON.\n        output (str, optional): The desired output format.\n        data_field (str, optional): The field in the response to extract.\n        file_path (str, optional): If the data is a file path, the file will be read and sent as the request body.\n\n    Returns:\n        Any: The formatted response data based on the provided output format.\n    \"\"\"\n\n    endpoint = parse.urljoin(self.api_url, endpoint)\n    response = self.session.put(endpoint, data=json.dumps(data))\n    _check_auth_error(response=response)\n    return formated_output(response=response, output_fmt=output, data_field=data_field)\n</code></pre>"},{"location":"base_client/#geostudio.backends.base_client.BaseClient.http_patch","title":"http_patch","text":"<pre><code>http_patch(endpoint, data, output=None, data_field=None)\n</code></pre> <p>Sends a PATCH request to the specified endpoint with the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The URL endpoint to send the PATCH request to.</p> required <code>data</code> <code>dict</code> <p>The data to be sent in the body of the PATCH request.</p> required <code>output</code> <code>str</code> <p>The format of the response.</p> <code>None</code> <code>data_field</code> <code>str</code> <p>The field in the response to extract.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>The formatted response data, or the raw response if no output format is specified.</p> Source code in <code>geostudio/backends/base_client.py</code> <pre><code>def http_patch(self, endpoint, data, output=None, data_field=None):\n    \"\"\"\n    Sends a PATCH request to the specified endpoint with the provided data.\n\n    Parameters:\n        endpoint (str): The URL endpoint to send the PATCH request to.\n        data (dict): The data to be sent in the body of the PATCH request.\n        output (str, optional): The format of the response.\n        data_field (str, optional): The field in the response to extract.\n\n    Returns:\n        Any: The formatted response data, or the raw response if no output format is specified.\n    \"\"\"\n    endpoint = parse.urljoin(self.api_url, endpoint)\n    response = self.session.patch(endpoint, data=json.dumps(data))\n    _check_auth_error(response=response)\n    return formated_output(response=response, output_fmt=output, data_field=data_field)\n</code></pre>"},{"location":"base_client/#geostudio.backends.base_client.BaseClient.http_delete","title":"http_delete","text":"<pre><code>http_delete(endpoint, output=None, data_field=None)\n</code></pre> <p>Sends a DELETE request to the specified endpoint and returns the response.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The URL endpoint to send the DELETE request to.</p> required <code>output</code> <code>str</code> <p>The format of the response.</p> <code>None</code> <code>data_field</code> <code>str</code> <p>The field in the response to extract.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>object</code> <p>The response from the DELETE request, formatted according to the 'output' parameter.</p> Source code in <code>geostudio/backends/base_client.py</code> <pre><code>def http_delete(self, endpoint, output=None, data_field=None):\n    \"\"\"\n    Sends a DELETE request to the specified endpoint and returns the response.\n\n    Parameters:\n        endpoint (str): The URL endpoint to send the DELETE request to.\n        output (str, optional): The format of the response.\n        data_field (str, optional): The field in the response to extract.\n\n    Returns:\n        object: The response from the DELETE request, formatted according to the 'output' parameter.\n    \"\"\"\n    endpoint = parse.urljoin(self.api_url, endpoint)\n    response = self.session.delete(endpoint)\n    _check_auth_error(response=response)\n    return formated_output(response=response, output_fmt=output, data_field=data_field)\n</code></pre>"},{"location":"base_client/#geostudio.backends.base_client.ResponseFormats","title":"ResponseFormats","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>An enumeration representing the supported output formats.</p> <p>Attributes:</p> Name Type Description <code>JSON</code> <code>Dict[str, Any]</code> <p>\"json\"</p> <code>DATAFRAME</code> <code>DataFrame</code> <p>\"df\"</p> <code>RAW</code> <code>object</code> <p>\"raw\"</p>"},{"location":"base_client/#geostudio.backends.base_client.formated_output","title":"formated_output","text":"<pre><code>formated_output(\n    response, output_fmt: str = JSON, data_field: str = None\n) -&gt; Union[DataFrame | Dict[str, Any | str]]\n</code></pre> <p>Formats the response data into the specified output format.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response</code> <p>The HTTP response object.</p> required <code>output_fmt</code> <code>str</code> <p>The desired output format. Defaults to ResponseFormats.JSON.</p> <code>JSON</code> <code>data_field</code> <code>str</code> <p>The specific data field to extract from the response. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[DataFrame | Dict[str, Any | str]]</code> <p>dict or pd.DataFrame: The formatted response data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified output format is not supported.</p> <code>JSONDecodeError</code> <p>If the response cannot be parsed as JSON.</p> Source code in <code>geostudio/backends/base_client.py</code> <pre><code>def formated_output(\n    response, output_fmt: str = ResponseFormats.JSON, data_field: str = None\n) -&gt; Union[pd.DataFrame | Dict[str, Any | str]]:\n    \"\"\"\n    Formats the response data into the specified output format.\n\n    Parameters:\n        response (requests.Response): The HTTP response object.\n        output_fmt (str, optional): The desired output format. Defaults to ResponseFormats.JSON.\n        data_field (str, optional): The specific data field to extract from the response. Defaults to None.\n\n    Returns:\n        dict or pd.DataFrame: The formatted response data.\n\n    Raises:\n        ValueError: If the specified output format is not supported.\n        json.JSONDecodeError: If the response cannot be parsed as JSON.\n    \"\"\"\n    supported_formats = [s.value for s in ResponseFormats]\n    if output_fmt not in supported_formats:\n        raise ValueError(f\"Service `{output_fmt}` is not supported. Valid Options: {supported_formats}\")\n\n    try:\n        resp_data = response.json()\n        if resp_data.get(data_field):\n            resp_data = resp_data[data_field]\n    except json.JSONDecodeError:\n        return {\"reason\": response.text}\n\n    if output_fmt == ResponseFormats.JSON:\n        return response.json()\n    elif output_fmt == ResponseFormats.DATAFRAME:\n        if data_field:\n            df = pd.json_normalize(data=resp_data, sep=\".\", max_level=1)\n        else:\n            df = pd.json_normalize(data=resp_data, sep=\".\", max_level=1)\n        return df\n    elif output_fmt == ResponseFormats.RAW:\n        return response\n</code></pre>"},{"location":"client/","title":"Client","text":""},{"location":"client/#geostudio.backends.main","title":"geostudio.backends.main","text":""},{"location":"client/#geostudio.backends.main.Client","title":"Client","text":"<pre><code>Client(*args, **kwargs)\n</code></pre> <p>               Bases: <code>BaseClient</code></p> <p>A client for interacting with the Geospatial Studio APIs.</p> Example usage <pre><code>from geostudio import Client\ngfm_client = Client(api_key_file=\"/.geostudio_apikey\")\ngfm_client.list_apikeys()\n</code></pre> Source code in <code>geostudio/backends/main.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.load_classes(*args, **kwargs)\n</code></pre>"},{"location":"client/#geostudio.backends.main.Client.load_classes","title":"load_classes","text":"<pre><code>load_classes(*args, **kwargs)\n</code></pre> <p>Dynamically load all classes from subpackages of geostudio.backends.v2</p> Source code in <code>geostudio/backends/main.py</code> <pre><code>def load_classes(self, *args, **kwargs):\n    \"\"\"Dynamically load all classes from subpackages of geostudio.backends.v2\"\"\"\n    package_path = Path(v2.__file__).parent\n    package_name = v2.__name__\n\n    for _, subpkg_name, is_pkg in pkgutil.iter_modules([str(package_path)]):\n        if not is_pkg:\n            continue\n\n        module_name = f\"{package_name}.{subpkg_name}.client\"\n        module = importlib.import_module(module_name)\n\n        if not hasattr(module, \"Client\"):\n            raise ImportError(f\"No Client class found in {module_name}\")\n\n        module_class = getattr(module, \"Client\")\n        instance = module_class(*args, **kwargs)\n\n        setattr(self, subpkg_name, instance)\n\n        for method_name, attr in module_class.__dict__.items():\n            if method_name.startswith(\"_\"):\n                continue\n            if isinstance(attr, property):\n                continue\n            method = getattr(instance, method_name)\n            if callable(method):\n                # bound_method = types.MethodType(method, self)\n                if hasattr(self, method_name):\n                    raise AttributeError(f\"Method conflict: {method_name} already exists\")\n                setattr(self, method_name, method)\n</code></pre>"},{"location":"client/#geostudio.backends.main.Client.list_apikeys","title":"list_apikeys","text":"<pre><code>list_apikeys(output: str = 'json')\n</code></pre> <p>Retrieves a list of API keys associated with the current user.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the list of API keys. The format depends on the 'output' parameter.</p> Source code in <code>geostudio/backends/main.py</code> <pre><code>def list_apikeys(self, output: str = \"json\"):\n    \"\"\"\n    Retrieves a list of API keys associated with the current user.\n\n    Args:\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary containing the list of API keys. The format depends on the 'output' parameter.\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/auth/api-keys\", output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"client/#geostudio.backends.main.Client.create_apikey","title":"create_apikey","text":"<pre><code>create_apikey(data: object = {}, output: str = 'json')\n</code></pre> <p>Creates a new API key.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dictionary containing the data to be sent in the request body. Defaults to an empty dictionary.</p> <code>{}</code> <code>output</code> <code>str</code> <p>The desired output format. Can be either \"json\" or \"xml\". Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The created API key item</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If user already has 2 API keys registered.</p> Source code in <code>geostudio/backends/main.py</code> <pre><code>def create_apikey(self, data: object = {}, output: str = \"json\"):\n    \"\"\"\n    Creates a new API key.\n\n    Args:\n        data (dict, optional): A dictionary containing the data to be sent in the request body.\n            Defaults to an empty dictionary.\n        output (str, optional): The desired output format. Can be either \"json\" or \"xml\".\n            Defaults to \"json\".\n\n    Returns:\n        dict: The created API key item\n\n    Raises:\n        HTTPException: If user already has 2 API keys registered.\n    \"\"\"\n    response = self.http_post(f\"{self.api_version}/auth/api-keys\", data=data, output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"client/#geostudio.backends.main.Client.activate_apikey","title":"activate_apikey","text":"<pre><code>activate_apikey(\n    apikey_id: str,\n    data={\"active\": True},\n    output: str = \"json\",\n)\n</code></pre> <p>Activate and deactivate an API Key.</p> <p>Parameters:</p> Name Type Description Default <code>apikey_id</code> <code>str</code> <p>The ID of the API key to activate/deactivate.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A message of successful activation/deactivation.</p> Source code in <code>geostudio/backends/main.py</code> <pre><code>def activate_apikey(self, apikey_id: str, data={\"active\": True}, output: str = \"json\"):\n    \"\"\"\n    Activate and deactivate an API Key.\n\n    Args:\n        apikey_id (str): The ID of the API key to activate/deactivate.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: A message of successful activation/deactivation.\n    \"\"\"\n    response = self.http_patch(\n        f\"{self.api_version}/auth/api-keys?apikey_id={apikey_id}\", data=data, output=output, data_field=\"results\"\n    )\n    return response\n</code></pre>"},{"location":"client/#geostudio.backends.main.Client.delete_apikey","title":"delete_apikey","text":"<pre><code>delete_apikey(apikey_id: str, output: str = 'json')\n</code></pre> <p>Deletes an API key by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>apikey_id</code> <code>str</code> <p>The ID of the API key to delete.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A message of successful deletion.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the API Key does not exist.</p> Source code in <code>geostudio/backends/main.py</code> <pre><code>def delete_apikey(self, apikey_id: str, output: str = \"json\"):\n    \"\"\"\n    Deletes an API key by its ID.\n\n    Args:\n        apikey_id (str): The ID of the API key to delete.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: A message of successful deletion.\n\n    Raises:\n        HTTPException: If the API Key does not exist.\n    \"\"\"\n    response = self.http_delete(\n        f\"{self.api_version}/auth/api-keys?apikey_id={apikey_id}\", output=output, data_field=\"results\"\n    )\n    return response\n</code></pre>"},{"location":"fine_tuning/","title":"Fine-tuning","text":""},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client","title":"geostudio.backends.v2.gtune.client","text":""},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client","title":"Client","text":"<pre><code>Client(\n    api_config: GeoFmSettings = None,\n    session: Session = None,\n    api_token: str = None,\n    api_key: str = None,\n    api_key_file: str = None,\n    geostudio_config_file: str = None,\n    *args,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseClient</code></p> Source code in <code>geostudio/backends/base_client.py</code> <pre><code>def __init__(\n    self,\n    api_config: GeoFmSettings = None,\n    session: requests.Session = None,\n    api_token: str = None,\n    api_key: str = None,\n    api_key_file: str = None,\n    geostudio_config_file: str = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the GeoFmClient with the provided configuration.\n\n    Args:\n        api_config (GeoFmSettings, optional): The configuration settings for the GeoFm API. Defaults to None.\n        session (requests.Session, optional): A pre-configured requests session. Defaults to None.\n        api_token (str, optional): The API token for authentication. Defaults to None.\n        api_key (str, optional): The API key for authentication. Defaults to None.\n        api_key_file (str, optional): The path to the file containing the API key. Defaults to None.\n        geostudio_config_file (str): The file path to the geostudio config path containing api_key + base_urls.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Raises:\n        GeoFMException: If no API token, API key, or API key file is provided.\n\n    Attributes:\n        api_config (GeoFmSettings): The configuration settings for the GeoFm API.\n        session (requests.Session): A pre-configured requests session.\n        logger (logging.Logger): The logger instance for logging messages.\n    \"\"\"\n    self.api_config = api_config or GeoFmSettings()\n\n    if api_token:\n        print(\"Using api_token\")\n        api_token = api_token or GeoFmSettings.GEOFM_API_TOKEN\n        self.session = gfm_session(access_token=api_token)\n    elif api_key:\n        print(\"Using api_key from sdk command\")\n        self.session = gfm_session(api_key=api_key)\n    elif api_key_file:\n        if not os.path.isfile(api_key_file):\n            raise GeoFMException(\"Config file does not exist, Please provide a valid config file.\")\n        print(\"Using api_key from file\")\n        self.session = gfm_session(api_key_file=api_key_file)\n    elif geostudio_config_file:\n        if not os.path.isfile(geostudio_config_file):\n            raise GeoFMException(\"Config file does not exist, Please provide a valid config file.\")\n        print(\"Using api key and base urls from geostudio config file\")\n        geostudio_config_file_values = dotenv_values(geostudio_config_file)\n        settings.BASE_GATEWAY_API_URL = geostudio_config_file_values.get(\"BASE_GATEWAY_API_URL\", \"\")\n        settings.BASE_STUDIO_UI_URL = geostudio_config_file_values.get(\"BASE_STUDIO_UI_URL\", \"\")\n        settings.GEOSTUDIO_API_KEY = geostudio_config_file_values.get(\"GEOSTUDIO_API_KEY\", None)\n        self.session = gfm_session(api_key=settings.GEOSTUDIO_API_KEY)\n    else:\n        raise GeoFMException(\"Missing APIToken. Add `GEOFM_API_TOKEN` to env variables.\")\n\n    # else:\n    #     self.session = session or gfm_session(\n    #         client_id=self.api_config.ISV_CLIENT_ID,\n    #         client_secret=self.api_config.ISV_CLIENT_SECRET,\n    #         well_known_url=self.api_config.ISV_WELL_KNOWN,\n    #         userinfo_endpoint=self.api_config.ISV_USER_ENDPOINT,\n    #     )\n    self.logger = logging.getLogger()\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.list_tunes","title":"list_tunes","text":"<pre><code>list_tunes(output: str = 'json')\n</code></pre> <p>Lists all fine tuning jobs in the studio.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the list of tunes found.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def list_tunes(self, output: str = \"json\"):\n    \"\"\"\n    Lists all fine tuning jobs in the studio.\n\n    Args:\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary containing the list of tunes found.\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/tunes\", output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_tune","title":"get_tune","text":"<pre><code>get_tune(tune_id: str, output: str = 'json')\n</code></pre> <p>Retrieves a tune by ID. If the tune's status is Failed, a pre-signed url for the logs is generated.</p> <p>Parameters:</p> Name Type Description Default <code>tune_id</code> <code>str</code> <p>The unique identifier of the tune to retrieve.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The tune's status and information</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def get_tune(self, tune_id: str, output: str = \"json\"):\n    \"\"\"\n    Retrieves a tune by ID. If the tune's status is Failed, a pre-signed url for the logs is generated.\n\n    Parameters:\n        tune_id (str): The unique identifier of the tune to retrieve.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: The tune's status and information\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/tunes/{tune_id}\", output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.update_tune","title":"update_tune","text":"<pre><code>update_tune(\n    tune_id: str, data: TuneUpdateIn, output: str = \"json\"\n)\n</code></pre> <p>Update a tune in the database</p> <p>Parameters:</p> Name Type Description Default <code>tune_id</code> <code>str</code> <p>The unique identifier of the tune to be updated.</p> required <code>data</code> <code>TuneUpdateIn</code> <p>A dictionary containing the data to update for the tune.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of the updated tune.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def update_tune(self, tune_id: str, data: TuneUpdateIn, output: str = \"json\"):\n    \"\"\"\n    Update a tune in the database\n\n    Args:\n        tune_id (str): The unique identifier of the tune to be updated.\n        data (TuneUpdateIn): A dictionary containing the data to update for the tune.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary of the updated tune.\n    \"\"\"\n    payload = json.loads(TuneUpdateIn(**data).model_dump_json())\n    response = self.http_patch(f\"{self.api_version}/tunes/{tune_id}\", data=payload, output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.delete_tune","title":"delete_tune","text":"<pre><code>delete_tune(tune_id, output: str = 'json')\n</code></pre> <p>Deletes a specified tune using its ID.</p> <p>Parameters:</p> Name Type Description Default <code>tune_id</code> <code>str</code> <p>The ID of the tune to be deleted.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Message of successfully deleted tune</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def delete_tune(self, tune_id, output: str = \"json\"):\n    \"\"\"\n    Deletes a specified tune using its ID.\n\n    Args:\n        tune_id (str): The ID of the tune to be deleted.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: Message of successfully deleted tune\n    \"\"\"\n    response = self.http_delete(f\"{self.api_version}/tunes/{tune_id}\", output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.submit_tune","title":"submit_tune","text":"<pre><code>submit_tune(data: TuneSubmitIn, output: str = 'json')\n</code></pre> <p>Submit a fine-tuning job to the Geospatial studio platform</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TuneSubmitIn</code> <p>Parameters for the tuning job.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The server's response containing the submitted tune info.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def submit_tune(self, data: TuneSubmitIn, output: str = \"json\"):\n    \"\"\"\n    Submit a fine-tuning job to the Geospatial studio platform\n\n    Args:\n        data (TuneSubmitIn): Parameters for the tuning job.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: The server's response containing the submitted tune info.\n    \"\"\"\n    data[\"name\"] = data[\"name\"].lower().replace(\" \", \"-\").replace(\"_\", \"-\")\n\n    payload = json.loads(TuneSubmitIn(**data).model_dump_json())\n    response = self.http_post(f\"{self.api_version}/submit-tune\", data=payload, output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.submit_hpo_tune","title":"submit_hpo_tune","text":"<pre><code>submit_hpo_tune(\n    data: HpoTuneSubmitIn, output: str = \"json\"\n)\n</code></pre> <p>Submit a fine-tuning job with terratorch-iterate enabled.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>HpoTuneSubmitIn</code> <p>Parameters for the tuning job</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The server's response containing the submitted tune info.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def submit_hpo_tune(self, data: HpoTuneSubmitIn, output: str = \"json\"):\n    \"\"\"Submit a fine-tuning job with terratorch-iterate enabled.\n\n    Args:\n        data (HpoTuneSubmitIn): Parameters for the tuning job\n        output ( str, optional):  The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: The server's response containing the submitted tune info.\n    \"\"\"\n    if isinstance(data, dict):\n        data = HpoTuneSubmitIn(**data)\n\n    if not os.path.isfile(data.config_file):\n        raise ValueError(f\"Config file not found: {data.config_file}\")\n    if os.path.getsize(data.config_file) == 0:\n        raise ValueError(f\"Config file is empty: {data.config_file}\")\n\n    filename = os.path.basename(data.config_file)\n    with open(data.config_file, \"rb\") as fobj:\n        config_content = fobj.read()\n\n    files = {\"config_file\": (filename, config_content, \"application/x-yaml\")}\n    payload = {\"tune_metadata\": data.tune_metadata.model_dump_json()}\n    response = self.http_post(\n        f\"{self.api_version}/submit-hpo-tune\",\n        data=payload,\n        files=files,\n        output=output,\n    )\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.upload_completed_tunes","title":"upload_completed_tunes","text":"<pre><code>upload_completed_tunes(data: UploadTuneInput)\n</code></pre> <p>Upload a completed fine-tuning job to the Geostudio platform</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>UploadTuneInput</code> <p>Parameters to update the tune with</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Message of successfully uploaded tune</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def upload_completed_tunes(self, data: UploadTuneInput):\n    \"\"\"\n    Upload a completed fine-tuning job to the Geostudio platform\n\n    Args:\n        data (UploadTuneInput): Parameters to update the tune with\n\n    Returns:\n        dict: Message of successfully uploaded tune\n    \"\"\"\n    payload = json.loads(UploadTuneInput(**data).model_dump_json())\n    response = self.http_post(f\"{self.api_version}/upload-completed-tunes\", data=payload, output=\"json\")\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.try_out_tune","title":"try_out_tune","text":"<pre><code>try_out_tune(tune_id: str, data: TryOutTuneInput)\n</code></pre> <p>Try-out inference on a tune without deploying the model.</p> <p>Parameters:</p> Name Type Description Default <code>tune_id</code> <code>str</code> <p>The unique identifier of the tune experiment.</p> required <code>data</code> <code>TryOutTuneInput</code> <p>The inference configurations to try the tune on</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary containing the details of the inference submitted.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def try_out_tune(self, tune_id: str, data: TryOutTuneInput):\n    \"\"\"Try-out inference on a tune without deploying the model.\n\n    Args:\n        tune_id (str): The unique identifier of the tune experiment.\n        data (TryOutTuneInput): The inference configurations to try the tune on\n\n    Returns:\n        dict: Dictionary containing the details of the inference submitted.\n    \"\"\"\n    payload = json.loads(TryOutTuneInput(**data).model_dump_json())\n    response = self.http_post(f\"{self.api_version}/tunes/{tune_id}/try-out\", data=payload, output=\"json\")\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.download_tune","title":"download_tune","text":"<pre><code>download_tune(tune_id: str, output: str = 'json')\n</code></pre> <p>Downloads a tuned model from the server.</p> <p>Parameters:</p> Name Type Description Default <code>tune_id</code> <code>str</code> <p>The unique identifier of the tuned model to download.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary with tune details including presigned urls to download the artifacts.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def download_tune(self, tune_id: str, output: str = \"json\"):\n    \"\"\"\n    Downloads a tuned model from the server.\n\n    Args:\n        tune_id (str): The unique identifier of the tuned model to download.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: Dictionary with tune details including presigned urls to download the artifacts.\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/tunes/{tune_id}/download\", output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_mlflow_metrics","title":"get_mlflow_metrics","text":"<pre><code>get_mlflow_metrics(tune_id: str, output: str = 'json')\n</code></pre> <p>Retrieves the MLflow URLs for the training and testing metrics of a given Tune experiment.</p> <p>Parameters:</p> Name Type Description Default <code>tune_id</code> <code>str</code> <p>The ID of the Tune experiment.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the MLflow URLs for the training and testing metrics. The dictionary will have the keys \"train_mlflow_url\" and \"test_mlflow_url\". If no metrics are found, the value for \"train_mlflow_url\" will be None.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def get_mlflow_metrics(self, tune_id: str, output: str = \"json\"):\n    \"\"\"\n    Retrieves the MLflow URLs for the training and testing metrics of a given Tune experiment.\n\n    Args:\n        tune_id (str): The ID of the Tune experiment.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary containing the MLflow URLs for the training and testing metrics.\n            The dictionary will have the keys \"train_mlflow_url\" and \"test_mlflow_url\".\n            If no metrics are found, the value for \"train_mlflow_url\" will be None.\n    \"\"\"\n    response = self.get_tune(tune_id)\n\n    ui_url_path = f\"{settings.BASE_STUDIO_UI_URL}mlflow/#\"\n    # Sample output [{'Train': '/experiments/exp_id/runs/run_id'}, {'Test': '/experiments/exp_id/runs/run_id'}]\n    train_path = None\n    test_path = None\n    try:\n        if response[\"metrics\"]:\n            merged = {k: v for d in response[\"metrics\"] for k, v in d.items()}\n            train_path = merged.get(\"Train\")\n            test_path = merged.get(\"Test\")\n\n            train_path = f\"{ui_url_path}{train_path}\"\n            if test_path:\n                test_path = f\"{ui_url_path}{test_path}\"\n\n            return {\"train_mlflow_url\": train_path, \"test_mlflow_url\": test_path}\n        else:\n            print(f\"No mlflow url found for {tune_id}\")\n\n    except Exception as e:\n        print(f\"Error getting metrics urls: {e} \")\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_tune_metrics","title":"get_tune_metrics","text":"<pre><code>get_tune_metrics(tune_id: str, output: str = 'json')\n</code></pre> <p>Retrieves the MLflow metrics for a specific tune.</p> <p>Parameters:</p> Name Type Description Default <code>tune_id</code> <code>str</code> <p>The unique identifier of the tune.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The metrics of the tune in the specified format.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def get_tune_metrics(self, tune_id: str, output: str = \"json\"):\n    \"\"\"\n    Retrieves the MLflow metrics for a specific tune.\n\n    Args:\n        tune_id (str): The unique identifier of the tune.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: The metrics of the tune in the specified format.\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/tunes/{tune_id}/metrics\", output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_tune_metrics_df","title":"get_tune_metrics_df","text":"<pre><code>get_tune_metrics_df(tune_id: str, run_name: str = 'Train')\n</code></pre> <p>Retrieves the MLflow metrics for a specific tune and displays them in a pandas DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>tune_id</code> <code>str</code> <p>The unique identifier of the tune.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A pandas DataFrame containing the tuning metrics.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def get_tune_metrics_df(self, tune_id: str, run_name: str = \"Train\"):\n    \"\"\"\n    Retrieves the MLflow metrics for a specific tune and displays them in a pandas DataFrame\n\n    Args:\n        tune_id (str): The unique identifier of the tune.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame containing the tuning metrics.\n\n    \"\"\"\n    m = self.get_tune_metrics(tune_id)\n    if not m.get(\"runs\"):\n        return pd.DataFrame()\n    run = next((run for run in m.get(\"runs\") if run.get(\"name\") == run_name), {})\n    if not run.get(\"metrics\"):\n        return pd.DataFrame()\n    mdf = pd.DataFrame.from_records(run[\"metrics\"][0])\n\n    for i in range(2, len(run[\"metrics\"])):\n        mdf_tmp = pd.DataFrame.from_records(run[\"metrics\"][i]).drop([\"epoch\"], axis=1)\n        mdf = pd.concat([mdf, mdf_tmp], axis=1)\n\n    mdf.sort_values([\"epoch\"], inplace=True)\n\n    return mdf\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.list_tuning_artefacts","title":"list_tuning_artefacts","text":"<pre><code>list_tuning_artefacts(tune_id: str)\n</code></pre> <p>Resolve the MLflow training run referenced by a tune and list artefact paths.</p> <p>This function: - Calls gfm_client.get_tune(tune_id) to obtain tune metadata that contains a     reference to the MLflow training run (expected under a metric named 'Train'). - Queries the MLflow server's artifacts list endpoint for that run to obtain     available artifact file paths.</p>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.list_tuning_artefacts--parameters","title":"Parameters","text":"<p>tune_id : str     Identifier of the tune (used to lookup metrics that contain the MLflow train run).</p>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.list_tuning_artefacts--returns","title":"Returns","text":"<p>tuple[list[str], str]     A tuple (art_files, train_run_id) where:     - art_files : list[str] \u2014 list of artifact paths returned by MLflow (from the         'files' array -&gt; each element's 'path').     - train_run_id : str \u2014 the resolved MLflow run id extracted from the tune metadata.</p>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.list_tuning_artefacts--notes","title":"Notes","text":"<ul> <li>The function expects the tune metadata (gfm_client.get_tune) to include a metric mapping containing a 'Train' entry whose value includes the MLflow run UUID (the run id is taken as the last path segment after splitting on '/').</li> <li>The MLflow artifacts list response is assumed to include a JSON 'files' array where each item has a 'path' key.</li> <li>Example:     art_files, run_id = list_tuning_artefacts('geotune-xxxxx', 'https://my-mlflow')</li> </ul> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def list_tuning_artefacts(self, tune_id: str):\n    \"\"\"\n    Resolve the MLflow training run referenced by a tune and list artefact paths.\n\n    This function:\n    - Calls gfm_client.get_tune(tune_id) to obtain tune metadata that contains a\n        reference to the MLflow training run (expected under a metric named 'Train').\n    - Queries the MLflow server's artifacts list endpoint for that run to obtain\n        available artifact file paths.\n\n    Parameters\n    ----------\n    tune_id : str\n        Identifier of the tune (used to lookup metrics that contain the MLflow train run).\n\n    Returns\n    -------\n    tuple[list[str], str]\n        A tuple (art_files, train_run_id) where:\n        - art_files : list[str] \u2014 list of artifact paths returned by MLflow (from the\n            'files' array -&gt; each element's 'path').\n        - train_run_id : str \u2014 the resolved MLflow run id extracted from the tune metadata.\n\n    Notes\n    -----\n    - The function expects the tune metadata (gfm_client.get_tune) to include a metric\n    mapping containing a 'Train' entry whose value includes the MLflow run UUID (the\n    run id is taken as the last path segment after splitting on '/').\n    - The MLflow artifacts list response is assumed to include a JSON 'files' array where\n    each item has a 'path' key.\n    - Example:\n        art_files, run_id = list_tuning_artefacts('geotune-xxxxx', 'https://my-mlflow')\n    \"\"\"\n\n    mlflow_url = f\"{'/'.join(self.api_url[:-1].split('/')[:-1])}/mlflow\"\n\n    tune_info = self.get_tune(tune_id)\n    train_run_id = {k: v for d in tune_info[\"metrics\"] for k, v in d.items()}[\"Train\"].split(\"/\")[-1]\n\n    # req = requests.get(f\"{mlflow_url}/api/2.0/mlflow/artifacts/list?run_id={train_run_id}\")\n    print(f\"{mlflow_url}/api/2.0/mlflow/artifacts/list?run_id={train_run_id}\")\n    req = self.http_get(f\"{mlflow_url}/api/2.0/mlflow/artifacts/list?run_id={train_run_id}\", output=\"json\")\n    art_list = req[\"files\"]\n    art_files = [X[\"path\"] for X in art_list]\n    print(f\"Found {len(art_files)} artefacts\")\n    return art_files, train_run_id\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_tuning_artefacts","title":"get_tuning_artefacts","text":"<pre><code>get_tuning_artefacts(\n    tune_id: str,\n    epochs: list = None,\n    image_numbers: list = None,\n)\n</code></pre> <p>Download fine\u2011tuning artefact images from an MLflow run referenced by a tune.</p> <p>This function: - Resolves the MLflow training run id for the given tune via gfm_client.get_tune(...) - Lists artefacts for that run from the MLflow server - Optionally filters artefact filenames by epoch and/or image number - Downloads matching artefacts in parallel and returns a list of records</p>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_tuning_artefacts--parameters","title":"Parameters","text":"<p>tune_id : str     Identifier of the tune (used to lookup metrics that contain the MLflow train run). epochs : list[int], optional     If provided, only artefacts whose filename encodes an epoch contained in this list     are retained. Filenames are assumed to contain epoch as the second underscore-separated     token (e.g. \"epoch_4_5.png\" -&gt; epoch 4). image_numbers : list[int], optional     If provided, only artefacts whose filename encodes an image number contained in this     list are retained. Filenames are assumed to contain the image number as the third     underscore-separated token (e.g. \"epoch_4_5.png\" -&gt; image_number 5).</p>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_tuning_artefacts--returns","title":"Returns","text":"<p>list[dict]     A list of dictionaries, one per downloaded artefact, with keys:     - 'filename' (str): artefact path from MLflow     - 'image' (bytes): raw downloaded bytes     - 'epoch' (int): parsed epoch number     - 'image_number' (int): parsed image/sample number</p>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_tuning_artefacts--notes","title":"Notes","text":"<ul> <li>Downloads are performed in parallel using joblib (threads).</li> <li>The function assumes artefact filenames follow the pattern containing \"epoch__.\". Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def get_tuning_artefacts(self, tune_id: str, epochs: list = None, image_numbers: list = None):\n    \"\"\"\n    Download fine\u2011tuning artefact images from an MLflow run referenced by a tune.\n\n    This function:\n    - Resolves the MLflow training run id for the given tune via gfm_client.get_tune(...)\n    - Lists artefacts for that run from the MLflow server\n    - Optionally filters artefact filenames by epoch and/or image number\n    - Downloads matching artefacts in parallel and returns a list of records\n\n    Parameters\n    ----------\n    tune_id : str\n        Identifier of the tune (used to lookup metrics that contain the MLflow train run).\n    epochs : list[int], optional\n        If provided, only artefacts whose filename encodes an epoch contained in this list\n        are retained. Filenames are assumed to contain epoch as the second underscore-separated\n        token (e.g. \"epoch_4_5.png\" -&gt; epoch 4).\n    image_numbers : list[int], optional\n        If provided, only artefacts whose filename encodes an image number contained in this\n        list are retained. Filenames are assumed to contain the image number as the third\n        underscore-separated token (e.g. \"epoch_4_5.png\" -&gt; image_number 5).\n\n    Returns\n    -------\n    list[dict]\n        A list of dictionaries, one per downloaded artefact, with keys:\n        - 'filename' (str): artefact path from MLflow\n        - 'image' (bytes): raw downloaded bytes\n        - 'epoch' (int): parsed epoch number\n        - 'image_number' (int): parsed image/sample number\n\n    Notes\n    -----\n    - Downloads are performed in parallel using joblib (threads).\n    - The function assumes artefact filenames follow the pattern containing\n    \"epoch_&lt;epoch&gt;_&lt;image_number&gt;.&lt;ext&gt;\".\n    \"\"\"\n\n    requests.packages.urllib3.disable_warnings()\n    art_files, train_run_id = self.list_tuning_artefacts(tune_id)\n\n    if epochs is not None:\n        art_files = [X for X in art_files if int(X.split(\"_\")[1]) in epochs]\n    if image_numbers is not None:\n        art_files = [X for X in art_files if int(X.split(\"_\")[2].split(\".\")[0]) in image_numbers]\n\n    print(f\"Downloading {len(art_files)} artefacts...\")\n\n    ans = list(\n        track(\n            Parallel(n_jobs=10, prefer=\"threads\")(\n                delayed(self.get_training_image)(fn, train_run_id) for fn in art_files\n            ),\n            total=len(art_files),\n        )\n    )\n\n    print(\"Downloaded all artefacts\")\n    img_dict = [\n        {\n            \"filename\": art_files[X],\n            \"image\": ans[X],\n            \"epoch\": int(art_files[X].split(\"_\")[1]),\n            \"image_number\": int(art_files[X].split(\"_\")[2].split(\".\")[0]),\n        }\n        for X in range(0, len(art_files))\n    ]\n\n    return img_dict\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.list_tune_templates","title":"list_tune_templates","text":"<pre><code>list_tune_templates(output: str = 'json')\n</code></pre> <p>Lists tune templates studio.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the list of tune templates in the studio</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def list_tune_templates(self, output: str = \"json\"):\n    \"\"\"\n    Lists tune templates studio.\n\n    Args:\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary containing the list of tune templates in the studio\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/tune-templates\", output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.create_task","title":"create_task","text":"<pre><code>create_task(data: TaskIn, output: str = 'json')\n</code></pre> <p>Creates a new task using the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TaskIn</code> <p>The data required to create a new task.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The response from the server containing the details of the newly created task.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def create_task(self, data: TaskIn, output: str = \"json\"):\n    \"\"\"\n    Creates a new task using the provided data.\n\n    Args:\n        data (TaskIn): The data required to create a new task.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: The response from the server containing the details of the newly created task.\n    \"\"\"\n    response = self.http_post(f\"{self.api_version}/tune-templates\", data=data, output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_task","title":"get_task","text":"<pre><code>get_task(task_id: str, output: str = 'json')\n</code></pre> <p>Retrieves a task by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The ID of the task to retrieve.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The response from the server containing the task details.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def get_task(self, task_id: str, output: str = \"json\"):\n    \"\"\"\n    Retrieves a task by its ID.\n\n    Args:\n        task_id (str): The ID of the task to retrieve.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: The response from the server containing the task details.\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/tune-templates/{task_id}\", output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.delete_task","title":"delete_task","text":"<pre><code>delete_task(task_id, output: str = 'json')\n</code></pre> <p>Deletes a task with the given task_id.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The unique identifier of the task to be deleted.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Message of successfully deleted task.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def delete_task(self, task_id, output: str = \"json\"):\n    \"\"\"\n    Deletes a task with the given task_id.\n\n    Args:\n        task_id (str): The unique identifier of the task to be deleted.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: Message of successfully deleted task.\n\n    \"\"\"\n    response = self.http_delete(f\"{self.api_version}/tune-templates/{task_id}\", output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_task_template","title":"get_task_template","text":"<pre><code>get_task_template(task_id: str, output: str = 'text')\n</code></pre> <p>Retrieves the task template yaml for the selected task</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The ID of the task to retrieve.</p> required <code>output</code> <code>str</code> <p>The format of the response. Can either be \"cell\", \"text\" or \"file\".</p> <code>'text'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The response from the server containing the task template yaml.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def get_task_template(self, task_id: str, output: str = \"text\"):\n    \"\"\"\n    Retrieves the task template yaml for the selected task\n\n    Args:\n        task_id (str): The ID of the task to retrieve.\n        output (str, optional): The format of the response. Can either be \"cell\", \"text\" or \"file\".\n\n    Returns:\n        dict: The response from the server containing the task template yaml.\n    \"\"\"\n    response = self.http_get(\n        f\"{self.api_version}/tune-templates/{task_id}/template\", output=\"json\", data_field=\"results\"\n    )\n    if output == \"text\":\n        return response[\"reason\"]\n    elif output == \"cell\":\n        create_new_cell(f\"ty = '''{response['reason']}''' \")\n    elif output == \"file\":\n        with open(task_id + \".yaml\", \"w\") as fp:\n            fp.write(response[\"reason\"])\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.update_task","title":"update_task","text":"<pre><code>update_task(\n    task_id: str, file_path: str, output: str = \"json\"\n)\n</code></pre> <p>Updates a task's content with a yaml file config</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The ID of the task to upload.</p> required <code>file_path</code> <code>str</code> <p>The path to the file containing the new template.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Message of successful task upload</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def update_task(self, task_id: str, file_path: str, output: str = \"json\"):\n    \"\"\"\n    Updates a task's content with a yaml file config\n\n    Args:\n        task_id (str): The ID of the task to upload.\n        file_path (str): The path to the file containing the new template.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: Message of successful task upload\n    \"\"\"\n\n    response = self.http_put_file(\n        f\"{self.api_version}/tune-templates/{task_id}/template\", file_path=file_path, output=output\n    )\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.update_task_schema","title":"update_task_schema","text":"<pre><code>update_task_schema(\n    task_id: str, task_schema: Any, output: str = \"json\"\n)\n</code></pre> <p>Update the JSONSchema of a task.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The ID of the task to update.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Message of successful task update</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def update_task_schema(self, task_id: str, task_schema: Any, output: str = \"json\"):\n    \"\"\"\n    Update the JSONSchema of a task.\n\n    Args:\n        task_id (str): The ID of the task to update.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: Message of successful task update\n    \"\"\"\n\n    response = self.http_put(f\"{self.api_version}/tune-templates/{task_id}/schema\", data=task_schema, output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_task_param_defaults","title":"get_task_param_defaults","text":"<pre><code>get_task_param_defaults(task_id: str)\n</code></pre> <p>Retrieves the default parameter values for a given task.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The unique identifier of the task.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the default parameter values for the task. The keys are the parameter names and the values are the default values.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def get_task_param_defaults(self, task_id: str):\n    \"\"\"\n    Retrieves the default parameter values for a given task.\n\n    Args:\n        task_id (str): The unique identifier of the task.\n\n    Returns:\n        dict: A dictionary containing the default parameter values for the task.\n            The keys are the parameter names and the values are the default values.\n    \"\"\"\n    task_meta = self.get_task(task_id)\n    defaults_dict = {}\n    for k in task_meta[\"model_params\"][\"properties\"].keys():\n        if \"properties\" in task_meta[\"model_params\"][\"properties\"][k]:\n            defaults_dict[k] = task_meta[\"model_params\"][\"properties\"][k][\"default\"]\n    return defaults_dict\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.check_task_content","title":"check_task_content","text":"<pre><code>check_task_content(\n    task_id: str,\n    dataset_id: str,\n    base_model_id: Any,\n    output: str = \"text\",\n)\n</code></pre> <p>Checks that the the task renders correctly</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The ID of the task to check.</p> required <code>output</code> <code>str</code> <p>The format of the returned template. Can be \"text\", \"cell\", or \"file\". Defaults to \"text\".</p> <code>'text'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Message of task content</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def check_task_content(self, task_id: str, dataset_id: str, base_model_id: Any, output: str = \"text\"):\n    \"\"\"\n    Checks that the the task renders correctly\n\n    Args:\n        task_id (str): The ID of the task to check.\n        output (str, optional): The format of the returned template. Can be \"text\", \"cell\", or \"file\". Defaults to \"text\".\n\n    Returns:\n        dict: Message of task content\n    \"\"\"\n    params = {\"dataset_id\": dataset_id, \"base_model\": base_model_id}\n    response = self.http_get(\n        f\"{self.api_version}/tune-templates/{task_id}/test-render\", params=params, output=\"json\"\n    )\n    if output == \"text\":\n        return response[\"reason\"]\n    elif output == \"cell\":\n        create_new_cell(f\"ty = '''{response['reason']}''' \")\n    elif output == \"file\":\n        with open(task_id + \".yaml\", \"w\") as fp:\n            fp.write(response[\"reason\"])\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.render_template","title":"render_template","text":"<pre><code>render_template(\n    task_id: str, dataset_id: str, output: str = \"text\"\n)\n</code></pre> <p>Checks that the the user defined task renders correctly.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>The ID of the task to check.</p> required <code>dataset_id</code> <code>str</code> <p>The ID of the dataset associated with the task.</p> required <code>output</code> <code>str</code> <p>The format of the returned template. Can be \"text\", \"cell\", or \"file\". Defaults to \"text\".</p> <code>'text'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The rendered template in the specified output format.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def render_template(self, task_id: str, dataset_id: str, output: str = \"text\"):\n    \"\"\"\n    Checks that the the user defined task renders correctly.\n\n    Args:\n        task_id (str): The ID of the task to check.\n        dataset_id (str): The ID of the dataset associated with the task.\n        output (str, optional): The format of the returned template. Can be \"text\", \"cell\", or \"file\". Defaults to \"text\".\n\n    Returns:\n        dict: The rendered template in the specified output format.\n    \"\"\"\n    t_params = {\"dataset_id\": dataset_id}\n    response = self.http_get(\n        f\"{self.api_version}/tune-templates/{task_id}/test-render-user-defined-task\", params=t_params, output=\"json\"\n    )\n    if output == \"text\":\n        return response[\"reason\"]\n    elif output == \"cell\":\n        create_new_cell(f\"ty = '''{response['reason']}''' \")\n    elif output == \"file\":\n        with open(task_id + \".yaml\", \"w\") as fp:\n            fp.write(response[\"reason\"])\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.list_datasets","title":"list_datasets","text":"<pre><code>list_datasets(output: str = 'json')\n</code></pre> <p>Lists all datasets available in the studio.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing a list of datasets found in the dataset factory</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def list_datasets(self, output: str = \"json\"):\n    \"\"\"\n    Lists all datasets available in the studio.\n\n    Parameters:\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary containing a list of datasets found in the dataset factory\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/datasets\", output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.pre_scan_dataset","title":"pre_scan_dataset","text":"<pre><code>pre_scan_dataset(\n    data: PreScanDatasetIn, output: str = \"json\"\n)\n</code></pre> <p>Scans a new dataset - checks accessibility of the dataset URL, ensures corresponding data and label files are present, and extracts bands and their descriptions from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>PreScanDatasetIn</code> <p>Link to the dataset to scan</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the scan results.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def pre_scan_dataset(self, data: PreScanDatasetIn, output: str = \"json\"):\n    \"\"\"\n    Scans a new dataset - checks accessibility of the dataset URL, ensures corresponding data and label files are present, and extracts bands and their descriptions from the dataset.\n\n    Args:\n        data (PreScanDatasetIn): Link to the dataset to scan\n\n    Returns:\n        dict: A dictionary containing the scan results.\n    \"\"\"\n    payload = json.loads(PreScanDatasetIn(**data).model_dump_json())\n    response = self.http_post(f\"{self.api_version}/datasets/pre-scan\", data=payload, output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_sample_images","title":"get_sample_images","text":"<pre><code>get_sample_images(dataset_id: str, output: str = 'json')\n</code></pre> <p>Retrieves a sample of images from a specified dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>The unique identifier of the dataset.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the sample data in the requested format.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def get_sample_images(self, dataset_id: str, output: str = \"json\"):\n    \"\"\"\n    Retrieves a sample of images from a specified dataset.\n\n    Args:\n        dataset_id (str): The unique identifier of the dataset.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary containing the sample data in the requested format.\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/datasets/{dataset_id}/sample\", output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.update_dataset","title":"update_dataset","text":"<pre><code>update_dataset(\n    dataset_id: str,\n    data: DatasetUpdateIn,\n    output: str = \"json\",\n)\n</code></pre> <p>Update a dataset metadata in the database</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>The unique identifier of the dataset to be updated.</p> required <code>data</code> <code>DatasetUpdateIn</code> <p>A dictionary containing the data to update for the dataset.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of the updated dataset.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def update_dataset(self, dataset_id: str, data: DatasetUpdateIn, output: str = \"json\"):\n    \"\"\"\n    Update a dataset metadata in the database\n\n    Args:\n        dataset_id (str): The unique identifier of the dataset to be updated.\n        data (DatasetUpdateIn): A dictionary containing the data to update for the dataset.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary of the updated dataset.\n    \"\"\"\n    payload = json.loads(DatasetUpdateIn(**data).model_dump_json())\n    response = self.http_patch(f\"{self.api_version}/datasets/{dataset_id}\", data=payload, output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(dataset_id: str, output: str = 'json')\n</code></pre> <p>Retrieves a dataset from the studio.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>The unique identifier of the dataset to retrieve.</p> required <code>output</code> <code>str</code> <p>The format of the response. Default is \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Information about the dataset found.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def get_dataset(self, dataset_id: str, output: str = \"json\"):\n    \"\"\"\n    Retrieves a dataset from the studio.\n\n    Parameters:\n        dataset_id (str): The unique identifier of the dataset to retrieve.\n        output (str, optional): The format of the response. Default is \"json\".\n\n    Returns:\n        dict: Information about the dataset found.\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/datasets/{dataset_id}\", output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.delete_dataset","title":"delete_dataset","text":"<pre><code>delete_dataset(dataset_id: str, output: str = 'json')\n</code></pre> <p>Deletes a dataset with the given ID.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>The ID of the dataset to delete.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with a message after dataset is deleted</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def delete_dataset(self, dataset_id: str, output: str = \"json\"):\n    \"\"\"\n    Deletes a dataset with the given ID.\n\n    Args:\n        dataset_id (str): The ID of the dataset to delete.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary with a message after dataset is deleted\n    \"\"\"\n    response = self.http_delete(f\"{self.api_version}/datasets/{dataset_id}\", output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.onboard_dataset","title":"onboard_dataset","text":"<pre><code>onboard_dataset(\n    data: DatasetOnboardIn, output: str = \"json\"\n)\n</code></pre> <p>Onboards a new dataset to the Geospatial studio.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DatasetOnboardIn</code> <p>The dataset information to be onboarded.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing information about the onboarded dataset.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def onboard_dataset(self, data: DatasetOnboardIn, output: str = \"json\"):\n    \"\"\"\n    Onboards a new dataset to the Geospatial studio.\n\n    Args:\n        data (DatasetOnboardIn): The dataset information to be onboarded.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary containing information about the onboarded dataset.\n    \"\"\"\n    payload = json.loads(DatasetOnboardIn(**data).model_dump_json())\n    response = self.http_post(f\"{self.api_version}/datasets/onboard\", data=payload, output=output)\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.list_base_models","title":"list_base_models","text":"<pre><code>list_base_models(output: str = 'json')\n</code></pre> <p>Lists all available base foundation models.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing a list of base foundation models available in the studio</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def list_base_models(self, output: str = \"json\"):\n    \"\"\"\n    Lists all available base foundation models.\n\n    Parameters:\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary containing a list of base foundation models available in the studio\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/base-models\", output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.create_base_model","title":"create_base_model","text":"<pre><code>create_base_model(data: BaseModelsIn, output: str = 'json')\n</code></pre> <p>Create a base foundation model in the Studio.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <code>data</code> <code>BaseModelsIn</code> <p>Parameters for creating the base model.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing a list of base foundation models available in the studio</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def create_base_model(self, data: BaseModelsIn, output: str = \"json\"):\n    \"\"\"\n    Create a base foundation model in the Studio.\n\n    Parameters:\n        output (str, optional): The format of the response. Defaults to \"json\".\n        data (BaseModelsIn): Parameters for creating the base model.\n\n    Returns:\n        dict: A dictionary containing a list of base foundation models available in the studio\n    \"\"\"\n    response = self.http_post(f\"{self.api_version}/base-models\", data=data, output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.get_base_model","title":"get_base_model","text":"<pre><code>get_base_model(base_id: str, output: str = 'json')\n</code></pre> <p>Get base foundation model by id.</p> <p>Parameters:</p> Name Type Description Default <code>base_id</code> <code>str</code> <p>Base model ID</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The Found base model</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def get_base_model(self, base_id: str, output: str = \"json\"):\n    \"\"\"\n    Get base foundation model by id.\n\n    Parameters:\n        base_id (str): Base model ID\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: The Found base model\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/base-models/{base_id}\", output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.update_base_model_params","title":"update_base_model_params","text":"<pre><code>update_base_model_params(\n    base_id: str,\n    data: BaseModelParamsIn,\n    output: str = \"json\",\n)\n</code></pre> <p>Update base foundation model params.</p> <p>Parameters:</p> Name Type Description Default <code>base_id</code> <code>str</code> <p>Base model ID.</p> required <code>data</code> <code>BaseModelParamsIn</code> <p>Base model params to update.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Updates Base model params</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def update_base_model_params(self, base_id: str, data: BaseModelParamsIn, output: str = \"json\"):\n    \"\"\"\n    Update base foundation model params.\n\n    Parameters:\n        base_id (str): Base model ID.\n        data (BaseModelParamsIn): Base model params to update.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: Updates Base model params\n    \"\"\"\n    response = self.http_patch(\n        f\"{self.api_version}/base-models/{base_id}/model-params\", data=data, output=output, data_field=\"results\"\n    )\n    return response\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.poll_onboard_dataset_until_finished","title":"poll_onboard_dataset_until_finished","text":"<pre><code>poll_onboard_dataset_until_finished(\n    dataset_id, poll_frequency=10\n)\n</code></pre> <p>Polls the status of an onboard dataset until it finishes processing. Defaults to a minimum of 5seconds poll frequency.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>The unique identifier of the dataset being onboarded.</p> required <code>poll_frequency</code> <code>int</code> <p>The time interval in seconds between polls. Defaults to 5 seconds.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The final status of the dataset, either \"Succeeded\" or \"Failed\".</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def poll_onboard_dataset_until_finished(self, dataset_id, poll_frequency=10):\n    \"\"\"\n    Polls the status of an onboard dataset until it finishes processing.\n    Defaults to a minimum of 5seconds poll frequency.\n\n    Args:\n        dataset_id (str): The unique identifier of the dataset being onboarded.\n        poll_frequency (int, optional): The time interval in seconds between polls. Defaults to 5 seconds.\n\n    Returns:\n        dict: The final status of the dataset, either \"Succeeded\" or \"Failed\".\n    \"\"\"\n    # Default to a minimum of 10 seconds poll frequency.\n    poll_frequency = 10 if poll_frequency &lt; 10 else poll_frequency\n    finished = False\n\n    while finished is False:\n        r = self.get_dataset(dataset_id)\n        status = r[\"status\"]\n        time_taken = (\n            datetime.now(timezone.utc)\n            - datetime.strptime(r[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%f%z\").replace(tzinfo=timezone.utc)\n        ).seconds\n\n        if status == \"Succeeded\":\n            print(status + \" - \" + str(time_taken) + \" seconds\")\n            finished = True\n            return r\n\n        elif status == \"Failed\":\n            print(status + \" - \" + str(time_taken) + \" seconds\")\n            finished = True\n            return r\n\n        else:\n            print(status + \" - \" + str(time_taken) + \" seconds\", end=\"\\r\")\n\n        sleep(poll_frequency)\n</code></pre>"},{"location":"fine_tuning/#geostudio.backends.v2.gtune.client.Client.poll_finetuning_until_finished","title":"poll_finetuning_until_finished","text":"<pre><code>poll_finetuning_until_finished(tune_id, poll_frequency=10)\n</code></pre> <p>Polls the status of a tune until it finishes or fails.</p> <p>Parameters:</p> Name Type Description Default <code>tune_id</code> <code>str</code> <p>The unique identifier of the tune to poll.</p> required <code>poll_frequency</code> <code>int</code> <p>The time interval in seconds between polls. Defaults to 5 seconds.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The final status of the tune, including details such as the number of epochs and any error messages if the tune failed.</p> Source code in <code>geostudio/backends/v2/gtune/client.py</code> <pre><code>def poll_finetuning_until_finished(self, tune_id, poll_frequency=10):\n    \"\"\"\n    Polls the status of a tune until it finishes or fails.\n\n    Args:\n        tune_id (str): The unique identifier of the tune to poll.\n        poll_frequency (int, optional): The time interval in seconds between polls. Defaults to 5 seconds.\n\n    Returns:\n        dict: The final status of the tune, including details such as the number of epochs and any error messages if the tune failed.\n    \"\"\"\n    # Default to a minimum of 10 seconds poll frequency.\n    poll_frequency = 10 if poll_frequency &lt; 10 else poll_frequency\n    finished = False\n\n    while finished is False:\n        r = self.get_tune(tune_id)\n        status = r[\"status\"]\n        time_taken = (\n            datetime.now(timezone.utc)\n            - datetime.strptime(r[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").replace(tzinfo=timezone.utc)\n        ).seconds\n\n        try:\n            m = self.get_tune_metrics(tune_id)\n            m_epochs = m.get(\"epochs\")\n        except Exception:\n            m_epochs = \"Unknown\"\n\n        if status == \"Finished\":\n            print(status + \" - Epoch: \" + str(m_epochs) + \" - \" + str(time_taken) + \" seconds\")\n            finished = True\n            return r\n\n        elif status == \"Failed\":\n            print(status + \" - Epoch: \" + str(m_epochs) + \" - \" + str(time_taken) + \" seconds\")\n            print(\"Download the logs from the link below:\")\n            print(r[\"logs_presigned_url\"])\n            finished = True\n            return r\n\n        else:\n            print(status + \" - Epoch: \" + str(m_epochs) + \" - \" + str(time_taken) + \" seconds\", end=\"\\r\")\n\n        sleep(poll_frequency)\n</code></pre>"},{"location":"inference/","title":"Inference","text":""},{"location":"inference/#geostudio.backends.v2.ginference.client","title":"geostudio.backends.v2.ginference.client","text":""},{"location":"inference/#geostudio.backends.v2.ginference.client.Client","title":"Client","text":"<pre><code>Client(\n    api_config: GeoFmSettings = None,\n    session: Session = None,\n    api_token: str = None,\n    api_key: str = None,\n    api_key_file: str = None,\n    geostudio_config_file: str = None,\n    *args,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseClient</code></p> <p>A client for interacting with the Geospatial Studio inference API endpoints</p> Source code in <code>geostudio/backends/base_client.py</code> <pre><code>def __init__(\n    self,\n    api_config: GeoFmSettings = None,\n    session: requests.Session = None,\n    api_token: str = None,\n    api_key: str = None,\n    api_key_file: str = None,\n    geostudio_config_file: str = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the GeoFmClient with the provided configuration.\n\n    Args:\n        api_config (GeoFmSettings, optional): The configuration settings for the GeoFm API. Defaults to None.\n        session (requests.Session, optional): A pre-configured requests session. Defaults to None.\n        api_token (str, optional): The API token for authentication. Defaults to None.\n        api_key (str, optional): The API key for authentication. Defaults to None.\n        api_key_file (str, optional): The path to the file containing the API key. Defaults to None.\n        geostudio_config_file (str): The file path to the geostudio config path containing api_key + base_urls.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Raises:\n        GeoFMException: If no API token, API key, or API key file is provided.\n\n    Attributes:\n        api_config (GeoFmSettings): The configuration settings for the GeoFm API.\n        session (requests.Session): A pre-configured requests session.\n        logger (logging.Logger): The logger instance for logging messages.\n    \"\"\"\n    self.api_config = api_config or GeoFmSettings()\n\n    if api_token:\n        print(\"Using api_token\")\n        api_token = api_token or GeoFmSettings.GEOFM_API_TOKEN\n        self.session = gfm_session(access_token=api_token)\n    elif api_key:\n        print(\"Using api_key from sdk command\")\n        self.session = gfm_session(api_key=api_key)\n    elif api_key_file:\n        if not os.path.isfile(api_key_file):\n            raise GeoFMException(\"Config file does not exist, Please provide a valid config file.\")\n        print(\"Using api_key from file\")\n        self.session = gfm_session(api_key_file=api_key_file)\n    elif geostudio_config_file:\n        if not os.path.isfile(geostudio_config_file):\n            raise GeoFMException(\"Config file does not exist, Please provide a valid config file.\")\n        print(\"Using api key and base urls from geostudio config file\")\n        geostudio_config_file_values = dotenv_values(geostudio_config_file)\n        settings.BASE_GATEWAY_API_URL = geostudio_config_file_values.get(\"BASE_GATEWAY_API_URL\", \"\")\n        settings.BASE_STUDIO_UI_URL = geostudio_config_file_values.get(\"BASE_STUDIO_UI_URL\", \"\")\n        settings.GEOSTUDIO_API_KEY = geostudio_config_file_values.get(\"GEOSTUDIO_API_KEY\", None)\n        self.session = gfm_session(api_key=settings.GEOSTUDIO_API_KEY)\n    else:\n        raise GeoFMException(\"Missing APIToken. Add `GEOFM_API_TOKEN` to env variables.\")\n\n    # else:\n    #     self.session = session or gfm_session(\n    #         client_id=self.api_config.ISV_CLIENT_ID,\n    #         client_secret=self.api_config.ISV_CLIENT_SECRET,\n    #         well_known_url=self.api_config.ISV_WELL_KNOWN,\n    #         userinfo_endpoint=self.api_config.ISV_USER_ENDPOINT,\n    #     )\n    self.logger = logging.getLogger()\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.create_model","title":"create_model","text":"<pre><code>create_model(data: ModelCreateInput, output: str = 'json')\n</code></pre> <p>Creates a new model using the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ModelCreateInput`</code> <p>The input data required to create a new model.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The response containing the created model Metadata.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def create_model(self, data: ModelCreateInput, output: str = \"json\"):\n    \"\"\"\n    Creates a new model using the provided data.\n\n    Args:\n        data (ModelCreateInput`): The input data required to create a new model.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: The response containing the created model Metadata.\n    \"\"\"\n    payload = json.loads(ModelCreateInput(**data).model_dump_json())\n    response = self.http_post(f\"{self.api_version}/models\", data=payload, output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.list_models","title":"list_models","text":"<pre><code>list_models(output: str = 'json')\n</code></pre> <p>Lists all available models.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the list of models.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def list_models(self, output: str = \"json\"):\n    \"\"\"\n    Lists all available models.\n\n    Args:\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary containing the list of models.\n    \"\"\"\n    response = self.http_get(\n        endpoint=f\"{self.api_version}/models?limit=1000&amp;skip=0\", output=output, data_field=\"results\"\n    )\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.update_model","title":"update_model","text":"<pre><code>update_model(\n    model_id: UUID,\n    data: ModelUpdateInput,\n    output: str = \"json\",\n)\n</code></pre> <p>Updates metadata of a specified model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>UUID</code> <p>The unique identifier of the model to be updated.</p> required <code>data</code> <code>dict</code> <p>A dictionary containing the new metadata for the model.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The response from the server containing the updated metadata.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def update_model(self, model_id: UUID, data: ModelUpdateInput, output: str = \"json\"):\n    \"\"\"\n    Updates metadata of a specified model.\n\n    Args:\n        model_id (UUID): The unique identifier of the model to be updated.\n        data (dict): A dictionary containing the new metadata for the model.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: The response from the server containing the updated metadata.\n    \"\"\"\n    payload = json.loads(ModelUpdateInput(**data).model_dump_json())\n    response = self.http_patch(\n        f\"{self.api_version}/models/{model_id}\", data=payload, output=output, data_field=\"results\"\n    )\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.deploy_model","title":"deploy_model","text":"<pre><code>deploy_model(\n    model_id: str,\n    data: ModelOnboardingInputSchema,\n    output=\"json\",\n)\n</code></pre> <p>Deploys a model</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The unique identifier of the model to be deployed</p> required <code>data</code> <code>ModelOnboardingInputSchema</code> <p>Urls to the model checkpoint and configs</p> required Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def deploy_model(self, model_id: str, data: ModelOnboardingInputSchema, output=\"json\"):\n    \"\"\"\n    Deploys a model\n\n    Args:\n        model_id (str): The unique identifier of the model to be deployed\n        data (ModelOnboardingInputSchema): Urls to the model checkpoint and configs\n\n    \"\"\"\n    payload = json.loads(ModelOnboardingInputSchema(**data).model_dump_json())\n    response = self.http_post(\n        f\"{self.api_version}/models/{model_id}/deploy\", data=payload, output=output, data_field=\"results\"\n    )\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.get_model","title":"get_model","text":"<pre><code>get_model(model_id: UUID, output: str = 'json')\n</code></pre> <p>Retrieves a model's information using its ID.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>UUID</code> <p>The unique identifier of the model to retrieve.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The model's status and information</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def get_model(self, model_id: UUID, output: str = \"json\"):\n    \"\"\"\n    Retrieves a model's information using its ID.\n\n    Parameters:\n        model_id (UUID): The unique identifier of the model to retrieve.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: The model's status and information\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/models/{model_id}\", output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.delete_model","title":"delete_model","text":"<pre><code>delete_model(model_id: str, output: str = 'json')\n</code></pre> <p>Deletes a specified model using its ID.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model to be deleted.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The response from the server after deleting the model.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def delete_model(self, model_id: str, output: str = \"json\"):\n    \"\"\"\n    Deletes a specified model using its ID.\n\n    Args:\n        model_id (str): The ID of the model to be deleted.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: The response from the server after deleting the model.\n    \"\"\"\n    response = self.http_delete(f\"{self.api_version}/models/{model_id}\", output=output)\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.submit_inference","title":"submit_inference","text":"<pre><code>submit_inference(\n    data: InferenceCreateInput, output: str = \"json\"\n)\n</code></pre> <p>Submits an inference task to the server.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>InferenceCreateInput</code> <p>The input data for the inference task.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The server's response containing the results of the inference task.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def submit_inference(self, data: InferenceCreateInput, output: str = \"json\"):\n    \"\"\"\n    Submits an inference task to the server.\n\n    Args:\n        data (InferenceCreateInput): The input data for the inference task.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: The server's response containing the results of the inference task.\n    \"\"\"\n    payload = json.loads(InferenceCreateInput(**data).model_dump_json())\n    response = self.http_post(f\"{self.api_version}/inference\", data=payload, output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.list_inferences","title":"list_inferences","text":"<pre><code>list_inferences(output: str = 'json')\n</code></pre> <p>Lists inferences submitted to the Studio. Limit to most recent 10.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing a list of inference tasks submitted to the studio</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def list_inferences(self, output: str = \"json\"):\n    \"\"\"\n    Lists inferences submitted to the Studio. Limit to most recent 10.\n\n    Args:\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary containing a list of inference tasks submitted to the studio\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/inference?limit=10&amp;skip=0\", output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.get_inference","title":"get_inference","text":"<pre><code>get_inference(inference_id: UUID, output: str = 'json')\n</code></pre> <p>Retrieves the inference with the given inference ID.</p> <p>Parameters:</p> Name Type Description Default <code>inference_id</code> <code>UUID</code> <p>The unique identifier of the inference task.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The inference task data in the specified output format.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def get_inference(self, inference_id: UUID, output: str = \"json\"):\n    \"\"\"\n    Retrieves the inference with the given inference ID.\n\n    Args:\n        inference_id (uuid.UUID): The unique identifier of the inference task.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: The inference task data in the specified output format.\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/inference/{inference_id}\", output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.delete_inference","title":"delete_inference","text":"<pre><code>delete_inference(inference_id: UUID, output: str = 'json')\n</code></pre> <p>Deletes an inference using its ID.</p> <p>Parameters:</p> Name Type Description Default <code>inference_id</code> <code>UUID</code> <p>The ID of the inference to be deleted.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The response from the server after deleting the inference.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def delete_inference(self, inference_id: UUID, output: str = \"json\"):\n    \"\"\"\n    Deletes an inference using its ID.\n\n    Args:\n        inference_id (uuid.UUID): The ID of the inference to be deleted.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: The response from the server after deleting the inference.\n    \"\"\"\n    response = self.http_delete(f\"{self.api_version}/inference/{inference_id}\", output=output)\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.get_inference_tasks","title":"get_inference_tasks","text":"<pre><code>get_inference_tasks(\n    inference_id: UUID, output: str = \"json\"\n)\n</code></pre> <p>Retrieves the tasks associated with an inference.</p> <p>Parameters:</p> Name Type Description Default <code>inference_id</code> <code>UUID</code> <p>The unique identifier of the inference.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The inference task data in the specified output format.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def get_inference_tasks(self, inference_id: UUID, output: str = \"json\"):\n    \"\"\"\n    Retrieves the tasks associated with an inference.\n\n    Args:\n        inference_id (uuid.UUID): The unique identifier of the inference.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: The inference task data in the specified output format.\n    \"\"\"\n    response = self.http_get(\n        f\"{self.api_version}/inference/{inference_id}/tasks\", output=output, data_field=\"results\"\n    )\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.get_task_output_url","title":"get_task_output_url","text":"<pre><code>get_task_output_url(task_id: UUID, output: str = 'json')\n</code></pre> <p>Retrieves the output url for a specific inference task.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>UUID</code> <p>The unique identifier of the task.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The inference task data in the specified output format.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def get_task_output_url(self, task_id: UUID, output: str = \"json\"):\n    \"\"\"\n    Retrieves the output url for a specific inference task.\n\n    Args:\n        task_id (UUID): The unique identifier of the task.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: The inference task data in the specified output format.\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/tasks/{task_id}/output\", output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.get_task_step_logs","title":"get_task_step_logs","text":"<pre><code>get_task_step_logs(\n    task_id: UUID, step_id: str, output: str = \"json\"\n)\n</code></pre> <p>Retrieves the logs for a specific step of an inference task.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>UUID</code> <p>The unique identifier of the task.</p> required <code>output</code> <code>str</code> <p>The desired output format. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The inference task data in the specified output format.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def get_task_step_logs(self, task_id: UUID, step_id: str, output: str = \"json\"):\n    \"\"\"\n    Retrieves the logs for a specific step of an inference task.\n\n    Args:\n        task_id (UUID): The unique identifier of the task.\n        output (str, optional): The desired output format. Defaults to \"json\".\n\n    Returns:\n        dict: The inference task data in the specified output format.\n    \"\"\"\n    response = self.http_get(\n        f\"{self.api_version}/tasks/{task_id}/logs/{step_id}\", output=output, data_field=\"results\"\n    )\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.check_data_availability","title":"check_data_availability","text":"<pre><code>check_data_availability(\n    datasource: str,\n    data: DataAdvisorIn,\n    output: str = \"json\",\n)\n</code></pre> <p>Query data-advisor service to check data availability before running an inference.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dictionary containing the necessary parameters for the data availability check.</p> required <code>output</code> <code>str</code> <p>The desired output format. Default is \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The response from the server containing the data availability information.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def check_data_availability(self, datasource: str, data: DataAdvisorIn, output: str = \"json\"):\n    \"\"\"\n    Query data-advisor service to check data availability before running an inference.\n\n    Args:\n        data (dict): A dictionary containing the necessary parameters for the data availability check.\n        output (str, optional): The desired output format. Default is \"json\".\n\n    Returns:\n        dict: The response from the server containing the data availability information.\n    \"\"\"\n    payload = json.loads(DataAdvisorIn(**data).model_dump_json())\n    response = self.http_post(\n        f\"{self.api_version}/data-advice/{datasource}\", data=payload, output=output, data_field=\"results\"\n    )\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.list_datasource_collections","title":"list_datasource_collections","text":"<pre><code>list_datasource_collections(\n    datasource: str, output: str = \"json\"\n)\n</code></pre> <p>Query data-advisor to list collections available for a specific data source</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def list_datasource_collections(self, datasource: str, output: str = \"json\"):\n    \"\"\"\n    Query data-advisor to list collections available for a specific data source\n    \"\"\"\n    response = self.http_get(f\"{self.api_version}/data-advice/{datasource}\", output=output, data_field=\"results\")\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.list_datasource","title":"list_datasource","text":"<pre><code>list_datasource(\n    connector: str = None,\n    collection: str = None,\n    limit: int = 25,\n    skip: int = 0,\n    output: str = \"json\",\n)\n</code></pre> <p>Lists all data sources available in the studio.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing a list of data sources available through the studio</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def list_datasource(\n    self, connector: str = None, collection: str = None, limit: int = 25, skip: int = 0, output: str = \"json\"\n):\n    \"\"\"\n    Lists all data sources available in the studio.\n\n    Args:\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: A dictionary containing a list of data sources available through the studio\n    \"\"\"\n    url = [f\"limit={limit}\", f\"skip={skip}\"]\n\n    if connector:\n        url.append(f\"connector={connector}\")\n    if collection:\n        url.append(f\"collection={collection}\")\n    response = self.http_get(\n        f\"{self.api_version}/data-sources?{'&amp;'.join(url)}\", output=output, data_field=\"results\"\n    )\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.get_datasource","title":"get_datasource","text":"<pre><code>get_datasource(datasource_id: UUID, output: str = 'json')\n</code></pre> <p>Retrieves a specific data source's information.</p> <p>Parameters:</p> Name Type Description Default <code>datasource_id</code> <code>UUID</code> <p>The unique identifier of the data source to retrieve.</p> required <code>output</code> <code>str</code> <p>The format of the response. Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The response from the server containing the data source details.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def get_datasource(self, datasource_id: UUID, output: str = \"json\"):\n    \"\"\"\n    Retrieves a specific data source's information.\n\n    Args:\n        datasource_id (UUID): The unique identifier of the data source to retrieve.\n        output (str, optional): The format of the response. Defaults to \"json\".\n\n    Returns:\n        dict: The response from the server containing the data source details.\n    \"\"\"\n    results = self.list_datasource()[\"results\"]\n    data_source = list(filter(lambda x: x[\"id\"] == datasource_id, results))\n    return data_source\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.get_fileshare_links","title":"get_fileshare_links","text":"<pre><code>get_fileshare_links(object_name: str)\n</code></pre> <p>Generate presigned urls for sharing files i.e uploading and downloading files.</p> <p>Parameters:</p> Name Type Description Default <code>object_name</code> <code>str</code> <p>The name of the object (file) for which to generate upload links.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the upload links.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def get_fileshare_links(self, object_name: str):\n    \"\"\"\n    Generate presigned urls for sharing files i.e uploading and downloading files.\n\n    Args:\n        object_name (str): The name of the object (file) for which to generate upload links.\n\n    Returns:\n        dict: A dictionary containing the upload links.\n    \"\"\"\n    print(\"Going to generate the upload link\")\n    response = self.http_get(\n        f\"{self.api_version}/file-share?object_name={object_name}\", output=\"json\", data_field=\"results\"\n    )\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.upload_file_to_url","title":"upload_file_to_url","text":"<pre><code>upload_file_to_url(upload_url: str, filepath: str)\n</code></pre> <p>Uploads a file to a specified URL using a PUT request.</p> <p>Parameters:</p> Name Type Description Default <code>upload_url</code> <code>str</code> <p>The URL to which the file will be uploaded.</p> required <code>filepath</code> <code>str</code> <p>The path to the file that will be uploaded.</p> required <p>Returns:</p> Type Description <p>requests.Response: The response from the server after the file upload.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def upload_file_to_url(self, upload_url: str, filepath: str):\n    \"\"\"\n    Uploads a file to a specified URL using a PUT request.\n\n    Args:\n        upload_url (str): The URL to which the file will be uploaded.\n        filepath (str): The path to the file that will be uploaded.\n\n    Returns:\n        requests.Response: The response from the server after the file upload.\n    \"\"\"\n\n    print(\"Going to upload the file to the url.\")\n\n    fields = {}\n    path = Path(filepath)\n    total_size = path.stat().st_size\n    filename = path.name\n\n    with Progress(\n        TextColumn(\"[bold black]{task.description}\"),\n        BarColumn(),\n        DownloadColumn(),\n        TransferSpeedColumn(),\n        TimeRemainingColumn(),\n    ) as progress_bar:\n        task = progress_bar.add_task(filename, total=total_size)\n        with open(filepath, \"rb\") as f:\n            fields[\"file\"] = (\"filename\", f)\n            e = MultipartEncoder(fields=fields)\n            last_bytes = 0\n\n            def monitor_callback(monitor):\n                nonlocal last_bytes\n                bytes_diff = monitor.bytes_read - last_bytes\n                progress_bar.update(task, advance=bytes_diff)\n                last_bytes = monitor.bytes_read\n\n            m = MultipartEncoderMonitor(e, monitor_callback)\n\n            headers = {\"Content-Type\": m.content_type}\n            response = requests.put(upload_url, data=m, headers=headers)\n    return response\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.upload_file","title":"upload_file","text":"<pre><code>upload_file(filename: str)\n</code></pre> <p>Streamlines :py:meth:<code>get_upload_links</code> and :py:meth:<code>upload_file_to_url</code>. Uploads a file to a specified location using the provided upload links.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def upload_file(self, filename: str):\n    \"\"\"\n    Streamlines :py:meth:`get_upload_links` and :py:meth:`upload_file_to_url`.\n    Uploads a file to a specified location using the provided upload links.\n    \"\"\"\n    links = self.get_fileshare_links(object_name=filename.split(\"/\")[-1])\n    # print(links)\n    upload_url = links.get(\"upload_url\", None)\n    if upload_url:\n        self.upload_file_to_url(links[\"upload_url\"], filename)\n    return links\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.create_download_presigned_url","title":"create_download_presigned_url","text":"<pre><code>create_download_presigned_url(\n    bucket_name: str,\n    object_key: str,\n    endpoint_url: str,\n    region_name: str,\n    service_name: str,\n    aws_access_key_id: str = None,\n    aws_secret_access_key: str = None,\n    expiration: int = 3600,\n    **kwargs\n)\n</code></pre> <p>Function to create presigned url to download object from bucket</p>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.create_download_presigned_url--parameters","title":"Parameters","text":"<p>bucket_name : str     The bucket name in the instance object_key : str     Object path to pre-sign endpoint_url: str     s3 Endpoint i.e https://s3.us-east.cloud-object-storage.appdomain.cloud region_name: str     Region where bucket lives. i.e us-east service_name: str     service to connect to i.e s3 aws_access_key_id: str     AWS Access key to the instance aws_secret_access_key: str     AWS secret access key to the instance expiration : int, optional     Expiration duration in seconds, by default 3600</p>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.create_download_presigned_url--returns","title":"Returns","text":"<p>str     Presigned download url</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def create_download_presigned_url(\n    self,\n    bucket_name: str,\n    object_key: str,\n    endpoint_url: str,\n    region_name: str,\n    service_name: str,\n    aws_access_key_id: str = None,\n    aws_secret_access_key: str = None,\n    expiration: int = 3600,\n    **kwargs,\n):\n    \"\"\"Function to create presigned url to download object from bucket\n\n    Parameters\n    ----------\n    bucket_name : str\n        The bucket name in the instance\n    object_key : str\n        Object path to pre-sign\n    endpoint_url: str\n        s3 Endpoint i.e https://s3.us-east.cloud-object-storage.appdomain.cloud\n    region_name: str\n        Region where bucket lives. i.e us-east\n    service_name: str\n        service to connect to i.e s3\n    aws_access_key_id: str\n        AWS Access key to the instance\n    aws_secret_access_key: str\n        AWS secret access key to the instance\n    expiration : int, optional\n        Expiration duration in seconds, by default 3600\n\n    Returns\n    -------\n    str\n        Presigned download url\n    \"\"\"\n\n    s3_client = boto3.client(\n        service_name,\n        region_name=region_name,\n        endpoint_url=endpoint_url,\n        aws_secret_access_key=aws_secret_access_key,\n        aws_access_key_id=aws_access_key_id,\n        **kwargs,\n    )\n    try:\n        download_url = s3_client.generate_presigned_url(\n            ClientMethod=\"get_object\",\n            Params={\"Bucket\": bucket_name, \"Key\": object_key},\n            ExpiresIn=expiration,\n        )\n    except ClientError as e:\n        print(f\"Error creating presigned URL: {e}\")\n        return None\n\n    return download_url\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.create_upload_presigned_url","title":"create_upload_presigned_url","text":"<pre><code>create_upload_presigned_url(\n    bucket_name: str,\n    object_key: str,\n    endpoint_url: str,\n    region_name: str,\n    service_name: str,\n    aws_access_key_id: str = None,\n    aws_secret_access_key: str = None,\n    expiration: int = 3600,\n    **kwargs\n)\n</code></pre> <p>Function to create presigned url to upload object from bucket</p>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.create_upload_presigned_url--parameters","title":"Parameters","text":"<p>bucket_name : str     The bucket name in the instance object_key : str     Object path to pre-sign endpoint_url: str     s3 Endpoint i.e https://s3.us-east.cloud-object-storage.appdomain.cloud region_name: str     Region where bucket lives. i.e us-east service_name: str     service to connect to i.e s3 aws_access_key_id: str     AWS Access key to the instance aws_secret_access_key: str     AWS secret access key to the instance expiration : int, optional     Expiration duration in seconds, by default 3600</p>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.create_upload_presigned_url--returns","title":"Returns","text":"<p>str     Presigned upload url</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def create_upload_presigned_url(\n    self,\n    bucket_name: str,\n    object_key: str,\n    endpoint_url: str,\n    region_name: str,\n    service_name: str,\n    aws_access_key_id: str = None,\n    aws_secret_access_key: str = None,\n    expiration: int = 3600,\n    **kwargs,\n):\n    \"\"\"Function to create presigned url to upload object from bucket\n\n    Parameters\n    ----------\n    bucket_name : str\n        The bucket name in the instance\n    object_key : str\n        Object path to pre-sign\n    endpoint_url: str\n        s3 Endpoint i.e https://s3.us-east.cloud-object-storage.appdomain.cloud\n    region_name: str\n        Region where bucket lives. i.e us-east\n    service_name: str\n        service to connect to i.e s3\n    aws_access_key_id: str\n        AWS Access key to the instance\n    aws_secret_access_key: str\n        AWS secret access key to the instance\n    expiration : int, optional\n        Expiration duration in seconds, by default 3600\n\n    Returns\n    -------\n    str\n        Presigned upload url\n    \"\"\"\n\n    s3_client = boto3.client(\n        service_name,\n        region_name=region_name,\n        endpoint_url=endpoint_url,\n        aws_secret_access_key=aws_secret_access_key,\n        aws_access_key_id=aws_access_key_id,\n        **kwargs,\n    )\n    try:\n        upload_url = s3_client.generate_presigned_url(\n            ClientMethod=\"put_object\",\n            Params={\"Bucket\": bucket_name, \"Key\": object_key},\n            ExpiresIn=expiration,\n        )\n    except ClientError as e:\n        print(f\"Error creating presigned URL: {e}\")\n        return None\n\n    return upload_url\n</code></pre>"},{"location":"inference/#geostudio.backends.v2.ginference.client.Client.poll_inference_until_finished","title":"poll_inference_until_finished","text":"<pre><code>poll_inference_until_finished(\n    inference_id, poll_frequency=10\n)\n</code></pre> <p>Polls the status of an inference task until it is completed or failed. Defaults to a minimum of 5seconds poll frequency.</p> <p>Parameters:</p> Name Type Description Default <code>inference_id</code> <code>str</code> <p>The unique identifier of the inference task.</p> required <code>poll_frequency</code> <code>int</code> <p>The time interval in seconds between polls. Defaults to 5 seconds.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The response from the inference task when it is completed or failed.</p> Source code in <code>geostudio/backends/v2/ginference/client.py</code> <pre><code>def poll_inference_until_finished(self, inference_id, poll_frequency=10):\n    \"\"\"\n    Polls the status of an inference task until it is completed or failed.\n    Defaults to a minimum of 5seconds poll frequency.\n\n    Args:\n        inference_id (str): The unique identifier of the inference task.\n        poll_frequency (int, optional): The time interval in seconds between polls. Defaults to 5 seconds.\n\n    Returns:\n        dict: The response from the inference task when it is completed or failed.\n    \"\"\"\n    poll_frequency = 10 if poll_frequency &lt; 10 else poll_frequency\n    finished = False\n\n    while finished is False:\n        r = self.get_inference(inference_id)\n        status = r[\"status\"]\n        time_taken = (\n            datetime.now(timezone.utc)\n            - datetime.strptime(r[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").replace(tzinfo=timezone.utc)\n        ).seconds\n\n        if \"COMPLETED\" in status:\n            print(status + \" - \" + str(time_taken) + \" seconds\")\n            finished = True\n            return r\n\n        elif status == \"FAILED\":\n            print(status + \" - \" + str(time_taken) + \" seconds\")\n            finished = True\n            return r\n\n        elif status == \"STOPPED\":\n            print(status + \" - \" + str(time_taken) + \" seconds\")\n            finished = True\n            return r\n\n        else:\n            print(status + \" - \" + str(time_taken) + \" seconds\", end=\"\\r\")\n\n        sleep(poll_frequency)\n</code></pre>"},{"location":"models/","title":"Models","text":""},{"location":"models/#geostudio.backends.v2.ginference.models","title":"geostudio.backends.v2.ginference.models","text":""},{"location":"models/#geostudio.backends.v2.ginference.models.ModelOnboardingInputSchema","title":"ModelOnboardingInputSchema","text":"<p>               Bases: <code>BaseModel</code></p> Used by: <ul> <li> SDK Client Inference client Client deploy_model </li> </ul>"},{"location":"models/#geostudio.backends.v2.ginference.models.ModelUpdateInput","title":"ModelUpdateInput","text":"<p>               Bases: <code>BaseModel</code></p> Subclassed by: <ul> <li> Models Models models ModelCreateInput </li> </ul> Used by: <ul> <li> SDK Client Inference client Client update_model </li> </ul>"},{"location":"models/#geostudio.backends.v2.ginference.models.ModelCreateInput","title":"ModelCreateInput","text":"<p>               Bases: <code>ModelUpdateInput</code></p> Used by: <ul> <li> SDK Client Inference client Client create_model </li> </ul>"},{"location":"models/#geostudio.backends.v2.ginference.models.SpatialDomain","title":"SpatialDomain","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"models/#geostudio.backends.v2.ginference.models.DataSource","title":"DataSource","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"models/#geostudio.backends.v2.ginference.models.GeoServerPush","title":"GeoServerPush","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"models/#geostudio.backends.v2.ginference.models.PostProcessing","title":"PostProcessing","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"models/#geostudio.backends.v2.ginference.models.InferenceConfig","title":"InferenceConfig","text":"<p>               Bases: <code>BaseModel</code></p> Subclassed by: <ul> <li> Models Models models InferenceCreateInput </li> </ul>"},{"location":"models/#geostudio.backends.v2.ginference.models.InferenceCreateInput","title":"InferenceCreateInput","text":"<p>               Bases: <code>InferenceConfig</code></p> Used by: <ul> <li> SDK Client Inference client Client submit_inference </li> </ul>"},{"location":"models/#geostudio.backends.v2.ginference.models.DataAdvisorIn","title":"DataAdvisorIn","text":"<p>               Bases: <code>BaseModel</code></p> Used by: <ul> <li> SDK Client Inference client Client check_data_availability </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models","title":"geostudio.backends.v2.gtune.models","text":""},{"location":"models/#geostudio.backends.v2.gtune.models.TuneUpdateIn","title":"TuneUpdateIn","text":"<p>               Bases: <code>BaseModel</code></p> Used by: <ul> <li> SDK Client Fine-tuning client Client update_tune </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models.TuneSubmitBase","title":"TuneSubmitBase","text":"<p>               Bases: <code>BaseModel</code></p> Subclassed by: <ul> <li> Models Models models TuneSubmitIn </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models.TuneSubmitBase.validate_name","title":"validate_name  <code>classmethod</code>","text":"<pre><code>validate_name(name: str) -&gt; str\n</code></pre> <p>Validates the tune name</p>"},{"location":"models/#geostudio.backends.v2.gtune.models.TuneSubmitBase.validate_name--parameters","title":"Parameters","text":"<p>name : str     The name of the tune.</p>"},{"location":"models/#geostudio.backends.v2.gtune.models.TuneSubmitBase.validate_name--returns","title":"Returns","text":"<p>str     Cleaned up name of the tune without special characters or white spaces.</p>"},{"location":"models/#geostudio.backends.v2.gtune.models.TuneSubmitBase.validate_name--raises","title":"Raises","text":"<p>ValueError      If <code>name</code> contains special characters or white spaces.</p> Source code in <code>geostudio/backends/v2/gtune/models.py</code> <pre><code>@field_validator(\"name\")\n@classmethod\ndef validate_name(cls, name: str) -&gt; str:\n    \"\"\"Validates the tune name\n\n    Parameters\n    ----------\n    name : str\n        The name of the tune.\n\n    Returns\n    -------\n    str\n        Cleaned up name of the tune without special characters or white spaces.\n\n    Raises\n    ------\n    ValueError\n         If `name` contains special characters or white spaces.\n    \"\"\"\n    # Clean-up the tune name.\n    name = name.replace(\" \", \"-\").replace(\"_\", \"-\").strip()\n    if not re.match(\"^[a-zA-Z0-9]+([.-]{0,1}[a-zA-Z0-9]+)*$\", name):\n        raise ValueError(\"must not contain special characters or white spaces. Replace underscores with hyphens.\")\n    return name\n</code></pre>"},{"location":"models/#geostudio.backends.v2.gtune.models.TuneSubmitIn","title":"TuneSubmitIn","text":"<p>               Bases: <code>TuneSubmitBase</code></p> Used by: <ul> <li> SDK Client Fine-tuning client Client submit_tune </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models.HpoTuneSubmitIn","title":"HpoTuneSubmitIn","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for hpo tune submission.</p> Used by: <ul> <li> SDK Client Fine-tuning client Client submit_hpo_tune </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models.TryOutTuneInput","title":"TryOutTuneInput","text":"<p>               Bases: <code>BaseModel</code></p> Used by: <ul> <li> SDK Client Fine-tuning client Client try_out_tune </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models.UploadTuneInput","title":"UploadTuneInput","text":"<p>               Bases: <code>BaseModel</code></p> Used by: <ul> <li> SDK Client Fine-tuning client Client upload_completed_tunes </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models.TaskPurposeEnum","title":"TaskPurposeEnum","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p>"},{"location":"models/#geostudio.backends.v2.gtune.models.TaskIn","title":"TaskIn","text":"<p>               Bases: <code>BaseModel</code></p> Used by: <ul> <li> SDK Client Fine-tuning client Client create_task </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models.PreScanDatasetIn","title":"PreScanDatasetIn","text":"<p>               Bases: <code>BaseModel</code></p> Used by: <ul> <li> SDK Client Fine-tuning client Client pre_scan_dataset </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models.DatasetUpdateIn","title":"DatasetUpdateIn","text":"<p>               Bases: <code>BaseModel</code></p> Used by: <ul> <li> SDK Client Fine-tuning client Client update_dataset </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models.GeoDatasetTrainParamUpdateSchema","title":"GeoDatasetTrainParamUpdateSchema","text":"<p>               Bases: <code>BaseModel</code></p> Subclassed by: <ul> <li> Models Models models DatasetOnboardIn </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models.DatasetOnboardIn","title":"DatasetOnboardIn","text":"<p>               Bases: <code>GeoDatasetTrainParamUpdateSchema</code></p> Used by: <ul> <li> SDK Client Fine-tuning client Client onboard_dataset </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models.ModelCategory","title":"ModelCategory","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p>"},{"location":"models/#geostudio.backends.v2.gtune.models.BaseModelParamsIn","title":"BaseModelParamsIn","text":"<p>               Bases: <code>BaseModel</code></p> Used by: <ul> <li> SDK Client Fine-tuning client Client update_base_model_params </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models.BaseModelsIn","title":"BaseModelsIn","text":"<p>               Bases: <code>BaseModel</code></p> Used by: <ul> <li> SDK Client Fine-tuning client Client create_base_model </li> </ul>"},{"location":"models/#geostudio.backends.v2.gtune.models.BaseModelParamsIn","title":"BaseModelParamsIn","text":"<p>               Bases: <code>BaseModel</code></p> Used by: <ul> <li> SDK Client Fine-tuning client Client update_base_model_params </li> </ul>"},{"location":"widgets/","title":"Widgets","text":""},{"location":"widgets/#geostudio.gswidgets","title":"geostudio.gswidgets","text":""},{"location":"widgets/#geostudio.gswidgets.geojson_to_details","title":"geojson_to_details","text":"<pre><code>geojson_to_details(geojson)\n</code></pre> <p>This function takes a GeoJSON object as input and returns a string containing the area, perimeter, and bounding box coordinates of the polygon.</p> <p>Parameters:</p> Name Type Description Default <code>geojson</code> <code>dict</code> <p>A dictionary representing a GeoJSON object with 'geometry' and 'type' keys.          The 'geometry' key should contain a dictionary with 'type' set to 'Polygon' and 'coordinates' containing a list of coordinate pairs.          The 'type' key should be set to 'Feature'.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A string containing the area (in square kilometers), perimeter (in kilometers), and bounding box coordinates (in decimal degrees) of the polygon.</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def geojson_to_details(geojson):\n    \"\"\"\n    This function takes a GeoJSON object as input and returns a string containing the area, perimeter, and bounding box coordinates of the polygon.\n\n    Args:\n        geojson (dict): A dictionary representing a GeoJSON object with 'geometry' and 'type' keys.\n                     The 'geometry' key should contain a dictionary with 'type' set to 'Polygon' and 'coordinates' containing a list of coordinate pairs.\n                     The 'type' key should be set to 'Feature'.\n\n    Returns:\n        str: A string containing the area (in square kilometers), perimeter (in kilometers), and bounding box coordinates (in decimal degrees) of the polygon.\n    \"\"\"\n\n    print(geojson.get(\"geometry\"))\n    # gdf4326 = gpd.GeoDataFrame(index=[0], crs='epsg:4326', geometry=[geojson.get('geometry')])\n    gdf4326 = gpd.GeoDataFrame.from_features([geojson], crs=\"epsg:4326\")\n\n    # Get the geometry from `gdf4326`\n    pgon = gdf4326.geometry.iloc[0]\n    # Extract list of longitude/latitude of polygon's boundary\n    lons, lats = pgon.exterior.xy[:][0], pgon.exterior.xy[:][1]\n\n    geod = Geod(\"+a=6378137 +f=0.0033528106647475126\")\n    poly_area, poly_perimeter = geod.polygon_area_perimeter(lons, lats)\n\n    bbox_list = [round(pgon.bounds[0], 5), round(pgon.bounds[1], 5), round(pgon.bounds[2], 5), round(pgon.bounds[3], 5)]\n\n    if bbox_list[0] &gt; 180:\n        bbox_list[0] = round(bbox_list[0] - 360, 5)\n    if bbox_list[2] &gt; 180:\n        bbox_list[2] = round(bbox_list[2] - 360, 5)\n\n    # Print the results\n    output_details = \"Area, (sq.km): {:.1f}\".format(abs(poly_area) / 1000000) + \"\\n\"\n    output_details = output_details + \"Perimeter, (km): {:.2f}\".format(poly_perimeter / 1000) + \"\\n\"\n    output_details = output_details + \"Bounding box coordinates: {}\".format(str(bbox_list))\n\n    return output_details\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.geojson_to_bbox","title":"geojson_to_bbox","text":"<pre><code>geojson_to_bbox(geojson)\n</code></pre> <p>Convert a GeoJSON feature to a bounding box in the format [west, south, east, north].</p> <p>Parameters:</p> Name Type Description Default <code>geojson</code> <code>dict</code> <p>A dictionary representing a GeoJSON feature. It should contain a 'geometry' key with a GeoJSON geometry object.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of four floating point numbers representing the bounding box in the order [west, south, east, north].</p> <p>The values are rounded to 5 decimal places. Longitude and latitude values are adjusted to be within the range [-180, 180].</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def geojson_to_bbox(geojson):\n    \"\"\"\n    Convert a GeoJSON feature to a bounding box in the format [west, south, east, north].\n\n    Args:\n        geojson (dict): A dictionary representing a GeoJSON feature. It should contain a 'geometry' key with a GeoJSON geometry object.\n\n    Returns:\n        list: A list of four floating point numbers representing the bounding box in the order [west, south, east, north].\n        The values are rounded to 5 decimal places. Longitude and latitude values are adjusted to be within the range [-180, 180].\n    \"\"\"\n\n    gdf4326 = gpd.GeoDataFrame.from_features([geojson], crs=\"epsg:4326\")\n\n    # Get the geometry from `gdf4326`\n    pgon = gdf4326.geometry.iloc[0]\n    bbox_list = [round(pgon.bounds[0], 5), round(pgon.bounds[1], 5), round(pgon.bounds[2], 5), round(pgon.bounds[3], 5)]\n\n    if bbox_list[0] &gt; 180:\n        bbox_list[0] = bbox_list[0] - 360\n    if bbox_list[2] &gt; 180:\n        bbox_list[2] = bbox_list[2] - 360\n\n    return bbox_list\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.list_output_files","title":"list_output_files","text":"<pre><code>list_output_files(url, just_tif=True)\n</code></pre> <p>Lists the files present in a remote zip archive.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the remote zip archive.</p> required <code>just_tif</code> <code>bool</code> <p>If True, only return files with '.tif' extension. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <p>List[str]: A list of filenames present in the zip archive.</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def list_output_files(url, just_tif=True):\n    \"\"\"\n    Lists the files present in a remote zip archive.\n\n    Args:\n        url (str): The URL of the remote zip archive.\n        just_tif (bool, optional): If True, only return files with '.tif' extension. Defaults to True.\n\n    Returns:\n        List[str]: A list of filenames present in the zip archive.\n    \"\"\"\n    with RemoteZip(url) as zip:\n        il = zip.infolist()\n    if just_tif == True:\n        return [X.filename for X in il if X.filename[-4:] == \".tif\"]\n    else:\n        return [X.filename for X in il]\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.download_file","title":"download_file","text":"<pre><code>download_file(url, filename, output_path='./')\n</code></pre> <p>Downloads a file from a given URL and saves it to a specified output path.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to download.</p> required <code>filename</code> <code>str</code> <p>The name of the file to save locally.</p> required <code>output_path</code> <code>str</code> <p>The directory where the file will be saved. Defaults to './'.</p> <code>'./'</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The path where the file was saved.</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def download_file(url, filename, output_path=\"./\"):\n    \"\"\"\n    Downloads a file from a given URL and saves it to a specified output path.\n\n    Args:\n        url (str): The URL of the file to download.\n        filename (str): The name of the file to save locally.\n        output_path (str, optional): The directory where the file will be saved. Defaults to './'.\n\n    Returns:\n        str: The path where the file was saved.\n    \"\"\"\n    with RemoteZip(url) as zip:\n        a = zip.extract(filename, path=output_path)\n    return a\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.bboxSelector","title":"bboxSelector","text":"<pre><code>bboxSelector()\n</code></pre> <p>Creates a user interface for selecting a bounding box on a map.</p> <p>Returns:</p> Type Description <p>ipywidgets.widgets.GridspecLayout: A grid layout containing that can be displayed in a Jupyter notebook or similar environment. It contains a map, a search control, a full screen control, a title, a text area for displaying bounding box details,</p> <p>a clear button, and a use button. The map has a draw control for selecting a bounding box.</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def bboxSelector():\n    \"\"\"\n    Creates a user interface for selecting a bounding box on a map.\n\n    Args:\n        None\n\n    Returns:\n        ipywidgets.widgets.GridspecLayout: A grid layout containing that can be displayed in a Jupyter notebook or similar environment. It contains a map, a search control, a full screen control, a title, a text area for displaying bounding box details,\n        a clear button, and a use button. The map has a draw control for selecting a bounding box.\n    \"\"\"\n    bbox = []\n\n    grid = widgets.GridspecLayout(4, 4, height=\"400px\")\n\n    m = Map(center=(50, 354), zoom=5, scroll_wheel_zoom=True)\n\n    draw_control = DrawControl(polyline={}, circle={}, polygon={}, circlemarker={})\n\n    draw_control.rectangle = {\"shapeOptions\": {\"fillColor\": \"#fca45d\", \"color\": \"#fca45d\", \"fillOpacity\": 0.3}}\n\n    m.add(SearchControl(position=\"topleft\", url=\"https://nominatim.openstreetmap.org/search?format=json&amp;q={s}\", zoom=8))\n\n    m.add(FullScreenControl())\n\n    grid[:, 1:4] = m\n\n    title = widgets.HTML(\n        value=\"&lt;h1&gt;Bounding box selector&lt;/h1&gt; &lt;/p&gt;Use the map on the right to draw a bounding box.  Once you selected it you will see the details of the bounding box below.\"\n    )\n\n    grid[0, 0] = title\n\n    bbox_details = widgets.Textarea(value=\"Your bbox info will appear here\", description=\"\", disabled=False, rows=5)\n\n    grid[1, 0] = bbox_details\n\n    def on_bbox_draw(self, action, geo_json):\n        \"\"\"\n        Callback function for the draw control's on_draw event.\n\n        Args:\n            self: The draw control object.\n            action: The action that triggered the event.\n            geo_json: The GeoJSON representation of the drawn shape.\n\n        Returns:\n            None\n        \"\"\"\n        print(geo_json)\n        bbox_details.value = geojson_to_details(geo_json)\n        # bbox_details.value = json.dumps(geo_json)\n\n    draw_control.on_draw(on_bbox_draw)\n\n    m.add(draw_control)\n\n    button_layout1 = widgets.Layout(width=\"auto\", height=\"40px\")  # set width and height\n    button_layout2 = widgets.Layout(width=\"auto\", height=\"40px\")  # set width and height\n\n    clear_button = widgets.Button(\n        description=\"Clear bboxes\",\n        button_style=\"warning\",\n        display=\"flex\",\n        flex_flow=\"column\",\n        align_items=\"stretch\",\n        layout=button_layout1,\n    )\n\n    def on_clear_click(b):\n        \"\"\"\n        Callback function for the clear button's on_click event.\n\n        Args:\n            b: The button object.\n\n        Returns:\n            None\n        \"\"\"\n        draw_control.clear()\n        bbox_details.value = \"Your bbox info will appear here\"\n\n    clear_button.on_click(on_clear_click)\n\n    grid[2, 0] = clear_button\n\n    use_button = widgets.Button(\n        description=\"Use this bounding box\",\n        button_style=\"success\",\n        display=\"flex\",\n        flex_flow=\"column\",\n        align_items=\"stretch\",\n        layout=button_layout2,\n    )\n\n    def on_use_click(b):\n        \"\"\"\n        Callback function for the use button's on_click event.\n\n        Args:\n            b: The button object.\n\n        Returns:\n            The bounding box coordinates as a list.\n        \"\"\"\n        # global bbox\n        bbox = geojson_to_bbox(draw_control.last_draw)\n        draw_control.last_draw[\"properties\"][\"style\"][\"fillColor\"] = \"#32a852\"\n        print(bbox)\n        return bbox\n\n    bbox = use_button.on_click(on_use_click)\n\n    # grid[3,0] = use_button\n\n    return grid\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.time_selector","title":"time_selector","text":"<pre><code>time_selector()\n</code></pre> <p>Creates a time range selector using ipywidgets.</p> <p>Returns:</p> Type Description <p>ipywidgets.widgets.GridspecLayout: A layout containing two datetime pickers for selecting a start and end time.</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def time_selector():\n    \"\"\"\n    Creates a time range selector using ipywidgets.\n\n    Args:\n        None\n\n    Returns:\n        ipywidgets.widgets.GridspecLayout: A layout containing two datetime pickers for selecting a start and end time.\n    \"\"\"\n    start_time_picker = widgets.DatetimePicker(description=\"Start time:  \", disabled=False)\n\n    end_time_picker = widgets.DatetimePicker(description=\"End time:  \", disabled=False)\n\n    grid = widgets.GridspecLayout(4, 2, height=\"200px\")\n\n    grid[0, 0:2] = widgets.HTML(\n        value=\"&lt;h1&gt;Time range selector&lt;/h1&gt; &lt;/p&gt;Use the datetime pickers to pick a time window for inference.\"\n    )\n    grid[1, 0] = start_time_picker\n    grid[2, 0] = end_time_picker\n\n    return grid\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.add_geotiff","title":"add_geotiff","text":"<pre><code>add_geotiff(\n    filename,\n    layer_name=\"\",\n    colormap=\"viridis\",\n    cmin=0,\n    cmax=\"\",\n    opacity=1.0,\n)\n</code></pre> <p>Adds a GeoTIFF file to a Folium map as an overlay.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The path to the GeoTIFF file.</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer. Defaults to \"\".</p> <code>''</code> <code>colormap</code> <code>str</code> <p>The colormap to use for the GeoTIFF data. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>cmin</code> <code>int or float</code> <p>The minimum value for the colormap. Defaults to 0.</p> <code>0</code> <code>cmax</code> <code>int or float</code> <p>The maximum value for the colormap. If not provided, it is automatically calculated as the maximum value in the GeoTIFF data. Defaults to \"\".</p> <code>''</code> <code>opacity</code> <code>float</code> <p>The opacity of the overlay. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>folium.raster_layers.ImageOverlay: An ImageOverlay object that can be added to a Folium map.</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def add_geotiff(filename, layer_name=\"\", colormap=\"viridis\", cmin=0, cmax=\"\", opacity=1.0):\n    \"\"\"\n    Adds a GeoTIFF file to a Folium map as an overlay.\n\n    Args:\n        filename (str): The path to the GeoTIFF file.\n        layer_name (str, optional): The name of the layer. Defaults to \"\".\n        colormap (str, optional): The colormap to use for the GeoTIFF data. Defaults to \"viridis\".\n        cmin (int or float, optional): The minimum value for the colormap. Defaults to 0.\n        cmax (int or float, optional): The maximum value for the colormap. If not provided, it is automatically calculated as the maximum value in the GeoTIFF data. Defaults to \"\".\n        opacity (float, optional): The opacity of the overlay. Defaults to 1.0.\n\n    Returns:\n        folium.raster_layers.ImageOverlay: An ImageOverlay object that can be added to a Folium map.\n    \"\"\"\n    with rasterio.open(filename) as src:\n        dataArray = src.read(1)\n        bounds = src.bounds\n        nd = src.nodata\n\n    # TODO remove - never used\n    # midLat = (bounds[3] + bounds[1]) / 2\n    # midLon = (bounds[2] + bounds[0]) / 2\n\n    if cmax == \"\":\n        cmax = np.max(dataArray)\n    dataArrayMasked = np.ma.masked_where(dataArray == nd, dataArray)\n    imc = colorize(dataArrayMasked, cmax, cmin=cmin, cmap=colormap)\n\n    return folium.raster_layers.ImageOverlay(\n        imc,\n        [[bounds[1], bounds[0]], [bounds[3], bounds[2]]],\n        name=layer_name,\n        opacity=opacity,\n    )\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.colorize","title":"colorize","text":"<pre><code>colorize(array, cmax, cmin=0, cmap='rainbow')\n</code></pre> <p>Converts a 2D numpy array of values into an RGBA array given a colour map and range.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> required <code>cmax</code> <code>float</code> <p>Max value for colour range</p> required <code>cmin</code> <code>float</code> <p>Min value for colour range</p> <code>0</code> <code>cmap</code> <code>string</code> <p>Colour map to use (from matplotlib colourmaps)</p> <code>'rainbow'</code> <p>Returns:</p> Name Type Description <code>rgba_array</code> <code>ndarray</code> <p>3D RGBA array which can be plotted.</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def colorize(array, cmax, cmin=0, cmap=\"rainbow\"):\n    \"\"\"Converts a 2D numpy array of values into an RGBA array given a colour map and range.\n\n    Args:\n        array (ndarray):\n        cmax (float): Max value for colour range\n        cmin (float): Min value for colour range\n        cmap (string): Colour map to use (from matplotlib colourmaps)\n\n    Returns:\n            rgba_array (ndarray): 3D RGBA array which can be plotted.\n    \"\"\"\n    normed_data = (array - cmin) / (array.max() - cmin)\n    cm = plt.cm.get_cmap(cmap)\n    return cm(normed_data)\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.available_models_ui","title":"available_models_ui","text":"<pre><code>available_models_ui(client)\n</code></pre> <p>A UI for browsing and selecting available models.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>object</code> <p>:py:class:<code>geostudio.backends.ginference.client.Client</code></p> required <p>Returns:</p> Type Description <p>widgets.VBox: A Jupyter widget containing a header, text input for filtering, checkbox for active models, interactive dropdown for selecting a model, and the model selection table.</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def available_models_ui(client):\n    \"\"\"\n    A UI for browsing and selecting available models.\n\n    Args:\n        client (object): :py:class:`geostudio.backends.ginference.client.Client`\n\n    Returns:\n        widgets.VBox: A Jupyter widget containing a header, text input for filtering, checkbox for active models, interactive dropdown for selecting a model, and the model selection table.\n    \"\"\"\n    # workflows = self.available_workflows()\n    models_df = client.list_models(output=\"df\")\n    models_trim = models_df[[\"name\", \"description\", \"created_at\", \"created_by\", \"active\"]].sort_values(by=[\"name\"])\n\n    # all_tags = []\n    # [all_tags.extend(X) for X in wf_trim.tags]\n    # diff_tags = list(np.unique(np.array(all_tags)))\n\n    def view(name=\"\", active=True):\n        \"\"\"\n        Displays a filtered list of models based on the provided name and active status.\n\n        Args:\n            name (str, optional): The name of the model(s) to filter by. If not provided, all models are displayed.\n            active (bool, optional): Whether to filter models by their active status. If True, only active models are displayed.\n\n        Returns:\n            None\n        \"\"\"\n        if name != \"\":\n            models_trim_filtered = models_trim[[name in X for X in models_trim.name]]\n        else:\n            models_trim_filtered = models_trim\n\n        if active == True:\n            models_trim_filtered = models_trim_filtered[models_trim_filtered.active == True]\n            #     [\"true\" in X for X in models_trim_filtered.active]\n            # ]\n        else:\n            models_trim_filtered = models_trim_filtered\n\n        model_names = list(models_trim_filtered.name)\n\n        if len(model_names) &gt; 0:\n            wf_dd.options = model_names\n        else:\n            wf_dd.value = \"None found\"\n            wf_dd.options = [\"None found\"]\n\n        return display(HTML(models_trim_filtered.to_html(index=False)))\n\n    keyText = widgets.Text(\n        value=\"\",\n        placeholder=\"Type something to filter results\",\n        description=\"Model name:\",\n        disabled=False,\n        layout=widgets.Layout(height=\"auto\", width=\"400px\"),\n    )\n\n    wf_dd = widgets.Dropdown(\n        options=list(models_trim.name),\n        value=list(models_trim.name)[0],\n        description=\"Select Model:\",\n        disabled=False,\n        layout=widgets.Layout(height=\"auto\", width=\"600px\"),\n    )\n\n    def on_change(change):\n        if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n            model_name = change[\"new\"]\n\n    wf_dd.observe(on_change)\n\n    activeCheck = widgets.Checkbox(value=True, description=\"Active only?\", disabled=False)\n\n    # hdr = widgets.Button(\n    #     description=\"GeoDN Modeling workflow catalogue\",\n    #     disabled=True,\n    #     button_style=\"info\",  # 'success', 'info', 'warning', 'danger' or ''\n    #     layout=widgets.Layout(height=\"auto\", width=\"800px\"),\n    # )\n    hdr = widgets.HTML(value=\"&lt;h1&gt;Inference model selector&lt;/h1&gt; &lt;/p&gt;Explore which models are deployed for inference.\")\n\n    models_filter = widgets.interactive(\n        view,\n        name=keyText,\n        # tags=tagsSelect,\n        active=activeCheck,\n        layout=widgets.Layout(height=\"auto\", width=\"800px\"),\n    )\n\n    # hdr = widgets.Button(\n    #     description=\"Inference model selector\",\n    #     disabled=True,\n    #     button_style=\"info\",  # 'success', 'info', 'warning', 'danger' or ''\n    #     layout=widgets.Layout(height=\"auto\", width=\"800px\"),\n    # )\n\n    models_table = widgets.VBox(\n        [\n            hdr,\n            widgets.HBox([widgets.VBox([keyText, activeCheck])]),\n            models_filter.children[2],\n            wf_dd,\n        ],\n        layout=widgets.Layout(margin=\"20px 20px 20px 20px\", padding=\"5px 5px 5px 5px\"),\n    )\n\n    return models_table\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.fileDownloader","title":"fileDownloader","text":"<pre><code>fileDownloader(client, id, just_tifs=True)\n</code></pre> <p>Downloads the output files of an inference task.</p> <p>Parameters:</p> Name Type Description Default <code>client (</code> <p>py:class:<code>geostudio.backends.ginference.client.Client</code>): An object representing the client to interact with the inference service.</p> required <code>id</code> <code>str</code> <p>The unique identifier of the inference task.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def fileDownloader(client, id, just_tifs=True):\n    \"\"\"\n    Downloads the output files of an inference task.\n\n    Args:\n        client (:py:class:`geostudio.backends.ginference.client.Client`): An object representing the client to interact with the inference service.\n        id (str): The unique identifier of the inference task.\n\n    Returns:\n        None\n    \"\"\"\n\n    r = client.get_inference_task(id)\n    fl = list_output_files(url=r[\"output_url\"], just_tif=just_tifs)\n\n    sm = widgets.SelectMultiple(\n        options=fl,\n        value=[],\n        rows=10,\n        description=\"Files:\",\n        disabled=False,\n        layout={\"width\": \"1000px\"},\n    )\n\n    db = widgets.Button(\n        description=\"Download\",\n        disabled=False,\n        button_style=\"\",  # 'success', 'info', 'warning', 'danger' or ''\n        tooltip=\"Click me to download the selected\",\n        icon=\"check\",\n        layout=widgets.Layout(height=\"auto\", width=\"800px\"),\n    )\n\n    dlp = widgets.Text(value=\"./\", description=\"Dl path:\", disabled=False)\n\n    hdr = widgets.HTML(\n        value=\"&lt;h1&gt;Inference output downloader&lt;/h1&gt; &lt;/p&gt;Select the files and the download path and hit download.\"\n    )\n\n    output = widgets.Output()\n\n    display(hdr, sm, dlp, db, output)\n\n    def on_button_clicked(db):\n        \"\"\"This function is triggered when a button is clicked.\"\"\"\n        with output:\n            for X in list(sm.value):\n                print(\"Downloading...\", end=\"\\r\")\n                a = download_file(r[\"output_url\"], X)\n                print(a)\n\n    db.on_click(on_button_clicked)\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.fileDownloaderTasks","title":"fileDownloaderTasks","text":"<pre><code>fileDownloaderTasks(client, task_id, just_tifs=True)\n</code></pre> <p>Downloads the output files of an inference task.</p> <p>Parameters:</p> Name Type Description Default <code>client (</code> <p>py:class:<code>geostudio.backends.v2.ginference.client.Client</code>): An object representing the client to interact with the inference service.</p> required <code>task_id</code> <code>str</code> <p>The unique identifier of the inference task id.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def fileDownloaderTasks(client, task_id, just_tifs=True):\n    \"\"\"\n    Downloads the output files of an inference task.\n\n    Args:\n        client (:py:class:`geostudio.backends.v2.ginference.client.Client`): An object representing the client to interact with the inference service.\n        task_id (str): The unique identifier of the inference task id.\n\n    Returns:\n        None\n    \"\"\"\n\n    r = client.get_task_output_url(task_id)\n    fl = list_output_files(url=r[\"output_url\"], just_tif=just_tifs)\n\n    sm = widgets.SelectMultiple(\n        options=fl,\n        value=[],\n        rows=10,\n        description=\"Files:\",\n        disabled=False,\n        layout={\"width\": \"1000px\"},\n    )\n\n    db = widgets.Button(\n        description=\"Download\",\n        disabled=False,\n        button_style=\"\",  # 'success', 'info', 'warning', 'danger' or ''\n        tooltip=\"Click me to download the selected\",\n        icon=\"check\",\n        layout=widgets.Layout(height=\"auto\", width=\"800px\"),\n    )\n\n    dlp = widgets.Text(value=\"./\", description=\"Dl path:\", disabled=False)\n\n    hdr = widgets.HTML(\n        value=\"&lt;h1&gt;Inference Task output downloader&lt;/h1&gt; &lt;/p&gt;Select the files and the download path and hit download.\"\n    )\n\n    output = widgets.Output()\n\n    display(hdr, sm, dlp, db, output)\n\n    def on_button_clicked(db):\n        \"\"\"This function is triggered when a button is clicked.\"\"\"\n        with output:\n            for X in list(sm.value):\n                print(\"Downloading...\", end=\"\\r\")\n                a = download_file(r[\"output_url\"], X)\n                print(a)\n\n    db.on_click(on_button_clicked)\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.geotiff2img","title":"geotiff2img","text":"<pre><code>geotiff2img(filename, band=1, cmax='')\n</code></pre> <p>Converts a GeoTIFF file to a base64 encoded PNG image URL.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The path to the GeoTIFF file.</p> required <code>band</code> <code>int</code> <p>The band number to use for the image. Default is 1.</p> <code>1</code> <code>cmax</code> <code>str or float</code> <p>The maximum value for color scaling. If not provided, it will be automatically calculated.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the base64 encoded PNG image URL and the image bounds.</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def geotiff2img(filename, band=1, cmax=\"\"):\n    \"\"\"\n    Converts a GeoTIFF file to a base64 encoded PNG image URL.\n\n    Args:\n        filename (str): The path to the GeoTIFF file.\n        band (int, optional): The band number to use for the image. Default is 1.\n        cmax (str or float, optional): The maximum value for color scaling. If not provided, it will be automatically calculated.\n\n    Returns:\n        tuple: A tuple containing the base64 encoded PNG image URL and the image bounds.\n    \"\"\"\n\n    if \"_rgb.tif\" not in filename:\n        # In the case of a non-RGB tagged image convert to an RGB png based on a color map from a single selected band\n        with rasterio.open(filename) as src:\n            dataArray = src.read(band)\n            bounds = src.bounds\n            nd = src.nodata\n\n        if cmax == \"\":\n            cmax = np.max(dataArray)\n        dataArrayMasked = np.ma.masked_where(dataArray == nd, dataArray)\n        imc = colorize(dataArrayMasked, cmax, cmin=0, cmap=\"viridis\")\n\n        img = 255 * imc\n        img = img.astype(np.uint8)\n        im = PIL.Image.fromarray(img, mode=\"RGBA\")\n\n    elif \"_rgb.tif\" in filename:\n        # In the case of an RGB tagged image convert to an RGB png\n        with rasterio.open(filename) as src:\n            rBand = src.read(1)\n            gBand = src.read(2)\n            bBand = src.read(3)\n            bounds = src.bounds\n            nd = src.nodata\n\n        opacity_layer = np.ma.masked_where(rBand == nd, 255 * np.ones(rBand.shape))\n        img = np.stack([rBand, gBand, bBand, opacity_layer], axis=-1)\n\n        img = 2 * img\n        img = img.astype(np.uint8)\n        im = PIL.Image.fromarray(img, mode=\"RGBA\")\n\n    f = BytesIO()\n    im.save(f, \"png\")\n\n    data = b64encode(f.getvalue())\n    data = data.decode(\"ascii\")\n    imgurl = \"data:image/png;base64,\" + data\n\n    return imgurl, bounds\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.inferenceViewer","title":"inferenceViewer","text":"<pre><code>inferenceViewer(client, id)\n</code></pre> <p>Creates a Jupyter widget for visualizing inference task outputs.</p> <p>Parameters:</p> Name Type Description Default <code>client (</code> <p>py:class:<code>geostudio.backends.ginference.client.Client</code>): An object representing the client for interacting with the inference service.</p> required <code>id</code> <code>str</code> <p>The unique identifier for the inference task.</p> required <p>Returns:</p> Name Type Description <code>object</code> <p>A Jupyter widget containing a map with image overlays of the inference outputs.</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def inferenceViewer(client, id):\n    \"\"\"\n    Creates a Jupyter widget for visualizing inference task outputs.\n\n    Args:\n        client (:py:class:`geostudio.backends.ginference.client.Client`): An object representing the client for interacting with the inference service.\n        id (str): The unique identifier for the inference task.\n\n    Returns:\n        object: A Jupyter widget containing a map with image overlays of the inference outputs.\n    \"\"\"\n\n    r = client.get_inference_task(id)\n    fl = list_output_files(r[\"output_url\"])\n\n    fl_options = [(X.split(\"/\")[-1].replace(r[\"event_id\"] + \"_\", \"\"), X) for X in fl]\n\n    sm = widgets.SelectMultiple(\n        options=fl_options,\n        value=[],\n        rows=10,\n        description=\"\",\n        disabled=False,\n        layout={\"width\": \"800px\"},\n    )\n\n    db = widgets.Button(\n        description=\"Update layers\",\n        disabled=False,\n        button_style=\"\",  # 'success', 'info', 'warning', 'danger' or ''\n        tooltip=\"Click me to update the layers on the map\",\n        icon=\"check\",\n        layout=widgets.Layout(height=\"auto\", width=\"100px\"),\n    )\n\n    output = widgets.Output()\n\n    left = widgets.VBox([sm, db, output])\n\n    header = widgets.HTML(value=\"&lt;h1&gt;Inference output viewer&lt;/h1&gt;\")\n    footer = widgets.HTML(value=\"Thanks for using the viewer, any questions please ask....\")\n\n    map = Map(center=(52.0, 0.0), zoom=8, scroll_wheel_zoom=True, world_copy_jump=False)\n    map.add(FullScreenControl())\n    control = LayersControl(position=\"topright\")\n    map.add(control)\n\n    def on_button_clicked(db):\n        \"\"\"\n        Adds image overlays to a map based on a list of URLs.\n\n        Args:\n            db (object): The database object containing necessary information for the operation.\n\n        Returns:\n            None\n        \"\"\"\n        layer_files = []\n\n        # map.clear_layers()\n\n        with output:\n            output.clear_output()\n            for X in list(sm.value):\n                print(\"Downloading...\", end=\"\\r\")\n                a = download_file(r[\"output_url\"], X)\n                print(a)\n                layer_files = layer_files + [a]\n                imgurl, bounds = geotiff2img(X)\n                imgBounds = ((bounds.bottom, bounds.left), (bounds.top, bounds.right))\n\n                map.add(\n                    ImageOverlay(\n                        name=a.split(\"/\")[-1].replace(r[\"event_id\"] + \"_\", \"\"),\n                        url=imgurl,\n                        bounds=imgBounds,\n                        opacity=0.9,\n                    )\n                )\n                # map.add(io)\n                # map.fit_bounds([[bounds.bottom, bounds.left],[bounds.top, bounds.right]])\n                map.center = (bounds.bottom, bounds.left)\n                print(\"&gt;&gt;&gt; added to map\")\n\n    db.on_click(on_button_clicked)\n\n    return widgets.VBox(\n        [\n            header,\n            # widgets.HBox([widgets.VBox([sm, db]), output]),\n            sm,\n            db,\n            map,\n            output,\n            # footer\n        ]\n    )\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.inferenceTaskViewer","title":"inferenceTaskViewer","text":"<pre><code>inferenceTaskViewer(client, task_id)\n</code></pre> <p>Creates a Jupyter widget for visualizing inference task outputs.</p> <p>Parameters:</p> Name Type Description Default <code>client (</code> <p>py:class:<code>geostudio.backends.v2.ginference.client.Client</code>): An object representing the client for interacting with the inference service.</p> required <code>task_id</code> <code>str</code> <p>The unique identifier for the inference task.</p> required <p>Returns:</p> Name Type Description <code>object</code> <p>A Jupyter widget containing a map with image overlays of the inference outputs.</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def inferenceTaskViewer(client, task_id):\n    \"\"\"\n    Creates a Jupyter widget for visualizing inference task outputs.\n\n    Args:\n        client (:py:class:`geostudio.backends.v2.ginference.client.Client`): An object representing the client for interacting with the inference service.\n        task_id (str): The unique identifier for the inference task.\n\n    Returns:\n        object: A Jupyter widget containing a map with image overlays of the inference outputs.\n    \"\"\"\n\n    r = client.get_task_output_url(task_id)\n    fl = list_output_files(r[\"output_url\"])\n\n    fl_options = [(X.split(\"/\")[-1].replace(r[\"task_id\"] + \"_\", \"\"), X) for X in fl]\n\n    sm = widgets.SelectMultiple(\n        options=fl_options,\n        value=[],\n        rows=10,\n        description=\"\",\n        disabled=False,\n        layout={\"width\": \"800px\"},\n    )\n\n    db = widgets.Button(\n        description=\"Update layers\",\n        disabled=False,\n        button_style=\"\",  # 'success', 'info', 'warning', 'danger' or ''\n        tooltip=\"Click me to update the layers on the map\",\n        icon=\"check\",\n        layout=widgets.Layout(height=\"auto\", width=\"100px\"),\n    )\n\n    output = widgets.Output()\n\n    left = widgets.VBox([sm, db, output])\n\n    header = widgets.HTML(value=\"&lt;h1&gt;Inference output viewer&lt;/h1&gt;\")\n    footer = widgets.HTML(value=\"Thanks for using the viewer, any questions please ask....\")\n\n    map = Map(center=(52.0, 0.0), zoom=8, scroll_wheel_zoom=True, world_copy_jump=False)\n    map.add(FullScreenControl())\n    control = LayersControl(position=\"topright\")\n    map.add(control)\n\n    def on_button_clicked(db):\n        \"\"\"\n        Adds image overlays to a map based on a list of URLs.\n\n        Args:\n            db (object): The database object containing necessary information for the operation.\n\n        Returns:\n            None\n        \"\"\"\n        layer_files = []\n\n        # map.clear_layers()\n\n        with output:\n            output.clear_output()\n            for X in list(sm.value):\n                print(\"Downloading...\", end=\"\\r\")\n                a = download_file(r[\"output_url\"], X)\n                print(a)\n                layer_files = layer_files + [a]\n                imgurl, bounds = geotiff2img(X)\n                imgBounds = ((bounds.bottom, bounds.left), (bounds.top, bounds.right))\n\n                map.add(\n                    ImageOverlay(\n                        name=a.split(\"/\")[-1].replace(r[\"task_id\"] + \"_\", \"\"),\n                        url=imgurl,\n                        bounds=imgBounds,\n                        opacity=0.9,\n                    )\n                )\n                # map.add(io)\n                # map.fit_bounds([[bounds.bottom, bounds.left],[bounds.top, bounds.right]])\n                map.center = (bounds.bottom, bounds.left)\n                print(\"&gt;&gt;&gt; added to map\")\n\n    db.on_click(on_button_clicked)\n\n    return widgets.VBox(\n        [\n            header,\n            # widgets.HBox([widgets.VBox([sm, db]), output]),\n            sm,\n            db,\n            map,\n            output,\n            # footer\n        ]\n    )\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.color_inference_tasks_by_status","title":"color_inference_tasks_by_status","text":"<pre><code>color_inference_tasks_by_status(val)\n</code></pre> <p>Takes a scalar and returns a string with the css property <code>'color: red'</code> for negative strings, black otherwise.</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def color_inference_tasks_by_status(val):\n    \"\"\"\n    Takes a scalar and returns a string with\n    the css property `'color: red'` for negative\n    strings, black otherwise.\n    \"\"\"\n\n    if val == \"READY\":\n        color = \"#ffd966\"\n    elif val == \"WAITING\":\n        color = \"#f6b26b\"\n    elif val == \"RUNNING\":\n        color = \"#d9ead3\"\n    elif val == \"FINISHED\":\n        color = \"#8fce00\"\n    elif val == \"FAILED\":\n        color = \"#ff0000\"\n    elif val == \"STOP\":\n        color = \"#b00020\"\n    else:\n        color = \"black\"\n\n    return \"color: %s\" % color\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.plot_tune_metrics","title":"plot_tune_metrics","text":"<pre><code>plot_tune_metrics(\n    client, tune_id: str, run_name: str = \"Train\"\n)\n</code></pre> <p>Plots training and validation metrics for a given tuning experiment in a 2x2 subplot grid.</p> <p>Parameters:</p> Name Type Description Default <code>tune_id</code> <code>str</code> <p>The unique identifier of the tuning experiment.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def plot_tune_metrics(client, tune_id: str, run_name: str = \"Train\"):\n    \"\"\"\n    Plots training and validation metrics for a given tuning experiment in a 2x2 subplot grid.\n\n    Parameters:\n        tune_id (str): The unique identifier of the tuning experiment.\n\n    Returns:\n        None\n    \"\"\"\n    mlflow_urls = client.get_mlflow_metrics(tune_id)\n    if mlflow_urls:\n        print(mlflow_urls)\n    else:\n        return f\"Tune {tune_id}, has not started to generate metrics. Try to rerun this cell after a few moments!\"\n\n    mdf = client.get_tune_metrics_df(tune_id, run_name)\n    r = client.get_tune(tune_id)\n    status = r[\"status\"]\n\n    mdf_columns = mdf.columns.tolist()\n    if not mdf_columns:\n        return f\"Tune {tune_id}, has not started to generate metrics. Try to rerun this cell after a few moments!\"\n    mdf_columns.remove(\"epoch\")\n    mdf_columns_len = len(mdf_columns)\n\n    nrows = math.ceil(mdf_columns_len / 2)\n    ncols = 2\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, sharey=False, figsize=(10, mdf_columns_len))\n    fig.tight_layout()\n    step_num = max(mdf.epoch)\n    fig.suptitle(f\"{tune_id} - {status} - Step number: {step_num}\")\n    fig.subplots_adjust(top=0.88)\n\n    axes = axes.flatten()\n\n    for i, column in enumerate(mdf_columns):\n        axes[i].plot(mdf[\"epoch\"], mdf[column], \"b.-\")\n        axes[i].set_title(column)\n        axes[i].grid(True)\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.crop_image_bytes","title":"crop_image_bytes","text":"<pre><code>crop_image_bytes(img_bytes)\n</code></pre> <p>Crops the white space from the training image provided as raw bytes and return PNG bytes.</p>"},{"location":"widgets/#geostudio.gswidgets.crop_image_bytes--parameters","title":"Parameters","text":"<p>img_bytes : bytes     Raw image bytes (any format supported by PIL.Image.open).</p>"},{"location":"widgets/#geostudio.gswidgets.crop_image_bytes--returns","title":"Returns","text":"<p>bytes     PNG-encoded bytes of the cropped image. The function uses a fixed crop box     (left=0, upper=350, right=image_width, lower=650) so the returned image contains     the horizontal strip between y=350 and y=650 from the original image.</p>"},{"location":"widgets/#geostudio.gswidgets.crop_image_bytes--notes","title":"Notes","text":"<ul> <li>The result is always encoded as PNG.</li> <li>If the crop box extends beyond the source image bounds, PIL.Image.crop behavior applies.</li> </ul> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def crop_image_bytes(img_bytes):\n    \"\"\"\n    Crops the white space from the training image provided as raw bytes and return PNG bytes.\n\n    Parameters\n    ----------\n    img_bytes : bytes\n        Raw image bytes (any format supported by PIL.Image.open).\n\n    Returns\n    -------\n    bytes\n        PNG-encoded bytes of the cropped image. The function uses a fixed crop box\n        (left=0, upper=350, right=image_width, lower=650) so the returned image contains\n        the horizontal strip between y=350 and y=650 from the original image.\n\n    Notes\n    -----\n    - The result is always encoded as PNG.\n    - If the crop box extends beyond the source image bounds, PIL.Image.crop behavior applies.\n    \"\"\"\n\n    imageFile = Image.open(io.BytesIO(img_bytes))\n    w, h = imageFile.size\n    croppedImageFile = imageFile.crop((0, 350, w, 650))\n    imgBytes = io.BytesIO()\n    croppedImageFile.save(imgBytes, format=\"PNG\")\n    return imgBytes.getvalue()\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.save_training_image","title":"save_training_image","text":"<pre><code>save_training_image(\n    image_number, epoch, img_dict, cropped=True\n)\n</code></pre> <p>Save a training sample image from img_dict to a PNG file.</p>"},{"location":"widgets/#geostudio.gswidgets.save_training_image--parameters","title":"Parameters","text":"<p>image_number : int     The sample index/number to save (matches the 'image_number' key in img_dict). epoch : int     The epoch number to save (matches the 'epoch' key in img_dict). img_dict : list[dict]     List of artefact records as returned by get_tuning_artefacts. Each item must contain:       - 'filename' : str       - 'image' : bytes       - 'epoch' : int       - 'image_number' : int cropped : bool, optional     If True (default) crop the image bytes using crop_image_bytes before saving.</p>"},{"location":"widgets/#geostudio.gswidgets.save_training_image--raises","title":"Raises","text":"<p>ValueError     If no matching image is found in img_dict.</p> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def save_training_image(image_number, epoch, img_dict, cropped=True):\n    \"\"\"\n    Save a training sample image from img_dict to a PNG file.\n\n    Parameters\n    ----------\n    image_number : int\n        The sample index/number to save (matches the 'image_number' key in img_dict).\n    epoch : int\n        The epoch number to save (matches the 'epoch' key in img_dict).\n    img_dict : list[dict]\n        List of artefact records as returned by get_tuning_artefacts. Each item must contain:\n          - 'filename' : str\n          - 'image' : bytes\n          - 'epoch' : int\n          - 'image_number' : int\n    cropped : bool, optional\n        If True (default) crop the image bytes using crop_image_bytes before saving.\n\n    Raises\n    ------\n    ValueError\n        If no matching image is found in img_dict.\n    \"\"\"\n    img_bytes = [X for X in img_dict if (X[\"epoch\"] == epoch) &amp; (X[\"image_number\"] == image_number)][0][\"image\"]\n    with open(f\"training_image_epoch_{epoch}_number_{image_number}.png\", \"wb\") as f:\n        if cropped:\n            f.write(crop_image_bytes(img_bytes))\n        else:\n            f.write(img_bytes)\n</code></pre>"},{"location":"widgets/#geostudio.gswidgets.browse_training_images","title":"browse_training_images","text":"<pre><code>browse_training_images(img_dict: object, tune_id: str)\n</code></pre> <p>Create an interactive Jupyter widget viewer to browse fine-tuning sample images.</p>"},{"location":"widgets/#geostudio.gswidgets.browse_training_images--parameters","title":"Parameters","text":"<p>img_dict : list[dict]     List of artefact records. Each item must be a dict with at least the keys:       - 'filename' : str       - 'image' : bytes  (raw image bytes as returned by get_tuning_artefacts)       - 'epoch' : int       - 'image_number' : int tune_id : str     Identifier shown in the viewer header.</p>"},{"location":"widgets/#geostudio.gswidgets.browse_training_images--notes","title":"Notes","text":"<ul> <li>Depends on crop_image_bytes(img_bytes) to produce the PNG bytes shown in the widget.</li> <li>Expects img_dict to contain at least one image; raises ValueError otherwise.</li> <li>Uses ipywidgets and functools to wire button callbacks.</li> <li>To use: viewer = browse_training_images(img_dict, tune_id); display(viewer)</li> </ul> Source code in <code>geostudio/gswidgets.py</code> <pre><code>def browse_training_images(img_dict: object, tune_id: str):\n    \"\"\"\n    Create an interactive Jupyter widget viewer to browse fine-tuning sample images.\n\n    Parameters\n    ----------\n    img_dict : list[dict]\n        List of artefact records. Each item must be a dict with at least the keys:\n          - 'filename' : str\n          - 'image' : bytes  (raw image bytes as returned by get_tuning_artefacts)\n          - 'epoch' : int\n          - 'image_number' : int\n    tune_id : str\n        Identifier shown in the viewer header.\n\n    Notes\n    -----\n    - Depends on crop_image_bytes(img_bytes) to produce the PNG bytes shown in the widget.\n    - Expects img_dict to contain at least one image; raises ValueError otherwise.\n    - Uses ipywidgets and functools to wire button callbacks.\n    - To use: viewer = browse_training_images(img_dict, tune_id); display(viewer)\n    \"\"\"\n\n    if not img_dict:\n        raise ValueError(\"img_dict is empty - must contain at least one image record\")\n\n    epochs = sorted(list(set([X[\"epoch\"] for X in img_dict])))\n    image_numbers = sorted(list(set([X[\"image_number\"] for X in img_dict])))\n\n    header = widgets.HTML(value=f\"&lt;h2&gt;Fine-tuning samples - {tune_id}&lt;/h2&gt;\")\n\n    image_widget = widgets.Image(\n        value=crop_image_bytes(\n            [X for X in img_dict if (X[\"epoch\"] == epochs[0]) &amp; (X[\"image_number\"] == image_numbers[0])][0][\"image\"]\n        ),\n        format=\"png\",\n        width=800,\n        height=400,\n    )\n\n    # Create buttons for navigation\n    back_epoch_button = widgets.Button(description=\"&lt; Back\")\n    forward_epoch_button = widgets.Button(description=\"Next &gt;\")\n    back_image_button = widgets.Button(description=\"&lt; Back\")\n    forward_image_button = widgets.Button(description=\"Next &gt;\")\n\n    # Use a widget to hold the current image index\n    epoch_index_w = widgets.IntText(value=0, visible=False)\n    image_number_index_w = widgets.IntText(value=0, visible=False)\n\n    epoch_text = widgets.Text(value=str(epochs[epoch_index_w.value]), description=\"Epoch:\", disabled=True)\n    image_text = widgets.Text(\n        value=str(image_numbers[image_number_index_w.value]), description=\"Sample:\", disabled=True\n    )\n\n    # Arrange the widgets in a horizontal box\n    viewer_container = widgets.VBox(\n        [\n            header,\n            widgets.HBox([epoch_text, back_epoch_button, forward_epoch_button]),\n            widgets.HBox([image_text, back_image_button, forward_image_button]),\n            image_widget,\n        ]\n    )\n\n    # Create a function to handle button clicks\n    def on_epoch_button_click(b, epochs=[], image_numbers=[]):\n        current_index = epoch_index_w.value\n        max_index = len(epochs) - 1\n\n        if b.description == \"Next &gt;\" and current_index &lt; max_index:\n            epoch_index_w.value += 1\n        elif b.description == \"&lt; Back\" and current_index &gt; 0:\n            epoch_index_w.value -= 1\n\n        epoch_text.value = str(epochs[epoch_index_w.value])\n\n        # Update the displayed image\n        image_widget.value = crop_image_bytes(\n            [\n                X\n                for X in img_dict\n                if (X[\"epoch\"] == epochs[epoch_index_w.value])\n                &amp; (X[\"image_number\"] == image_numbers[image_number_index_w.value])\n            ][0][\"image\"]\n        )\n\n    # Attach the click event to the buttons\n    back_epoch_button.on_click(functools.partial(on_epoch_button_click, epochs=epochs, image_numbers=image_numbers))\n    forward_epoch_button.on_click(functools.partial(on_epoch_button_click, epochs=epochs, image_numbers=image_numbers))\n\n    def on_image_button_click(b, epochs=[], image_numbers=[]):\n        current_index = image_number_index_w.value\n        max_index = len(image_numbers) - 1\n\n        if b.description == \"Next &gt;\" and current_index &lt; max_index:\n            image_number_index_w.value += 1\n        elif b.description == \"&lt; Back\" and current_index &gt; 0:\n            image_number_index_w.value -= 1\n\n        image_text.value = str(image_numbers[image_number_index_w.value])\n\n        # Update the displayed image\n        image_widget.value = crop_image_bytes(\n            [\n                X\n                for X in img_dict\n                if (X[\"epoch\"] == epochs[epoch_index_w.value])\n                &amp; (X[\"image_number\"] == image_numbers[image_number_index_w.value])\n            ][0][\"image\"]\n        )\n\n    # Attach the click event to the buttons\n    back_image_button.on_click(functools.partial(on_image_button_click, epochs=epochs, image_numbers=image_numbers))\n    forward_image_button.on_click(functools.partial(on_image_button_click, epochs=epochs, image_numbers=image_numbers))\n\n    # Display the interactive viewer\n    return viewer_container\n</code></pre>"},{"location":"examples/dataset-onboarding/001-Introduction-to-Onboarding-Tuning-Data/","title":"001-Introduction-to-Onboarding-Tuning-Data","text":"In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre># import the required packages\nimport urllib3\nimport json\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\nfrom geostudio import Client\n</pre> # import the required packages import urllib3 import json urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  from geostudio import Client In\u00a0[\u00a0]: Copied! <pre>#############################################################\n# Initialize Geostudio client using a geostudio config file\n#############################################################\ngfm_client = Client(geostudio_config_file=\".geostudio_config_file\")\n</pre> ############################################################# # Initialize Geostudio client using a geostudio config file ############################################################# gfm_client = Client(geostudio_config_file=\".geostudio_config_file\")  In\u00a0[\u00a0]: Copied! <pre>gfm_client.list_datasets(output=\"df\")\n</pre> gfm_client.list_datasets(output=\"df\") In\u00a0[\u00a0]: Copied! <pre># paste the dataset_id of the dataset you want to explore\ngfm_client.get_dataset(\"geodata-dtxfvhqh2poaylszpfigfd\")\n</pre> # paste the dataset_id of the dataset you want to explore gfm_client.get_dataset(\"geodata-dtxfvhqh2poaylszpfigfd\") In\u00a0[\u00a0]: Copied! <pre># (Optional) If you wish to upload the data archive through the studio, you can uncomment and use this function and paste the path to your zipped dataset. \nuploaded_links = gfm_client.upload_file('/Users/beldinemoturi/Downloads/flood-dataset-test.zip')\nuploaded_links\n</pre> # (Optional) If you wish to upload the data archive through the studio, you can uncomment and use this function and paste the path to your zipped dataset.  uploaded_links = gfm_client.upload_file('/Users/beldinemoturi/Downloads/flood-dataset-test.zip') uploaded_links <p>Below are some example data connectors, collections and modality_tags to be provided in the dataset need to be correctly matched. See table below.  (The modality tags relate to the modalities in the Terramind model)</p> Collections Modality tag Connector s2_l1c S2L1C sentinelhub dem DEM sentinelhub s1_grd S1GRD sentinelhub hls_l30 HLS_L30 sentinelhub hls_s30 HLS_S30 sentinelhub s2_l2a S2L2A sentinelhub In\u00a0[\u00a0]: Copied! <pre># Multi-modal data\n# Edit the details in the dict and dataset_url below to suit your dataset\n\nmulti_modal_datasetDict = {\n    \"dataset_name\": \"Sentinel Flood Multimodal Test\",\n    \"data_sources\": [\n        {\n            \"bands\": [\n                {\n                    \"index\": \"0\",\n                    \"band_name\": \"Coastal_aerosol\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"1\",\n                    \"band_name\": \"Blue\",\n                    \"RGB_band\": \"B\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"2\",\n                    \"band_name\": \"Green\",\n                    \"RGB_band\": \"G\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"3\",\n                    \"band_name\": \"Red\",\n                    \"RGB_band\": \"R\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"4\",\n                    \"band_name\": \"05_-_Vegetation_Red_Edge\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"5\",\n                    \"band_name\": \"06_-_Vegetation_Red_Edge\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"6\",\n                    \"band_name\": \"07_-_Vegetation_Red_Edge\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"7\",\n                    \"band_name\": \"08_-_NIR\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"8\",\n                    \"band_name\": \"08A_-_Vegetation_Red_Edge\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"9\",\n                    \"band_name\": \"09_-_Water_vapour\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"10\",\n                    \"band_name\": \"11_-_SWIR\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"11\",\n                    \"band_name\": \"12_-_SWIR\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"12\",\n                    \"band_name\": \"Cloud_Probability\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n            ],\n            \"connector\": \"sentinelhub\",\n            \"collection\": \"s2_l2a\",\n            \"modality_tag\": \"S2L1C\",\n            \"file_suffix\": \"_S2Hand.tif\"\n        },\n        {\n            \"bands\": [\n                {\"index\": \"0\", \"band_name\": \"VV (Gray)\", \"description\": \"\"},\n                {\"index\": \"1\", \"band_name\": \"VH\", \"description\": \"\"},\n            ],\n            \"connector\": \"sentinelhub\",\n            \"collection\": \"s1_grd\",\n            \"modality_tag\": \"S1GRD\",\n            \"align_dates\": \"true\",\n            \"file_suffix\": \"_S1Hand.tif\",\n            \"scaling_factor\": [1, 1],\n        },\n    ],\n    \"label_categories\": [\n        {\"id\": \"0\", \"name\": \"No Floods\", \"description\": \"Flooding assets\"},\n        {\"id\": \"1\", \"name\": \"Floods\", \"description\": \"Flooding assets\"},\n    ],\n    \"dataset_url\": uploaded_links[\"download_url\"],\n    \"description\": \"Flood data from places\",\n    \"label_suffix\": \"_LabelHand.tif\",\n    \"purpose\": \"Segmentation\",\n}\n</pre> # Multi-modal data # Edit the details in the dict and dataset_url below to suit your dataset  multi_modal_datasetDict = {     \"dataset_name\": \"Sentinel Flood Multimodal Test\",     \"data_sources\": [         {             \"bands\": [                 {                     \"index\": \"0\",                     \"band_name\": \"Coastal_aerosol\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"1\",                     \"band_name\": \"Blue\",                     \"RGB_band\": \"B\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"2\",                     \"band_name\": \"Green\",                     \"RGB_band\": \"G\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"3\",                     \"band_name\": \"Red\",                     \"RGB_band\": \"R\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"4\",                     \"band_name\": \"05_-_Vegetation_Red_Edge\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"5\",                     \"band_name\": \"06_-_Vegetation_Red_Edge\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"6\",                     \"band_name\": \"07_-_Vegetation_Red_Edge\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"7\",                     \"band_name\": \"08_-_NIR\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"8\",                     \"band_name\": \"08A_-_Vegetation_Red_Edge\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"9\",                     \"band_name\": \"09_-_Water_vapour\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"10\",                     \"band_name\": \"11_-_SWIR\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"11\",                     \"band_name\": \"12_-_SWIR\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"12\",                     \"band_name\": \"Cloud_Probability\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },             ],             \"connector\": \"sentinelhub\",             \"collection\": \"s2_l2a\",             \"modality_tag\": \"S2L1C\",             \"file_suffix\": \"_S2Hand.tif\"         },         {             \"bands\": [                 {\"index\": \"0\", \"band_name\": \"VV (Gray)\", \"description\": \"\"},                 {\"index\": \"1\", \"band_name\": \"VH\", \"description\": \"\"},             ],             \"connector\": \"sentinelhub\",             \"collection\": \"s1_grd\",             \"modality_tag\": \"S1GRD\",             \"align_dates\": \"true\",             \"file_suffix\": \"_S1Hand.tif\",             \"scaling_factor\": [1, 1],         },     ],     \"label_categories\": [         {\"id\": \"0\", \"name\": \"No Floods\", \"description\": \"Flooding assets\"},         {\"id\": \"1\", \"name\": \"Floods\", \"description\": \"Flooding assets\"},     ],     \"dataset_url\": uploaded_links[\"download_url\"],     \"description\": \"Flood data from places\",     \"label_suffix\": \"_LabelHand.tif\",     \"purpose\": \"Segmentation\", } In\u00a0[\u00a0]: Copied! <pre># Unimodal data\n# Edit the details in the dict and dataset_url below to suit your dataset\nunimodal_datasetDict = {\n    \"dataset_name\": \"Inria Dataset Buildings dataset\",\n    \"data_sources\": [\n        {\n            \"bands\": [\n                {\n                    \"index\": \"0\",\n                    \"band_name\": \"Red\",\n                    \"RGB_band\": \"R\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"1\",\n                    \"band_name\": \"Green\",\n                    \"RGB_band\": \"G\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n                {\n                    \"index\": \"2\",\n                    \"band_name\": \"Blue\",\n                    \"RGB_band\": \"B\",\n                    \"description\": \"\",\n                    \"scaling_factor\": \"1\",\n                },\n            ],\n            \"connector\": \"sentinelhub\",\n            \"collection\": \"hls_l30\",\n            \"modality_tag\": \"HLS_L30\",\n            \"file_suffix\": \"_train.tif\",\n        }\n    ],\n    \"label_categories\": [\n        {\"id\": \"0\", \"name\": \"No buildings\", \"description\": \"Building assets\"},\n        {\"id\": \"1\", \"name\": \"Buildings\", \"description\": \"Building assets\"},\n    ],\n    \"dataset_url\": uploaded_links[\"download_url\"],\n    \"description\": \"Inria building labeling dataset\",\n    \"label_suffix\": \"_label.tif\",\n    \"purpose\": \"Segmentation\",\n}\n</pre> # Unimodal data # Edit the details in the dict and dataset_url below to suit your dataset unimodal_datasetDict = {     \"dataset_name\": \"Inria Dataset Buildings dataset\",     \"data_sources\": [         {             \"bands\": [                 {                     \"index\": \"0\",                     \"band_name\": \"Red\",                     \"RGB_band\": \"R\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"1\",                     \"band_name\": \"Green\",                     \"RGB_band\": \"G\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },                 {                     \"index\": \"2\",                     \"band_name\": \"Blue\",                     \"RGB_band\": \"B\",                     \"description\": \"\",                     \"scaling_factor\": \"1\",                 },             ],             \"connector\": \"sentinelhub\",             \"collection\": \"hls_l30\",             \"modality_tag\": \"HLS_L30\",             \"file_suffix\": \"_train.tif\",         }     ],     \"label_categories\": [         {\"id\": \"0\", \"name\": \"No buildings\", \"description\": \"Building assets\"},         {\"id\": \"1\", \"name\": \"Buildings\", \"description\": \"Building assets\"},     ],     \"dataset_url\": uploaded_links[\"download_url\"],     \"description\": \"Inria building labeling dataset\",     \"label_suffix\": \"_label.tif\",     \"purpose\": \"Segmentation\", } <p>Once we have prepared the dataset onboading payload, we can use the <code>onboard_dataset</code> function to onboard the dataset to studio.  This sends the payload to studio backend api and the dataset onboarding process is triggered.  This involves downloading the data, validating it, calculating statistics and metadata, before storing it ready for model tuning.</p> In\u00a0[\u00a0]: Copied! <pre># Replace the data with the correct dataset payload\nonboard_response = gfm_client.onboard_dataset(data=unimodal_datasetDict)\ndisplay(json.dumps(onboard_response, indent=2))\n</pre> # Replace the data with the correct dataset payload onboard_response = gfm_client.onboard_dataset(data=unimodal_datasetDict) display(json.dumps(onboard_response, indent=2))  In\u00a0[\u00a0]: Copied! <pre># poll onboarding status\ngfm_client.poll_onboard_dataset_until_finished(onboard_response[\"dataset_id\"])\n</pre> # poll onboarding status gfm_client.poll_onboard_dataset_until_finished(onboard_response[\"dataset_id\"]) In\u00a0[\u00a0]: Copied! <pre>gfm_client.get_dataset(dataset_id=onboard_response[\"dataset_id\"])\n</pre> gfm_client.get_dataset(dataset_id=onboard_response[\"dataset_id\"])"},{"location":"examples/dataset-onboarding/001-Introduction-to-Onboarding-Tuning-Data/#001-introduction-to-onboarding-tuning-data","title":"001-Introduction-to-Onboarding-Tuning-Data\u00b6","text":"<p>\ud83d\udce5 Download 001-Introduction-to-Onboarding-Tuning-Data.ipynb and try it out</p>"},{"location":"examples/dataset-onboarding/001-Introduction-to-Onboarding-Tuning-Data/#introduction","title":"Introduction\u00b6","text":"<p>This notebook is intended to be a guide to onboarding a new fine-tuning dataset Geospatial Studio using the python SDK.</p> <p>For more information about the Geospatial Studio see the docs page: Geospatial Studio Docs</p> <p>For more information about the Geospatial Studio SDK and all the functions available through it, see the SDK docs page: Geospatial Studio SDK Docs</p>"},{"location":"examples/dataset-onboarding/001-Introduction-to-Onboarding-Tuning-Data/#prerequisites","title":"Prerequisites\u00b6","text":"<ol> <li>Access to a deploy instance of the Geospatial Studio.</li> <li>Ability to run and edit a copy of this notebook.</li> <li>A sample dataset you want to onboard</li> </ol>"},{"location":"examples/dataset-onboarding/001-Introduction-to-Onboarding-Tuning-Data/#install-sdk","title":"Install SDK:\u00b6","text":"<ol> <li><p>Prepare a python 3.9+ environment, however you normally do that (e.g. conda, pyenv, poetry, etc.) and activate this new environment.</p> </li> <li><p>Install Jupyter into that environment: <code>python -m pip install --upgrade pip</code> then <code>pip install notebook</code></p> </li> <li><p>Install the SDK with: <code>python -m pip install geostudio</code></p> </li> </ol>"},{"location":"examples/dataset-onboarding/001-Introduction-to-Onboarding-Tuning-Data/#connecting-to-the-platform","title":"Connecting to the platform\u00b6","text":"<p>First, we set up the connection to the platform backend.  To do this we need the base url for the studio UI and an API key.</p> <p>To get an API Key:</p> <ol> <li>Go to the Geospatial Studio UI page and navigate to the Manage your API keys link.</li> <li>This should pop-up a window where you can generate, access and delete your api keys. NB: every user is limited to a maximum of two activate api keys at any one time.</li> </ol> <p>Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:</p> <pre>echo \"GEOSTUDIO_API_KEY=&lt;paste_api_key_here&gt;\" &gt; .geostudio_config_file\necho \"BASE_STUDIO_UI_URL=&lt;paste_ui_base_url_here&gt;\" &gt;&gt; .geostudio_config_file\n</pre> <p>Copy and paste the file path to this credentials file in call below.</p>"},{"location":"examples/dataset-onboarding/001-Introduction-to-Onboarding-Tuning-Data/#list-and-explore-existing-datasets-in-the-platform","title":"List and explore existing datasets in the platform\u00b6","text":""},{"location":"examples/dataset-onboarding/001-Introduction-to-Onboarding-Tuning-Data/#onboard-a-new-dataset","title":"Onboard a new Dataset\u00b6","text":"<p>In order to onboard your dataset to the Geospatial Studio, you need to have a direct download URL pointing to a zip file of the dataset. You can use this dataset url as an example to go through this notebook.</p> <p>If you have the dataset locally, you can use Box, OneDrive or any other cloud storage you are used to.</p> <p>Optionally, you can upload your data to a temporary location in the cloud (with in Studio object storage) and get a url which can be used to pass to the onboarding process. (NB: the same upload function can be useful for pushing files for inferecnce or to processing pipelines.)</p> <p>The dataset needs to packaged as a zip file.</p> <p>Optional: zip data files for upload:</p> <p><code>zip -j flooding-dataset-upload.zip /Downloads/flooding-dataset-upload/*</code></p>"},{"location":"examples/dataset-onboarding/001-Introduction-to-Onboarding-Tuning-Data/#onboard-the-dataset-to-the-dataset-factory","title":"Onboard the dataset to the dataset factory\u00b6","text":"<p>Now we provide information about the dataset, including name, description, data and label file suffixes, dataset purpose, data sources, etc. Below is an example payload that defines most of the values you will need to onboard a dataset to the Studio. For more information on what you can provide during the onboarding process, check out the SDK Documentation</p> <p>The Geospatial Studio allows users to onboard either multi-modal data or uni-modal data. For the multi-modal data, users shall provide, as a list, a different data source for each input modality of the dataset.</p>"},{"location":"examples/dataset-onboarding/001-Introduction-to-Onboarding-Tuning-Data/#monitor-onboarding-status","title":"Monitor onboarding status\u00b6","text":"<p>You can then monitor the status of the onboarding process through the API with the <code>get_dataset()</code> function or polling function.  You can alternatively monitor progress and view the dataset in the UI.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/","title":"Mapping wildfire burn scars using HLS data","text":"In\u00a0[\u00a0]: Copied! <pre># Import the required packages\nimport json\nimport rasterio\nimport matplotlib.pyplot as plt\n\nimport urllib3\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\nfrom geostudio import Client\nfrom geostudio import gswidgets\n</pre> # Import the required packages import json import rasterio import matplotlib.pyplot as plt  import urllib3 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  from geostudio import Client from geostudio import gswidgets In\u00a0[\u00a0]: Copied! <pre>#############################################################\n# Initialize Geostudio client using a geostudio config file\n#############################################################\ngfm_client = Client(geostudio_config_file=\".geostudio_config_file\")\n</pre> ############################################################# # Initialize Geostudio client using a geostudio config file ############################################################# gfm_client = Client(geostudio_config_file=\".geostudio_config_file\") In\u00a0[\u00a0]: Copied! <pre># # (Optional) If you wish to upload the data archive through the studio, you can use this function. Copy the path to your zipped dataset below.\n# uploaded_links = gfm_client.upload_file('../../../geobench-datasets/burn-scar-training-data.zip')\n# uploaded_links\n</pre> # # (Optional) If you wish to upload the data archive through the studio, you can use this function. Copy the path to your zipped dataset below. # uploaded_links = gfm_client.upload_file('../../../geobench-datasets/burn-scar-training-data.zip') # uploaded_links In\u00a0[\u00a0]: Copied! <pre># Explore datasets in the Studio\n\ngfm_client.list_datasets(output=\"df\")\n</pre> # Explore datasets in the Studio  gfm_client.list_datasets(output=\"df\") In\u00a0[\u00a0]: Copied! <pre># Copy the dataset_id of the dataset you want to explore further and replace with the dataset id below\n\ngfm_client.get_dataset(\"geodata-yqesa6ozmzcuuauggkt9kq\", output=\"json\")\n</pre> # Copy the dataset_id of the dataset you want to explore further and replace with the dataset id below  gfm_client.get_dataset(\"geodata-yqesa6ozmzcuuauggkt9kq\", output=\"json\") In\u00a0[\u00a0]: Copied! <pre># Edit the details in the dict and dataset_url below to suit your dataset\n\ndataset_url = 'https://s3.us-east.cloud-object-storage.appdomain.cloud/geospatial-studio-example-data/burn-scar-training-data.zip'\n\ndataset_dict = {\n    \"purpose\": \"Segmentation\",\n    \"dataset_url\": dataset_url,\n    \"label_suffix\": \".mask.tif\",\n    \"dataset_name\": \"Burn Scars SDK demo\",\n    \"description\": \"Burn Scars SDK data\",\n    \"data_sources\": [\n        {\n            \"bands\": [\n                {\"index\": \"0\", \"band_name\": \"Blue\", \"scaling_factor\": 0.0001, \"RGB_band\": \"B\"},\n                {\"index\": \"1\", \"band_name\": \"Green\", \"scaling_factor\": 0.0001, \"RGB_band\": \"G\"},\n                {\"index\": \"2\", \"band_name\": \"Red\", \"scaling_factor\": 0.0001, \"RGB_band\": \"R\"},\n                {\"index\": \"3\", \"band_name\": \"NIR_Narrow\", \"scaling_factor\": 0.0001},\n                {\"index\": \"4\", \"band_name\": \"SWIR1\", \"scaling_factor\": 0.0001},\n                {\"index\": \"5\", \"band_name\": \"SWIR2\", \"scaling_factor\": 0.0001}\n            ],\n            \"connector\": \"sentinelhub\",\n            \"collection\": \"hls_l30\",\n            \"file_suffix\": \"_merged.tif\",\n            \"modality_tag\": \"HLS_L30\"\n        }\n    ],\n    \"label_categories\": [\n        {\"id\": \"-1\", \"name\": \"Igone\", \"color\": \"#000000\", \"opacity\": \"0\", \"weight\": None},\n        {\"id\": \"0\", \"name\": \"NoData\", \"color\": \"#000000\", \"opacity\": \"0\", \"weight\": None},\n        {\"id\": \"1\", \"name\": \"BurnScar\", \"color\": \"#ea7171\", \"opacity\": 1, \"weight\": None}\n    ],\n    \"version\": \"v2\"\n}\n</pre> # Edit the details in the dict and dataset_url below to suit your dataset  dataset_url = 'https://s3.us-east.cloud-object-storage.appdomain.cloud/geospatial-studio-example-data/burn-scar-training-data.zip'  dataset_dict = {     \"purpose\": \"Segmentation\",     \"dataset_url\": dataset_url,     \"label_suffix\": \".mask.tif\",     \"dataset_name\": \"Burn Scars SDK demo\",     \"description\": \"Burn Scars SDK data\",     \"data_sources\": [         {             \"bands\": [                 {\"index\": \"0\", \"band_name\": \"Blue\", \"scaling_factor\": 0.0001, \"RGB_band\": \"B\"},                 {\"index\": \"1\", \"band_name\": \"Green\", \"scaling_factor\": 0.0001, \"RGB_band\": \"G\"},                 {\"index\": \"2\", \"band_name\": \"Red\", \"scaling_factor\": 0.0001, \"RGB_band\": \"R\"},                 {\"index\": \"3\", \"band_name\": \"NIR_Narrow\", \"scaling_factor\": 0.0001},                 {\"index\": \"4\", \"band_name\": \"SWIR1\", \"scaling_factor\": 0.0001},                 {\"index\": \"5\", \"band_name\": \"SWIR2\", \"scaling_factor\": 0.0001}             ],             \"connector\": \"sentinelhub\",             \"collection\": \"hls_l30\",             \"file_suffix\": \"_merged.tif\",             \"modality_tag\": \"HLS_L30\"         }     ],     \"label_categories\": [         {\"id\": \"-1\", \"name\": \"Igone\", \"color\": \"#000000\", \"opacity\": \"0\", \"weight\": None},         {\"id\": \"0\", \"name\": \"NoData\", \"color\": \"#000000\", \"opacity\": \"0\", \"weight\": None},         {\"id\": \"1\", \"name\": \"BurnScar\", \"color\": \"#ea7171\", \"opacity\": 1, \"weight\": None}     ],     \"version\": \"v2\" } In\u00a0[\u00a0]: Copied! <pre># start onboarding process\n\nonboard_response = gfm_client.onboard_dataset(dataset_dict)\ndisplay(json.dumps(onboard_response, indent=2))\n</pre> # start onboarding process  onboard_response = gfm_client.onboard_dataset(dataset_dict) display(json.dumps(onboard_response, indent=2)) In\u00a0[\u00a0]: Copied! <pre># Poll onboarding status\ngfm_client.poll_onboard_dataset_until_finished(onboard_response[\"dataset_id\"])\n</pre> # Poll onboarding status gfm_client.poll_onboard_dataset_until_finished(onboard_response[\"dataset_id\"]) In\u00a0[\u00a0]: Copied! <pre>tasks = gfm_client.list_tune_templates(output=\"df\")\ndisplay(tasks[['name','description', 'id','created_by','updated_at']])\n</pre> tasks = gfm_client.list_tune_templates(output=\"df\") display(tasks[['name','description', 'id','created_by','updated_at']]) In\u00a0[\u00a0]: Copied! <pre># Choose a task from the options above.  Copy and paste the id into the variable, tid, below.\ntask_id = 'e4791b2c-bb17-4a5e-9f05-1be5411a4fa6'\n</pre> # Choose a task from the options above.  Copy and paste the id into the variable, tid, below. task_id = 'e4791b2c-bb17-4a5e-9f05-1be5411a4fa6' In\u00a0[\u00a0]: Copied! <pre># Now we can view the full meta-data and details of the selected task\ntask_meta = gfm_client.get_task(task_id, output=\"df\")\ntask_meta\n</pre> # Now we can view the full meta-data and details of the selected task task_meta = gfm_client.get_task(task_id, output=\"df\") task_meta <p>If you are happy with your choice, you can decide which (if any) hyperparameters you want to set (otherwise defaults will be used).</p> <p>Here we can see the available parameters and their associated defaults.  To update a parameter you can just set values in the dictionary (as shown below for <code>max_epochs</code>).</p> In\u00a0[\u00a0]: Copied! <pre>task_params = gfm_client.get_task_param_defaults(task_id)\ntask_params\n</pre> task_params = gfm_client.get_task_param_defaults(task_id) task_params In\u00a0[\u00a0]: Copied! <pre>task_params['runner']['max_epochs'] = '2'\ntask_params['optimizer']['type'] = 'AdamW'\ntask_params['data']['batch_size'] = 4\n</pre> task_params['runner']['max_epochs'] = '2' task_params['optimizer']['type'] = 'AdamW' task_params['data']['batch_size'] = 4  In\u00a0[\u00a0]: Copied! <pre>base = gfm_client.list_base_models(output='df')\ndisplay(base[['name','description','id','updated_at']])\n</pre> base = gfm_client.list_base_models(output='df') display(base[['name','description','id','updated_at']]) In\u00a0[\u00a0]: Copied! <pre># copy and paste the id of the base model you wish to use\n\nbase_model_id = 'f24fad3d-d5b5-40aa-a8ce-700a1a3d0a83'\n</pre> # copy and paste the id of the base model you wish to use  base_model_id = 'f24fad3d-d5b5-40aa-a8ce-700a1a3d0a83' In\u00a0[\u00a0]: Copied! <pre># create the tune payload\n\ndataset_id = onboard_response[\"dataset_id\"] # the dataset_id of the dataset you onboarded above\n\ntune_payload = {\n  \"name\": \"burn-scars-demo\",\n  \"description\": \"Segmentation\",\n  \"dataset_id\": dataset_id,\n  \"base_model_id\": base_model_id,\n  \"tune_template_id\": task_id,\n  \"model_parameters\": task_params # uncomment this line if you customised task_params in the cells above otherwise, defaults will be used\n}\n\nprint(json.dumps(tune_payload, indent=2))\n</pre> # create the tune payload  dataset_id = onboard_response[\"dataset_id\"] # the dataset_id of the dataset you onboarded above  tune_payload = {   \"name\": \"burn-scars-demo\",   \"description\": \"Segmentation\",   \"dataset_id\": dataset_id,   \"base_model_id\": base_model_id,   \"tune_template_id\": task_id,   \"model_parameters\": task_params # uncomment this line if you customised task_params in the cells above otherwise, defaults will be used }  print(json.dumps(tune_payload, indent=2)) In\u00a0[\u00a0]: Copied! <pre>submitted = gfm_client.submit_tune(\n        data = tune_payload,\n        output = 'json'\n)\n\nprint(submitted)\n</pre> submitted = gfm_client.submit_tune(         data = tune_payload,         output = 'json' )  print(submitted) In\u00a0[\u00a0]: Copied! <pre># Poll fine tuning status\ngfm_client.poll_finetuning_until_finished(tune_id=submitted[\"tune_id\"])\n</pre> # Poll fine tuning status gfm_client.poll_finetuning_until_finished(tune_id=submitted[\"tune_id\"]) In\u00a0[\u00a0]: Copied! <pre>tune_id = submitted[\"tune_id\"]\n\ntune_info = gfm_client.get_tune(tune_id, output='df')\ntune_info\n</pre> tune_id = submitted[\"tune_id\"]  tune_info = gfm_client.get_tune(tune_id, output='df') tune_info <p>Once it has started training, you will also be able to access the training metrics.  The <code>get_tune_metrics_df</code> function returns a dataframe containing the up-to-date training metrics, which you are free to explore and analyse.  In addition to that, you can simply plot the training and validation loss and multi-class accuracy using the <code>plot_tune_metrics</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>gfm_client.get_tune_metrics_df(tune_id)\n</pre> gfm_client.get_tune_metrics_df(tune_id) In\u00a0[\u00a0]: Copied! <pre>gswidgets.plot_tune_metrics(client=gfm_client, tune_id=tune_id)\n</pre> gswidgets.plot_tune_metrics(client=gfm_client, tune_id=tune_id) <p>Once your model is finished training and you are happy with the metrics (and images in MLflow), you can run some inference in test mode through the inference service.</p> In\u00a0[\u00a0]: Copied! <pre># define the inference payload\n\nbbox = [-121.837006,39.826468,-121.641312,40.038655]\n\nrequest_payload = {\n\t\"description\": \"Park Fire 2024 SDK\",\n\t\"location\": \"Red Bluff, California, United States\",\n\t\"spatial_domain\": {\n\t\t\t\"bbox\": [bbox],\n\t\t\t\"polygons\": [],\n\t\t\t\"tiles\": [],\n\t\t\t\"urls\": []\n\t},\n\t\"temporal_domain\": [\n\t\t\t\"2024-08-12_2024-08-13\"\n\t]\n}\n</pre> # define the inference payload  bbox = [-121.837006,39.826468,-121.641312,40.038655]  request_payload = { \t\"description\": \"Park Fire 2024 SDK\", \t\"location\": \"Red Bluff, California, United States\", \t\"spatial_domain\": { \t\t\t\"bbox\": [bbox], \t\t\t\"polygons\": [], \t\t\t\"tiles\": [], \t\t\t\"urls\": [] \t}, \t\"temporal_domain\": [ \t\t\t\"2024-08-12_2024-08-13\" \t] } <p>Once you have defined your inference payload, you can now run it with a test inference.  As with the main inference service, this is done by either supplying a bounding box (<code>bbox</code>), time range (<code>start_date</code>, <code>end_date</code>) and the <code>model_id</code>.  You can then monitor it and visualise the outputs either through the SDK, or in the UI.</p> In\u00a0[\u00a0]: Copied! <pre># Now submit the test inference request\ninference_response = gfm_client.try_out_tune(tune_id=tune_id, data=request_payload)\ninference_response\n</pre> # Now submit the test inference request inference_response = gfm_client.try_out_tune(tune_id=tune_id, data=request_payload) inference_response In\u00a0[\u00a0]: Copied! <pre>gfm_client.get_inference(inference_response['id'], output=\"df\")\n</pre> gfm_client.get_inference(inference_response['id'], output=\"df\") In\u00a0[\u00a0]: Copied! <pre># Poll inference status\ngfm_client.poll_inference_until_finished(inference_id=inference_response['id'], poll_frequency=10)\n</pre> # Poll inference status gfm_client.poll_inference_until_finished(inference_id=inference_response['id'], poll_frequency=10) In\u00a0[\u00a0]: Copied! <pre># Get the inference tasks list\ninf_tasks_res = gfm_client.get_inference_tasks(inference_response[\"id\"])\ninf_tasks_res\n</pre> # Get the inference tasks list inf_tasks_res = gfm_client.get_inference_tasks(inference_response[\"id\"]) inf_tasks_res In\u00a0[\u00a0]: Copied! <pre>df = gfm_client.inference_task_status_df(inference_response[\"id\"])\n\n\ndisplay(df.style.map(gswidgets.color_inference_tasks_by_status))\n</pre> df = gfm_client.inference_task_status_df(inference_response[\"id\"])   display(df.style.map(gswidgets.color_inference_tasks_by_status)) In\u00a0[\u00a0]: Copied! <pre>gswidgets.view_inference_process_timeline(gfm_client, inference_id = inference_response[\"id\"])\n</pre> gswidgets.view_inference_process_timeline(gfm_client, inference_id = inference_response[\"id\"]) <p>Next, Identify the task you want to view from the response above, ensure status of the task is FINISHED and set <code>selected_task</code> variable below to the task number at the end of the task id string. For example, if <code>task_id</code> is \"6d1149fa-302d-4612-82dd-5879fc06081d-task_0\", selected_task woul be 0</p> In\u00a0[\u00a0]: Copied! <pre># Select a task to view\n\nselected_task = 0 \nselected_task_id = f\"{inference_response[\"id\"]}-task_{selected_task}\"\n</pre> # Select a task to view  selected_task = 0  selected_task_id = f\"{inference_response[\"id\"]}-task_{selected_task}\" In\u00a0[\u00a0]: Copied! <pre># Download task output files\n\ngswidgets.fileDownloaderTasks(client=gfm_client, task_id=selected_task_id)\n</pre> # Download task output files  gswidgets.fileDownloaderTasks(client=gfm_client, task_id=selected_task_id) In\u00a0[\u00a0]: Copied! <pre># Paste the name (+path) to one of the files you downloaded and select the band you want to load+plot\nfilename = '9a0588de-8858-4c31-8c6d-91b9c74333d1-task_0_HLS_L30_2024-08-12__merged.tif.tif'\nband_number = 1\n\n# open the file and read the band and metadata with rasterio\nwith rasterio.open(filename) as fp:\n    data = fp.read(band_number)\n    bounds = fp.bounds\n\n\nprint(\"Image dimensions: \" + str(data.shape))\n\nplt.imshow(data, extent=[bounds.left, bounds.right, bounds.bottom, bounds.top])\nplt.xlabel('Longitude'); plt.xlabel('Latitude')\n</pre> # Paste the name (+path) to one of the files you downloaded and select the band you want to load+plot filename = '9a0588de-8858-4c31-8c6d-91b9c74333d1-task_0_HLS_L30_2024-08-12__merged.tif.tif' band_number = 1  # open the file and read the band and metadata with rasterio with rasterio.open(filename) as fp:     data = fp.read(band_number)     bounds = fp.bounds   print(\"Image dimensions: \" + str(data.shape))  plt.imshow(data, extent=[bounds.left, bounds.right, bounds.bottom, bounds.top]) plt.xlabel('Longitude'); plt.xlabel('Latitude') In\u00a0[\u00a0]: Copied! <pre># Visualize output files with the SDK\ngswidgets.inferenceTaskViewer(client=gfm_client, task_id=selected_task_id)\n</pre> # Visualize output files with the SDK gswidgets.inferenceTaskViewer(client=gfm_client, task_id=selected_task_id)"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#mapping-wildfire-burn-scars-using-hls-data","title":"Mapping wildfire burn scars using HLS data\u00b6","text":"<p>\ud83d\udce5 Download GeospatialStudio-Walkthrough-BurnScars.ipynb and try it out</p> <p>Imagine that you work in disaster response and need a rapid way to map the extent areas burned by wildfires.  You need to do this in an automated, scalable manner.  We can achieve this using an AI model which ingests satellite data (in this instance the NASA Harmonized Landsat Sentinel2 dataset) and outputs a map of burned area.  We could potentially then integrate the burned area extent with details of infrastructure or assets to quantify impact.</p> <p></p> <p>In this walkthrough we will assume that a model doesn't exist yet and we want to train a new model.  We will then show how to drive the model to map impact.</p> <p>We will walk through the following steps as part of this walkthrough:</p> <ol> <li>Upload and onboarding of data</li> <li>Configuring and submitting a tuning task</li> <li>Monitoring model training</li> <li>Testing and validation of the outputs</li> </ol>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#pre-requisites","title":"Pre-requisites\u00b6","text":"<p>This walkthrough assumes you have the data downloaded locally, it can be downloaded here: https://s3.us-east.cloud-object-storage.appdomain.cloud/geospatial-studio-example-data/burn-scar-training-data.zip</p> <p>For more information about the Geospatial Studio see the docs page: Geospatial Studio Docs</p> <p>For more information about the Geospatial Studio SDK and all the functions available through it, see the SDK docs page: Geospatial Studio SDK Docs</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#get-the-training-data","title":"Get the training data\u00b6","text":"<p>To train the AI model, we will need some training data which contains the input data and the labels (aka ground truth burn scar extent).  To train our model we will use the following dataset: https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars</p> <p>We can download it here: https://s3.us-east.cloud-object-storage.appdomain.cloud/geospatial-studio-example-data/burn-scar-training-data.zip</p> <p>Download and unzip the above archive and if you wish you can explore the data with QGIS (or any similar tool).</p> <p>NB: If you already have the data in online you can skip this step.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#connecting-to-the-platform","title":"Connecting to the platform\u00b6","text":"<p>First, we set up the connection to the platform backend.  To do this we need the base url for the studio UI and an API key.</p> <p>To get an API Key:</p> <ol> <li>Go to the Geospatial Studio UI page and navigate to the Manage your API keys link.</li> <li>This should pop-up a window where you can generate, access and delete your api keys. NB: every user is limited to a maximum of two activate api keys at any one time.</li> </ol> <p>Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:</p> <pre>echo \"GEOSTUDIO_API_KEY=&lt;paste_api_key_here&gt;\" &gt; .geostudio_config_file\necho \"BASE_STUDIO_UI_URL=&lt;paste_ui_base_url_here&gt;\" &gt;&gt; .geostudio_config_file\n</pre> <p>Copy and paste the file path to this credentials file in call below.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#data-onboarding","title":"Data onboarding\u00b6","text":"<p>In order to onboard your dataset to the Geospatial Studio, you need to have a direct download URL pointing to a zip file of the dataset.  You can use Box, OneDrive or any other cloud storage you are used to, but in addition, to make this easier for you, there is a function which will upload your data to a temporary location in the cloud (with in Studio object storage) and provide you with a url which can be used to pass to the onboarding process.  NB: the same upload function can be useful for pushing files for inferecnce or to processing pipelines.</p> <p>If needed you can package a set of files for upload, you can use a command like:</p> <pre>zip -j burn-scars-upload.zip /Users/blair/Downloads/burn-scar-upload/*\n</pre>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#onboard-the-dataset-to-the-dataset-factory","title":"Onboard the dataset to the dataset factory\u00b6","text":"<p>Now we use the SDK to provide the information about the dataset, including name, suffixes etc.  A more detailed description of the dataset details is provided in the UI walkthrough.  Here the SDK will do some basic sanity checks, and will (if possible) check that you have matching data and label pairs, and check that you have specified the correct number of bands.  This creates dictionary with the required details, which you can then submit to the platform using the step below.</p> <p>Note:</p> <ul> <li>Change the value of the <code>dataset_url</code>variable below to the url of your zip file or the <code>download_url</code> link you got from using the SDK upload_file function above</li> <li>Change the values of <code>training_data_suffix</code> and <code>label_suffix</code> to the suffixes of your training and label data files respectively if using a different dataset (aside from the one provided)</li> <li>Change the <code>label_categories</code>, <code>custom_bands</code> and descriptions to those that match your dataset</li> </ul>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#onboard-a-new-dataset","title":"Onboard a new dataset\u00b6","text":""},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#fine-tuning-submission","title":"Fine-tuning submission\u00b6","text":"<p>Once the data is onboarded, you are ready to setup your tuning task.  In order to run a fine-tuning task, you need to select the following items:</p> <ul> <li>tuning task type - what type of learning task are you attempting?  segmentation, regression etc</li> <li>fine-tuning dataset - what dataset will you use to train the model for your particular application?</li> <li>base foundation model - which geospatial foundation model will you use as the starting point for your tuning task?</li> </ul> <p>Below we walk you through how to use the Geospatial Studio SDK to see what options are available in the platform for each of these, then once you have made your selection, how we configure our task and submit it.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#tuning-task","title":"Tuning task\u00b6","text":"<p>The tuning task tells the model what type of task it is (segmentation, regression etc), and exposes a range of optional hyperparameters which the user can set.  These all have reasonable defaults, but it gives uses the possibility to configure the model training how they wish.  Below, we will check what task templates are available to us, and then update some parameters.</p> <p>Advanced users can create and upload new task templates to the platform, and instructions are found in the relevant notebook and documentation.  The templates are for Terratorch (the backend tuning library), and more details of Terratroch and configuration options can be found here: https://ibm.github.io/terratorch/</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#base-foundation-model","title":"Base foundation model\u00b6","text":"<p>The base model is the foundation model (encoder) which has been pre-trained and has the basic understanding of the data.  More information can currently be found on the different models in the documentation.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#submitting-the-tune","title":"Submitting the tune\u00b6","text":"<p>Now we pull these choices together into a payload which we then submit to the platform.  This will then deploy the job in the backend and we will see below how we can monitor it.  First, we populate the payload so we can check it, then we simply submit.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#monitoring-training","title":"Monitoring training\u00b6","text":"<p>Once the tune has been submitted you can check its status and monitor tuning progress through the SDK.  You can also access the training metrics and images in MLflow.  The <code>get_tune</code> function will give you the meta-data of the tune, including the status.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#testing-your-model","title":"Testing your model\u00b6","text":"<p>To do a test deployment and inference with the model, we need to register the model with the inference service.  To do this you need to select a model style (describing the visulisation style of the model output), and define the data required to feed the model (in the example here it is using Sentinel Hub).  For the data specification, you need to define the data collection and bands from sentinelhub (using the collection and band names for SH).  In addition, if the data to be fed in is returned from SH with a scale factor that needs to be added here too.  Data collection data for HLS are found here: https://docs.sentinel-hub.com/api/latest/data/hls/</p> <p>Example test locations</p> Location Date Bounding box Park Fire, CA, USA (Cohasset, CA) 2024-08-12 [-121.837006, 39.826468, -121.641312, 40.038655] Rhodes, Greece 2023-08-01 [27.91, 35.99, 28.10, 36.25] Rafina, Greece 2018-08-04 [23.92, 38.00, 24.03, 38.08] Bandipura State Forest, Karnataka, India 2019-02-26 [76.503245, 11.631803, 76.690118, 11.762633] Amur Oblast fires, Russia 2018-05-29 [127.589722, 54.055357, 128.960266, 54.701614]"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#try-out-the-model-for-inference","title":"Try out the model for inference\u00b6","text":"<p>Once your model has finished tuning, if you want to run inference as a test you can do by passing either a location (bbox) or a url to a pre-prepared files.  The steps to test the model are:</p> <ol> <li>Define the inference payload</li> <li>Try out the tune temporarily</li> </ol>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#monitoring-your-inference-task","title":"Monitoring your inference task\u00b6","text":"<p>Once submitted you can check on progress using the following function which will return all the metadata about the inference task, including the status.  You can optionally use the <code>poll_until_finished</code> to watch the status until it completes.  For a test inference it can take 5-10 minutes, depending on the size of the data query, the size of the model etc.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#accessing-inference-outputs","title":"Accessing inference outputs\u00b6","text":"<p>Once an inference run is completed, the inputs and outputs of each task within an inference are packaged up into a zip file which is uploaded to a url you can use to download the files.</p> <p>To access the inference task files:</p> <ol> <li>Get the inference tasks list</li> <li>Identify the specific inference task you want to view</li> <li>Download task output files</li> </ol>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#visualizing-the-output-of-the-inference-runs","title":"Visualizing the output of the inference runs\u00b6","text":"<p>You can check out the results visually in the Studio UI, or with the quick widget below.  You can alternatively use the SDK to download selected files for further analysis see documentation.</p> <p>We have several options for visualising the data:</p> <ul> <li>we can load the data with a package like rasterio and plot the images, and/or access the values.</li> <li>we could use the widget from the SDK to visualise the chosen files for a inference run. (shown below)</li> <li>view the data in the Geospatial Studio Inference lab UI.</li> <li>load the files in an external software, such as QGIS.</li> </ul>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#load-the-data-with-a-package-rasterio-and-plot-the-images-andor-access-the-values","title":"Load the data with a package rasterio and plot the images, and/or access the values.\u00b6","text":""},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-BurnScars/#visualize-through-the-sdk-widgets","title":"Visualize through the SDK widgets\u00b6","text":""},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/","title":"Mapping flooding using an AI flood model","text":"In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>import os\nimport json\nimport uuid\nimport pandas as pd\nimport rasterio\nfrom rasterio.plot import show\nimport matplotlib.pyplot as plt\n\nimport urllib3\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\nfrom geostudio import Client\nfrom geostudio import gswidgets\n</pre> import os import json import uuid import pandas as pd import rasterio from rasterio.plot import show import matplotlib.pyplot as plt  import urllib3 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  from geostudio import Client from geostudio import gswidgets In\u00a0[\u00a0]: Copied! <pre>#############################################################\n# Initialize Geostudio client using a geostudio config file\n#############################################################\ngfm_client = Client(geostudio_config_file=\".geostudio_config_file\")\n</pre> ############################################################# # Initialize Geostudio client using a geostudio config file ############################################################# gfm_client = Client(geostudio_config_file=\".geostudio_config_file\") In\u00a0[\u00a0]: Copied! <pre># (Optional) If you wish to upload the data archive through the studio, you can uncomment and use this function. \nuploaded_links = gfm_client.upload_file('/Users/beldinemoturi/Downloads/flooding_dataset.zip')\nuploaded_links\n</pre> # (Optional) If you wish to upload the data archive through the studio, you can uncomment and use this function.  uploaded_links = gfm_client.upload_file('/Users/beldinemoturi/Downloads/flooding_dataset.zip') uploaded_links In\u00a0[\u00a0]: Copied! <pre>uploaded_links[\"download_url\"]\n</pre> uploaded_links[\"download_url\"] In\u00a0[\u00a0]: Copied! <pre># Edit the dict below to suit your dataset details.\n\ndataset_dict = {\n    \"dataset_name\": \"Sentinel Flood Multimodal\",\n    \"data_sources\": [\n        {\n            \"bands\": [\n                {\"index\":\"0\", \"band_name\": \"Coastal_aerosol\", \"RGB_band\": \"R\", \"description\": \"\"},\n                {\"index\":\"1\", \"band_name\": \"Blue\", \"RGB_band\": \"G\", \"description\": \"\"},\n                {\"index\":\"2\", \"band_name\": \"Green\", \"RGB_band\": \"B\", \"description\": \"\"},\n                {\"index\":\"3\", \"band_name\": \"Red\", \"description\": \"\"},\n                {\"index\":\"4\", \"band_name\": \"05_-_Vegetation_Red_Edge\", \"description\": \"\"},\n                {\"index\":\"5\", \"band_name\": \"06_-_Vegetation_Red_Edge\", \"description\": \"\"},\n                {\"index\":\"6\", \"band_name\": \"07_-_Vegetation_Red_Edge\", \"description\": \"\"},\n                {\"index\":\"7\", \"band_name\": \"08_-_NIR\", \"description\": \"\"},\n                {\"index\":\"8\", \"band_name\": \"08A_-_Vegetation_Red_Edge\", \"description\": \"\"},\n                {\"index\":\"9\", \"band_name\": \"09_-_Water_vapour\", \"description\": \"\"},\n                {\"index\":\"10\", \"band_name\": \"11_-_SWIR\", \"description\": \"\"},\n                {\"index\":\"11\", \"band_name\": \"12_-_SWIR\", \"description\": \"\"},\n                {\"index\":\"12\", \"band_name\": \"Cloud_Probability\", \"description\": \"\"}\n            ],\n            \"connector\": \"sentinelhub\",\n            \"collection\": \"s2_l2a\",\n            \"modality_tag\": \"S2L1C\",\n            \"file_suffix\": \"_S2Hand.tif\",\n            \"scaling_factor\": [1, 1, 1, 1, 1, 1]\n        },\n        {\n            \"bands\": [\n                {\"index\":\"0\", \"band_name\": \"VV (Gray)\", \"description\": \"\"},\n                {\"index\":\"1\", \"band_name\": \"VH\", \"description\": \"\"}\n            ],\n            \"connector\": \"sentinelhub\",\n            \"collection\": \"s1_grd\",\n            \"modality_tag\": \"S1GRD\",\n            \"align_dates\": \"true\",\n            \"file_suffix\": \"_S1Hand.tif\",\n            \"scaling_factor\": [1, 1]\n        }\n    ],\n    \"label_categories\": [\n        {\"id\": \"0\", \"name\": \"No Floods\", \"description\": \"Flooding assets\"},\n        {\"id\": \"1\", \"name\": \"Floods\", \"description\": \"Flooding assets\"}\n    ],\n    \"dataset_url\": uploaded_links[\"download_url\"],\n    \"description\": \"Flood data from places\",\n    \"label_suffix\": \"_LabelHand.tif\",\n    \"purpose\": \"Segmentation\"\n}\n</pre> # Edit the dict below to suit your dataset details.  dataset_dict = {     \"dataset_name\": \"Sentinel Flood Multimodal\",     \"data_sources\": [         {             \"bands\": [                 {\"index\":\"0\", \"band_name\": \"Coastal_aerosol\", \"RGB_band\": \"R\", \"description\": \"\"},                 {\"index\":\"1\", \"band_name\": \"Blue\", \"RGB_band\": \"G\", \"description\": \"\"},                 {\"index\":\"2\", \"band_name\": \"Green\", \"RGB_band\": \"B\", \"description\": \"\"},                 {\"index\":\"3\", \"band_name\": \"Red\", \"description\": \"\"},                 {\"index\":\"4\", \"band_name\": \"05_-_Vegetation_Red_Edge\", \"description\": \"\"},                 {\"index\":\"5\", \"band_name\": \"06_-_Vegetation_Red_Edge\", \"description\": \"\"},                 {\"index\":\"6\", \"band_name\": \"07_-_Vegetation_Red_Edge\", \"description\": \"\"},                 {\"index\":\"7\", \"band_name\": \"08_-_NIR\", \"description\": \"\"},                 {\"index\":\"8\", \"band_name\": \"08A_-_Vegetation_Red_Edge\", \"description\": \"\"},                 {\"index\":\"9\", \"band_name\": \"09_-_Water_vapour\", \"description\": \"\"},                 {\"index\":\"10\", \"band_name\": \"11_-_SWIR\", \"description\": \"\"},                 {\"index\":\"11\", \"band_name\": \"12_-_SWIR\", \"description\": \"\"},                 {\"index\":\"12\", \"band_name\": \"Cloud_Probability\", \"description\": \"\"}             ],             \"connector\": \"sentinelhub\",             \"collection\": \"s2_l2a\",             \"modality_tag\": \"S2L1C\",             \"file_suffix\": \"_S2Hand.tif\",             \"scaling_factor\": [1, 1, 1, 1, 1, 1]         },         {             \"bands\": [                 {\"index\":\"0\", \"band_name\": \"VV (Gray)\", \"description\": \"\"},                 {\"index\":\"1\", \"band_name\": \"VH\", \"description\": \"\"}             ],             \"connector\": \"sentinelhub\",             \"collection\": \"s1_grd\",             \"modality_tag\": \"S1GRD\",             \"align_dates\": \"true\",             \"file_suffix\": \"_S1Hand.tif\",             \"scaling_factor\": [1, 1]         }     ],     \"label_categories\": [         {\"id\": \"0\", \"name\": \"No Floods\", \"description\": \"Flooding assets\"},         {\"id\": \"1\", \"name\": \"Floods\", \"description\": \"Flooding assets\"}     ],     \"dataset_url\": uploaded_links[\"download_url\"],     \"description\": \"Flood data from places\",     \"label_suffix\": \"_LabelHand.tif\",     \"purpose\": \"Segmentation\" } In\u00a0[\u00a0]: Copied! <pre># [Optional]\n\ngfm_client.pre_scan_dataset({\n  \"dataset_url\": uploaded_links[\"download_url\"],\n  \"label_suffix\": \"_LabelHand.tif\",\n  \"training_data_suffixes\": \n    [\"_S2Hand.tif\", \"_S1Hand.tif\"]\n})\n</pre> # [Optional]  gfm_client.pre_scan_dataset({   \"dataset_url\": uploaded_links[\"download_url\"],   \"label_suffix\": \"_LabelHand.tif\",   \"training_data_suffixes\":      [\"_S2Hand.tif\", \"_S1Hand.tif\"] }) In\u00a0[\u00a0]: Copied! <pre># start onboarding process\n\nonboard_response = gfm_client.onboard_dataset(data=dataset_dict)\ndisplay(json.dumps(onboard_response, indent=2))\n</pre> # start onboarding process  onboard_response = gfm_client.onboard_dataset(data=dataset_dict) display(json.dumps(onboard_response, indent=2))  In\u00a0[\u00a0]: Copied! <pre># list tasks available\n\ntasks = gfm_client.list_tune_templates(output=\"df\")\ndisplay(tasks[['name','description', 'id','created_by','updated_at']])\n</pre> # list tasks available  tasks = gfm_client.list_tune_templates(output=\"df\") display(tasks[['name','description', 'id','created_by','updated_at']]) In\u00a0[\u00a0]: Copied! <pre># Choose a task from the options above.  Copy and paste the id into the variable, task_id, below. For this example, it is a segmentation task since we are classifying flooded and non-flooded areas\ntask_id = '48c878d8-3b05-4ca5-bd89-89400c8790eb'\n</pre> # Choose a task from the options above.  Copy and paste the id into the variable, task_id, below. For this example, it is a segmentation task since we are classifying flooded and non-flooded areas task_id = '48c878d8-3b05-4ca5-bd89-89400c8790eb' In\u00a0[\u00a0]: Copied! <pre># view the full meta-data and details of the selected task\ntask_meta = gfm_client.get_task(task_id)\ntask_meta\n</pre> # view the full meta-data and details of the selected task task_meta = gfm_client.get_task(task_id) task_meta <p>If you are happy with your choice, you can decide which (if any) hyperparameters you want to set (otherwise defaults will be used).</p> <p>Here we can see the available parameters and their associated defaults.  To update a parameter you can just set values in the dictionary (as shown below for <code>max_epochs</code>).</p> In\u00a0[\u00a0]: Copied! <pre># show the default values for parameters\n\ntask_params = gfm_client.get_task_param_defaults(task_id)\ntask_params\n</pre> # show the default values for parameters  task_params = gfm_client.get_task_param_defaults(task_id) task_params In\u00a0[\u00a0]: Copied! <pre># configure the parameters you want \n\ntask_params['runner']['max_epochs'] = '3'\n# task_params['optimizer']['type'] = 'AdamW'\n# task_params['data']['batch_size'] = 4\n</pre> # configure the parameters you want   task_params['runner']['max_epochs'] = '3' # task_params['optimizer']['type'] = 'AdamW' # task_params['data']['batch_size'] = 4 In\u00a0[\u00a0]: Copied! <pre># list foundation models available\n\nbase = gfm_client.list_base_models(output='df')\ndisplay(base[['name','description','id','updated_at']])\n</pre> # list foundation models available  base = gfm_client.list_base_models(output='df') display(base[['name','description','id','updated_at']]) In\u00a0[\u00a0]: Copied! <pre># select base foundation model\nbase_model_id = '55e638d9-7a7c-4e8b-bda2-035b172922af'\n</pre> # select base foundation model base_model_id = '55e638d9-7a7c-4e8b-bda2-035b172922af' In\u00a0[\u00a0]: Copied! <pre># create the tune payload\n\ndataset_id = \"geodata-zvgkj5qqwxbhzzz25qbuxz\" # the dataset_id of the dataset you onboarded above\n\ntune_payload = {\n  \"name\": \"test-fine-tuning-multimodal\",\n  \"description\": \"Segmentation\",\n  \"dataset_id\": dataset_id,\n  \"base_model_id\": base_model_id,\n  \"tune_template_id\": task_id,\n}\n\nprint(json.dumps(tune_payload, indent=2))\n</pre> # create the tune payload  dataset_id = \"geodata-zvgkj5qqwxbhzzz25qbuxz\" # the dataset_id of the dataset you onboarded above  tune_payload = {   \"name\": \"test-fine-tuning-multimodal\",   \"description\": \"Segmentation\",   \"dataset_id\": dataset_id,   \"base_model_id\": base_model_id,   \"tune_template_id\": task_id, }  print(json.dumps(tune_payload, indent=2)) In\u00a0[\u00a0]: Copied! <pre># submit tune\n\nsubmitted = gfm_client.submit_tune(\n        data = tune_payload,\n        output = 'json'\n)\n\nprint(submitted)\n</pre> # submit tune  submitted = gfm_client.submit_tune(         data = tune_payload,         output = 'json' )  print(submitted) In\u00a0[\u00a0]: Copied! <pre># get metadata about the submitted tune\n\ntune_id = submitted.get(\"tune_id\")\n\ntune_info = gfm_client.get_tune(tune_id, output='json')\ntune_info\n</pre> # get metadata about the submitted tune  tune_id = submitted.get(\"tune_id\")  tune_info = gfm_client.get_tune(tune_id, output='json') tune_info <p>Once the model has started training, you will also be able to access the training metrics.  The <code>get_tune_metrics_df</code> function returns a dataframe containing the up-to-date training metrics, which you are free to explore and analyse.  In addition to that, you can simply plot the training and validation loss and multi-class accuracy using the <code>plot_tune_metrics</code> function.</p> In\u00a0[\u00a0]: Copied! <pre># get training metrics\n\nmdf = gfm_client.get_mlflow_metrics(\"geotune-iskjtu463hxou9mukjct9o\")\nmdf.head()\n</pre> # get training metrics  mdf = gfm_client.get_mlflow_metrics(\"geotune-iskjtu463hxou9mukjct9o\") mdf.head() In\u00a0[\u00a0]: Copied! <pre># plot some basic training metrics\n\n# gfm_client.plot_tune_metrics(tune_id)\n</pre> # plot some basic training metrics  # gfm_client.plot_tune_metrics(tune_id)  <p>Once your model is finished training and you are happy with the metrics (and images in MLflow), you can run some inference in test mode through the inference service.</p> In\u00a0[\u00a0]: Copied! <pre># define the inference payload\n\nbbox = [92.40665153547121, 26.1051042015407,92.92535070071905,26.498933088370826]\n\nrequest_payload = {\n\t\"description\": \"Jarani, Nagaon, Nagaon, Assam, India\",\n\t\"location\": \"Jarani, Nagaon, Nagaon, Assam, India\",\n\t\"spatial_domain\": {\n\t\t\t\"bbox\": [bbox],\n\t\t\t\"polygons\": [],\n\t\t\t\"tiles\": [],\n\t\t\t\"urls\": []\n\t},\n\t\"temporal_domain\": [\n\t\t\t\"2024-07-25_2024-07-27\"\n\t]\n}\n</pre> # define the inference payload  bbox = [92.40665153547121, 26.1051042015407,92.92535070071905,26.498933088370826]  request_payload = { \t\"description\": \"Jarani, Nagaon, Nagaon, Assam, India\", \t\"location\": \"Jarani, Nagaon, Nagaon, Assam, India\", \t\"spatial_domain\": { \t\t\t\"bbox\": [bbox], \t\t\t\"polygons\": [], \t\t\t\"tiles\": [], \t\t\t\"urls\": [] \t}, \t\"temporal_domain\": [ \t\t\t\"2024-07-25_2024-07-27\" \t] } <p>Once you have registered the model, you can now run it with a test inference.  As with the main inference service, this is done by either supplying a bounding box (<code>bbox</code>), time range (<code>start_date</code>, <code>end_date</code>) and the <code>model_id</code>.  You can then monitor it and visualise the outputs either through the SDK, or in the UI.</p> In\u00a0[\u00a0]: Copied! <pre># Now submit the test inference request\ninference_response = gfm_client.try_out_tune(tune_id=tune_id, data=request_payload)\ninference_response\n</pre> # Now submit the test inference request inference_response = gfm_client.try_out_tune(tune_id=tune_id, data=request_payload) inference_response In\u00a0[\u00a0]: Copied! <pre># get metadata about the inference task\n\ngfm_client.get_inference(inference_response['id'])\n</pre> # get metadata about the inference task  gfm_client.get_inference(inference_response['id']) In\u00a0[\u00a0]: Copied! <pre># view inference results\n\n# gswidgets.inferenceViewer(gfm_client, inference_response['id'])\n</pre> # view inference results  # gswidgets.inferenceViewer(gfm_client, inference_response['id'])"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#mapping-flooding-using-an-ai-flood-model","title":"Mapping flooding using an AI flood model\u00b6","text":"<p>\ud83d\udce5 Download GeospatialStudio-Walkthrough-Flooding.ipynb and try it out</p> <p>Assume you are interested in mapping flooding, traditionally you might have either relied on on-the-ground mapping, or possibly for manual analysis of remote-sensing imagery (i.e. satellite or UAV). In order to scale up these efforts and operationalise, we need a way to automate the extraction of flood extent from satellite imagery. This is where we turn to AI models.</p> <p></p> <p>The model you will use in this walkthrough was fine-tuned from the Prithvi foundation model and using the Sen1Floods11 dataset link here.</p> <p>In this walkthrough we will assume that a model doesn't exist yet and we want to train a new model.  We will then show how to drive the model to map impact.</p> <p>We will walk through the following steps as part of this walkthrough:</p> <ol> <li>Upload and onboarding of data</li> <li>Configuring and submitting a tuning task</li> <li>Monitoring model training</li> <li>Testing and validation of the outputs</li> </ol>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#pre-requisites","title":"Pre-requisites\u00b6","text":"<p>You will require access to an instance of the Geospatial Studio.  For more information about the Geospatial Studio see the docs page: Geospatial Studio Docs</p> <p>For more information about the Geospatial Studio SDK and all the functions available through it, see the SDK docs page: Geospatial Studio SDK Docs</p> <p>This walkthrough also requires you to have a direct download URL pointing to a zip file of the dataset you wish to use. We provide a sample dataset url (zip file) below to go through this notebook. If you have the dataset locally, you can find instructions on how to use the SDK to temporarily upload it to the cloud and create a download url link in the steps that follow.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#get-the-training-data","title":"Get the training data\u00b6","text":"<p>To train the AI model, we will need some training data which contains the input data and the labels (aka ground truth flooding extent).  To train our model we will use the following dataset: https://geospatial-studio-example-data.s3.us-east.cloud-object-storage.appdomain.cloud/sen2_flood_dst.zip</p> <p>Download and unzip the above archive and if you wish you can explore the data with QGIS (or any similar tool).</p> <p>NB: If you already have the data in online you can skip this step.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#connecting-to-the-platform","title":"Connecting to the platform\u00b6","text":"<p>First, we set up the connection to the platform backend.  To do this we need the base url for the studio UI and an API key.</p> <p>To get an API Key:</p> <ol> <li>Go to the Geospatial Studio UI page and navigate to the Manage your API keys link.</li> <li>This should pop-up a window where you can generate, access and delete your api keys. NB: every user is limited to a maximum of two activate api keys at any one time.</li> </ol> <p>Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:</p> <pre>echo \"GEOSTUDIO_API_KEY=&lt;paste_api_key_here&gt;\" &gt; .geostudio_config_file\necho \"BASE_STUDIO_UI_URL=&lt;paste_ui_base_url_here&gt;\" &gt;&gt; .geostudio_config_file\n</pre> <p>Copy and paste the file path to this credentials file in call below.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#data-onboarding","title":"Data onboarding\u00b6","text":"<p>In order to onboard your dataset to the Geospatial Studio, you need to have a direct download URL pointing to a zip file of the dataset. You can use this dataset url as an example to go through this notebook.</p> <p>If you have the dataset locally, you can use Box, OneDrive or any other cloud storage you are used to, but in addition, to make this easier for you, there is a function which will upload your data to a temporary location in the cloud (with in Studio object storage) and provide you with a url which can be used to pass to the onboarding process.  NB: the same upload function can be useful for pushing files for inferecnce or to processing pipelines.</p> <p>If needed you can package a set of files for upload, you can use a command like:</p> <pre>zip -j flooding-dataset-upload.zip /Users/beldinemoturi/Downloads/flooding-dataset-upload/*\n</pre>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#onboard-the-dataset-to-the-dataset-factory","title":"Onboard the dataset to the dataset factory\u00b6","text":"<p>Now we use the SDK to provide the information about the dataset, including name, suffixes etc.  A more detailed description of the dataset details is provided in the UI walkthrough.  Here the SDK will do some basic sanity checks, and will (if possible) check that you have matching data and label pairs, and check that you have specified the correct number of bands.  This creates dictionary with the required details, which you can then submit to the platform using the step below.</p> <p>Note:</p> <ul> <li>Change the value of the <code>dataset_url</code>variable below to the url of your zip file or the <code>download_url</code> link you got from using the SDK upload_file function above</li> <li>Change the values of <code>training_data_suffix</code> and <code>label_suffix</code> to the suffixes of your training and label data files respectively if using a different dataset (aside from the one provided)</li> <li>Change the <code>label_categories</code>, <code>custom_bands</code> and descriptions to those that match your dataset</li> </ul>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#pre-scan-the-dataset","title":"Pre-scan the dataset\u00b6","text":"<p>Pre-scan the dataset to check the accessibility of the dataset URL, ensure corresponding data and label files are present, and extract bands and their descriptions from the dataset.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#fine-tuning-submission","title":"Fine-tuning submission\u00b6","text":"<p>Once the data is onboarded, you are ready to setup your tuning task.  In order to run a fine-tuning task, you need to select the following items:</p> <ul> <li>tuning task type/config template - what type of learning task are you attempting?  segmentation, regression etc</li> <li>fine-tuning dataset - what dataset will you use to train the model for your particular application?</li> <li>base foundation model - which geospatial foundation model will you use as the starting point for your tuning task?</li> </ul> <p>Below we walk you through how to use the Geospatial Studio SDK to see what options are available in the platform for each of these, then once you have made your selection, how we configure our task and submit it.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#tuning-task","title":"Tuning task\u00b6","text":"<p>The tuning task tells the model what type of task it is (segmentation, regression etc), and exposes a range of optional hyperparameters which the user can set.  These all have reasonable defaults, but it gives uses the possibility to configure the model training how they wish.  Below, we will check what task templates are available to us, and then update some parameters.</p> <p>Advanced users can create and upload new task templates to the platform, and instructions are found in the relevant notebook and documentation.  The templates are for Terratorch (the backend tuning library), and more details of Terratroch and configuration options can be found here: https://ibm.github.io/terratorch/</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#base-foundation-model","title":"Base foundation model\u00b6","text":"<p>The base model is the foundation model (encoder) which has been pre-trained and has the basic understanding of the data.  More information can currently be found on the different models we have open-sourced on hugging face.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#submitting-the-tune","title":"Submitting the tune\u00b6","text":"<p>Now we pull these choices together into a payload which we then submit to the platform.  This will then deploy the job in the backend and we will see below how we can monitor it.  First, we populate the payload so we can check it, then we simply submit.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#monitoring-training","title":"Monitoring training\u00b6","text":"<p>Once the tune has been submitted you can check its status and monitor tuning progress through the SDK.  You can also access the training metrics and images in MLflow.  The <code>get_tune</code> function will give you the meta-data of the tune, including the status.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#testing-your-model","title":"Testing your model\u00b6","text":"<p>To do a test deployment and inference with the model, we need to register the model with the inference service.  To do this you need to select a model style(describing the visulisation style of the model output), and define the data required to feed the model (in the example here it is using Sentinel Hub).  For the data specification, you need to define the data collection and bands from sentinelhub (using the collection and band names for SH).  In addition, if the data to be fed in is returned from SH with a scale factor that needs to be added here too.  Data collection data for HLS are found here: https://docs.sentinel-hub.com/api/latest/data/hls/</p> <p>Example flood events</p> Location Date Bounding box Link Maiduguri, Nigeria 2024-09-12 [13.146418, 11.799808, 13.215874, 11.871586] https://www.aljazeera.com/features/2024/9/19/a-disaster-homes-lost-relatives-missing-in-floods-in-northeast-nigeria Porto Alegre, Brazil 2024-05-06 [-51.33225, -30.08903, -51.19011, -29.97489] https://www.reuters.com/pictures/stunning-images-show-extent-flooding-southern-brazil-2024-05-07/ Ahero, Kenya 2024-05-05 [34.838652, -0.231379, 34.977847, -0.131439] Gloucester, UK 2024-01-09 [-2.311807, 51.855573, -2.17892, 51.952735]"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#try-out-the-model-for-inference","title":"Try out the model for inference\u00b6","text":"<p>Once your model has finished tuning, if you want to run inference as a test you can do by passing either a location (bbox) or a url to a pre-prepared files.  The steps to test the model are:</p> <ol> <li>Define the inference payload</li> <li>Try out the tune temporarily</li> </ol>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#monitoring-your-inference-task","title":"Monitoring your inference task\u00b6","text":"<p>Once submitted you can check on progress using the following function which will return all the metadata about the inference task, including the status.  You can optionally use the <code>poll_until_finished</code> to watch the status until it completes.  For a test inference it can take 5-10 minutes, depending on the size of the data query, the size of the model etc.</p>"},{"location":"examples/e2e-walkthroughs/GeospatialStudio-Walkthrough-Flooding/#checking-model-outputs","title":"Checking model outputs\u00b6","text":"<p>You can check out the results visually in the Studio UI, or with the quick widget below.  You can alternatively use the SDK to download selected files for further analysis see documentation.</p> <p>Note:</p> <p>For now, you can check out the inference output and results visually in the Studio UI through the history tab of the inference page.</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/","title":"001-Introduction-to-Finetuning","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install seaborn\n</pre> !pip install seaborn In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre># first import the required packages\nimport json\nimport uuid\nimport pandas as pd\nimport wget\nimport rasterio\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML\nimport seaborn as sns\nimport getpass # For use in Colab as well\n\nimport urllib3\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\nfrom geostudio import Client\nfrom geostudio import gswidgets\n</pre> # first import the required packages import json import uuid import pandas as pd import wget import rasterio import matplotlib.pyplot as plt from IPython.display import display, HTML import seaborn as sns import getpass # For use in Colab as well  import urllib3 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  from geostudio import Client from geostudio import gswidgets In\u00a0[\u00a0]: Copied! <pre>#############################################################\n# Initialize Geostudio client using a geostudio config file\n#############################################################\ngfm_client = Client(geostudio_config_file=\".geostudio_config_file\")\n</pre> ############################################################# # Initialize Geostudio client using a geostudio config file ############################################################# gfm_client = Client(geostudio_config_file=\".geostudio_config_file\")  In\u00a0[\u00a0]: Copied! <pre>tasks = gfm_client.list_tune_templates(output=\"df\")\ndisplay(tasks[['name','description', 'id','created_by','updated_at']])\n</pre> tasks = gfm_client.list_tune_templates(output=\"df\") display(tasks[['name','description', 'id','created_by','updated_at']])  In\u00a0[\u00a0]: Copied! <pre># Choose a task from the options above.  Copy and paste the id into the variable, task_id, below.\ntask_id = 'e4791b2c-bb17-4a5e-9f05-1be5411a4fa6'\n</pre> # Choose a task from the options above.  Copy and paste the id into the variable, task_id, below. task_id = 'e4791b2c-bb17-4a5e-9f05-1be5411a4fa6' In\u00a0[\u00a0]: Copied! <pre># Now we can view the full meta-data and details of the selected task\ntask_meta = gfm_client.get_task(task_id=task_id)\ntask_meta\n</pre> # Now we can view the full meta-data and details of the selected task task_meta = gfm_client.get_task(task_id=task_id) task_meta <p>If you are happy with your choice, you can decide which (if any) hyperparameters you want to set (otherwise defaults will be used).</p> <p>Here we can see the available parameters and their associated defaults.  To update a parameter you can just set values in the dictionary (as shown below for <code>max_epochs</code>).</p> In\u00a0[\u00a0]: Copied! <pre>task_params = gfm_client.get_task_param_defaults(task_id)\ntask_params\n</pre> task_params = gfm_client.get_task_param_defaults(task_id) task_params In\u00a0[\u00a0]: Copied! <pre>task_params['runner']['max_epochs'] = 5\ntask_params['optimizer']['type'] = 'AdamW'\ntask_params['data']['batch_size'] = 4\n</pre> task_params['runner']['max_epochs'] = 5 task_params['optimizer']['type'] = 'AdamW' task_params['data']['batch_size'] = 4  In\u00a0[\u00a0]: Copied! <pre>datasets = gfm_client.list_datasets(output='df')\ndisplay(datasets[['dataset_name','description','id','status','created_by','updated_at']])\n</pre> datasets = gfm_client.list_datasets(output='df') display(datasets[['dataset_name','description','id','status','created_by','updated_at']]) In\u00a0[\u00a0]: Copied! <pre># Explore the dataset\n\ngfm_client.get_dataset(\"geodata-ferctkm2brxpkbqz9apa6z\")\n</pre> # Explore the dataset  gfm_client.get_dataset(\"geodata-ferctkm2brxpkbqz9apa6z\") In\u00a0[\u00a0]: Copied! <pre># Copy and paste the id of the dataset into the variable below\ndataset_id = 'geodata-ferctkm2brxpkbqz9apa6z'\n</pre> # Copy and paste the id of the dataset into the variable below dataset_id = 'geodata-ferctkm2brxpkbqz9apa6z' In\u00a0[\u00a0]: Copied! <pre>base = gfm_client.list_base_models(output='df')\ndisplay(base[['name','description','id','updated_at']])\n</pre> base = gfm_client.list_base_models(output='df') display(base[['name','description','id','updated_at']]) In\u00a0[\u00a0]: Copied! <pre># Copy and paste the id of the base model you selected into the variable below\nbase_model_id = '71c82e28-c0ee-44b8-aba9-7facd94e08ec'\n</pre> # Copy and paste the id of the base model you selected into the variable below base_model_id = '71c82e28-c0ee-44b8-aba9-7facd94e08ec' In\u00a0[\u00a0]: Copied! <pre>tune_payload = {\n  \"name\": \"test-fine-tuning\",\n  \"description\": \"testing\",\n  \"dataset_id\": dataset_id,\n  \"base_model_id\": base_model_id,\n  \"tune_template_id\": task_id,\n  # \"model_parameters\": task_params # uncomment this line if you customised task_params in the cells above otherwise, defaults will be used\n}\n\nprint(json.dumps(tune_payload, indent=2))\n</pre> tune_payload = {   \"name\": \"test-fine-tuning\",   \"description\": \"testing\",   \"dataset_id\": dataset_id,   \"base_model_id\": base_model_id,   \"tune_template_id\": task_id,   # \"model_parameters\": task_params # uncomment this line if you customised task_params in the cells above otherwise, defaults will be used }  print(json.dumps(tune_payload, indent=2)) In\u00a0[\u00a0]: Copied! <pre>submitted = gfm_client.submit_tune(\n        data = tune_payload,\n        output = 'json'\n)\n\nprint(submitted)\n</pre> submitted = gfm_client.submit_tune(         data = tune_payload,         output = 'json' )  print(submitted) In\u00a0[\u00a0]: Copied! <pre># If you wish to you can keep polling the tuning task to monitor its progress.\nr = gfm_client.poll_finetuning_until_finished(tune_id=submitted['tune_id'])\n</pre>  # If you wish to you can keep polling the tuning task to monitor its progress. r = gfm_client.poll_finetuning_until_finished(tune_id=submitted['tune_id']) In\u00a0[\u00a0]: Copied! <pre>tune_id = submitted[\"tune_id\"]\n\ntune_info = gfm_client.get_tune(tune_id, output='json')\ntune_info\n</pre> tune_id = submitted[\"tune_id\"]  tune_info = gfm_client.get_tune(tune_id, output='json') tune_info In\u00a0[\u00a0]: Copied! <pre>mdf = gfm_client.get_tune_metrics_df(tune_id)\nmdf.head()\n</pre> mdf = gfm_client.get_tune_metrics_df(tune_id) mdf.head() In\u00a0[\u00a0]: Copied! <pre>gswidgets.plot_tune_metrics(client=gfm_client, tune_id=tune_id)\n</pre> gswidgets.plot_tune_metrics(client=gfm_client, tune_id=tune_id) In\u00a0[\u00a0]: Copied! <pre>upload_url = gfm_client.create_upload_presigned_url(\n    bucket_name=\"bucket_name\", # bucket name\n    object_key=\"data/train/austin1_sdk_upload.tiff\", # file path to upload in the bucket\n    endpoint_url=\"https://s3.us-east.cloud-object-storage.appdomain.cloud\", # s3 endpoint url\n    service_name= \"s3\", # service to use\n    region_name=\"us-east\", # cloud region \n    expiration=3600 # expiration\n    # Add any other args to pass to the s3 client\n)\nupload_url\n</pre> upload_url = gfm_client.create_upload_presigned_url(     bucket_name=\"bucket_name\", # bucket name     object_key=\"data/train/austin1_sdk_upload.tiff\", # file path to upload in the bucket     endpoint_url=\"https://s3.us-east.cloud-object-storage.appdomain.cloud\", # s3 endpoint url     service_name= \"s3\", # service to use     region_name=\"us-east\", # cloud region      expiration=3600 # expiration     # Add any other args to pass to the s3 client ) upload_url In\u00a0[\u00a0]: Copied! <pre># Push your file to the bucket using the url generated.\n!curl -X PUT -T **your_file.zip or your_file.tiff or your_file.tif** \"**upload_url**\"\n</pre> # Push your file to the bucket using the url generated. !curl -X PUT -T **your_file.zip or your_file.tiff or your_file.tif** \"**upload_url**\" <p>Once the image is uploaded to your s3 bucket, create a download link to use in the inference request.</p> In\u00a0[\u00a0]: Copied! <pre>download_url = gfm_client.create_download_presigned_url(\n    bucket_name=\"geospatial-studio-example-data\", # bucket name\n    object_key=\"data/train/austin1_sdk_upload.tiff\", # file path to upload in the bucket\n    endpoint_url=\"https://s3.us-east.cloud-object-storage.appdomain.cloud\", # s3 endpoint url\n    service_name= \"s3\", # service to use\n    region_name=\"us-east\", # cloud region \n    expiration=7200 # expiration\n    # Add any other args to pass to the s3 client\n\n)\ndownload_url\n</pre> download_url = gfm_client.create_download_presigned_url(     bucket_name=\"geospatial-studio-example-data\", # bucket name     object_key=\"data/train/austin1_sdk_upload.tiff\", # file path to upload in the bucket     endpoint_url=\"https://s3.us-east.cloud-object-storage.appdomain.cloud\", # s3 endpoint url     service_name= \"s3\", # service to use     region_name=\"us-east\", # cloud region      expiration=7200 # expiration     # Add any other args to pass to the s3 client  ) download_url <p>If you would like to upload to a geostudio temporary bucket, use this function <code>get_fileshare_links</code> function.</p> In\u00a0[\u00a0]: Copied! <pre># Unique object name to be used in temporary COS for each layer you want to upload\nobject_name = \"austin1_sdk_upload.tiff\"\ngfm_client.get_fileshare_links(object_name)\n</pre>  # Unique object name to be used in temporary COS for each layer you want to upload object_name = \"austin1_sdk_upload.tiff\" gfm_client.get_fileshare_links(object_name)  In\u00a0[\u00a0]: Copied! <pre># Push your file to the bucket using the url generated.\n!curl -X PUT -T **your_file.zip or your_file.tiff or your_file.tif** \"**upload_url**\"\n</pre> # Push your file to the bucket using the url generated. !curl -X PUT -T **your_file.zip or your_file.tiff or your_file.tif** \"**upload_url**\" In\u00a0[\u00a0]: Copied! <pre># define the inference payload\n\nbbox = [-121.837006,39.826468,-121.641312,40.038655]\ndownload_url_tiff = download_url\n\n# When using a bbox\nrequest_payload_with_bbox = {\n\t\"description\": \"Park Fire 2024 SDK\",\n\t\"location\": \"Red Bluff, California, United States\",\n\t\"spatial_domain\": {\n\t\t\t\"bbox\": [bbox], # When using bboxes\n\t\t\t\"polygons\": [],\n\t\t\t\"tiles\": [],\n\t\t\t\"urls\": []\n\t},\n\t\"temporal_domain\": [\n\t\t\t\"2024-08-12_2024-08-13\"\n\t]\n}\n\n# When using a presigned link\nrequest_payload_with_url = {\n\t\"description\": \"Park Fire 2024 SDK\",\n\t\"location\": \"Red Bluff, California, United States\",\n\t\"spatial_domain\": {\n\t\t\t\"bbox\": [],\n\t\t\t\"polygons\": [],\n\t\t\t\"tiles\": [],\n\t\t\t\"urls\": [download_url_tiff] # When using url\n\t},\n\t\"temporal_domain\": [\n\t\t\t\"2024-08-12_2024-08-13\"\n\t]\n}\n</pre> # define the inference payload  bbox = [-121.837006,39.826468,-121.641312,40.038655] download_url_tiff = download_url  # When using a bbox request_payload_with_bbox = { \t\"description\": \"Park Fire 2024 SDK\", \t\"location\": \"Red Bluff, California, United States\", \t\"spatial_domain\": { \t\t\t\"bbox\": [bbox], # When using bboxes \t\t\t\"polygons\": [], \t\t\t\"tiles\": [], \t\t\t\"urls\": [] \t}, \t\"temporal_domain\": [ \t\t\t\"2024-08-12_2024-08-13\" \t] }  # When using a presigned link request_payload_with_url = { \t\"description\": \"Park Fire 2024 SDK\", \t\"location\": \"Red Bluff, California, United States\", \t\"spatial_domain\": { \t\t\t\"bbox\": [], \t\t\t\"polygons\": [], \t\t\t\"tiles\": [], \t\t\t\"urls\": [download_url_tiff] # When using url \t}, \t\"temporal_domain\": [ \t\t\t\"2024-08-12_2024-08-13\" \t] } In\u00a0[\u00a0]: Copied! <pre># Now submit the test inference request\n# Change the request to the correct one when using urls\ninference_response = gfm_client.try_out_tune(tune_id=tune_id, data=request_payload_with_bbox)\ninference_response\n</pre> # Now submit the test inference request # Change the request to the correct one when using urls inference_response = gfm_client.try_out_tune(tune_id=tune_id, data=request_payload_with_bbox) inference_response In\u00a0[\u00a0]: Copied! <pre>gfm_client.download_tune(tune_id)\n</pre> gfm_client.download_tune(tune_id)"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#001-introduction-to-finetuning","title":"001-Introduction-to-Finetuning\u00b6","text":"<p>\ud83d\udce5 Download 001-Introduction-to-Finetuning.ipynb and try it out</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#introduction","title":"Introduction\u00b6","text":"<p>This notebook is intended to be an introduction to using the python SDK to fine-tune a new model from a geospatial foundation model backbone using the Geospatial Studio.</p> <p>For more information about the Geospatial Studio see the docs page: Geospatial Studio Docs</p> <p>For more information about the Geospatial Studio SDK and all the functions available through it, see the SDK docs page: Geospatial Studio SDK Docs</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#prerequisites","title":"Prerequisites\u00b6","text":"<ol> <li>Access to a deploy instance of the Geospatial Studio.</li> <li>Ability to run and edit a copy of this notebook.</li> </ol>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#install-sdk","title":"Install SDK:\u00b6","text":"<ol> <li><p>Prepare a python 3.9+ environment, however you normally do that (e.g. conda, pyenv, poetry, etc.) and activate this new environment.</p> </li> <li><p>Install Jupyter into that environment: <code>python -m pip install --upgrade pip</code> then <code>pip install notebook</code></p> </li> <li><p>Install the SDK with: <code>python -m pip install geostudio</code></p> </li> </ol>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#install-notebook-dependecies","title":"Install notebook dependecies\u00b6","text":""},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#connecting-to-the-platform","title":"Connecting to the platform\u00b6","text":"<p>First, we set up the connection to the platform backend.  To do this we need the base url for the studio UI and an API key.</p> <p>To get an API Key:</p> <ol> <li>Go to the Geospatial Studio UI page and navigate to the Manage your API keys link.</li> <li>This should pop-up a window where you can generate, access and delete your api keys. NB: every user is limited to a maximum of two activate api keys at any one time.</li> </ol> <p>Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:</p> <pre>echo \"GEOSTUDIO_API_KEY=&lt;paste_api_key_here&gt;\" &gt; .geostudio_config_file\necho \"BASE_STUDIO_UI_URL=&lt;paste_ui_base_url_here&gt;\" &gt;&gt; .geostudio_config_file\n</pre> <p>Copy and paste the file path to this credentials file in call below.</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#setting-up-a-fine-tuning-task","title":"Setting up a fine-tuning task\u00b6","text":"<p>Now we are all set to prepare our fine-tuning task.  This assumes that the tuning dataset to be used is already present in the platform (if it is not, please see the dataset factory examples and return here once the dataset is onboarded).</p> <p>In order to run a fine-tuning task, you need to select the following items:</p> <ul> <li>tuning task type - what type of learning task are you attempting?  segmentation, regression etc</li> <li>fine-tuning dataset - what dataset will you use to train the model for your particular application?</li> <li>base foundation model - which geospatial foundation model will you use as the starting point for your tuning task?</li> </ul> <p>Below we walk you through how to use the Geospatial Studio SDK to see what options are available in the platform for each of these, then once you have made your selection, how we configure our task and submit it.</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#tuning-task-selection","title":"Tuning task selection\u00b6","text":"<p>The tuning task tells the model what type of task it is (segmentation, regression etc), and exposes a range of optional hyperparameters which the user can set.  These all have reasonable defaults, but it gives uses the possibility to configure the model training how they wish.  Below, we will check what task templates are available to us, and then update some parameters.</p> <p>Advanced users can create and upload new task templates to the platform, and instructions are found in the relevant notebook and documentation.  The templates are for Terratorch (the backend tuning library), and more details of Terratroch and configuration options can be found here: https://terrastackai.github.io/terratorch/</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#dataset-selection","title":"Dataset selection\u00b6","text":"<p>Now we have chosen the type of tuning task we wish to carry out, we need to decide on the tuning dataset.  There are two options available:</p> <ul> <li>use a dataset already registered in the Studio</li> <li>create a new dataset by uploading or curating a dataset</li> </ul> <p>In this notebook, we use a already existing dataset.  For a walkthrough of how to create new datasets see the relevant example and documentation.</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#foundation-model-selection","title":"Foundation model selection\u00b6","text":"<p>The final selection we need to make before kicking off our tuning task is to select the backbone/base model we wish to start from.  Again, we can first view the available options in the studio, then make our selection.</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#submitting-the-tuning-task","title":"Submitting the tuning task\u00b6","text":"<p>Now we put that information into the payload below and send the request to the cluster.  In this case we will use the asynchronous submission (avoids issues with timeouts for large areas and time windows).</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#monitor-tuning-status-and-progress","title":"Monitor tuning status and progress\u00b6","text":"<p>After submitting the request, we can poll the inference service to check the progress and get the output details once its complete (this could take a few minutes depending on the request size and the current service load).</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#check-the-training-metrics-from-the-tune","title":"Check the training metrics from the tune\u00b6","text":"<p>The metrics from the model training are logged in a backend MLflow service and can be accessed through the APIs, SDK and UI.</p> <p>You can get access the training metrics either in full as a json using:</p> <pre>gfm_client.get_tune_metrics(tune_id)\n</pre> <p>Or directly to a pandas dataframe for ready analysis using the function below <code>get_tune_metrics_df</code>.  In addition, the SDK provides functionality to quickly plot some top level metrics for training and validation.</p> <p>In addition to that, you can simply plot the training and validation loss and multi-class accuracy using the <code>plot_tune_metrics</code> function.</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#try-out-the-model-for-inference","title":"Try out the model for inference\u00b6","text":"<p>Once your model has finished tuning, if you want to run inference as a test you can do by passing either a location (bbox) or a url to a pre-prepared files.  The steps to test the model are:</p> <ol> <li>Define the inference payload</li> <li>Try out the tune temporarily</li> </ol>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#using-an-s3-pre-signed-link","title":"Using an S3 pre-signed link\u00b6","text":"<p>If you have your image locally and would like to pre-sign the image using S3.</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#personal-buckets","title":"Personal buckets\u00b6","text":"<p>Use the <code>create_upload_presigned_url</code> to generate an upload link that you can use to upload the file to the dataset.</p> <p>This function assumes you have your own storage bucket to upload to.</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#geostudio-temporary-buckets","title":"Geostudio temporary buckets\u00b6","text":""},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#submit-inference","title":"Submit Inference\u00b6","text":"<p>Now you can create the inference payload using the download link.</p>"},{"location":"examples/fine-tuning/001-Introduction-to-Finetuning/#downloading-the-tuned-model-artefacts","title":"Downloading the tuned model artefacts\u00b6","text":"<p>If you want to download the model artefacts (e.g. checkpoint and config) in order to run the model locally or elsewhere, you can use the following function to do it.</p>"},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/","title":"002-Upload-Completed-Tune-Artifacts","text":"In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>from IPython.display import JSON\n\nfrom geostudio import Client\nfrom geostudio import gswidgets\n</pre>  from IPython.display import JSON  from geostudio import Client from geostudio import gswidgets In\u00a0[\u00a0]: Copied! <pre>#############################################################\n# Initialize Geostudio client using a geostudio config file\n#############################################################\ngeostudio_client = Client(geostudio_config_file=\".geostudio_config_file\")\n</pre>  ############################################################# # Initialize Geostudio client using a geostudio config file ############################################################# geostudio_client = Client(geostudio_config_file=\".geostudio_config_file\")  In\u00a0[\u00a0]: Copied! <pre>object_name = \"test-checkpoint.ckpt\" #Must be a valid string, not a path\ncheckpoint_file= \"../sample_files/best-state_dict.ckpt\"\n\n\ncheckpoint_urls = geostudio_client.get_fileshare_links(object_name)\n\ncheckpoint_upload_url = checkpoint_urls[\"upload_url\"]\ncheckpoint_download_url = checkpoint_urls[\"download_url\"]\n\n\n!curl --progress-bar -O -X PUT -T \"$checkpoint_file\" \"$checkpoint_upload_url\"\n</pre> object_name = \"test-checkpoint.ckpt\" #Must be a valid string, not a path checkpoint_file= \"../sample_files/best-state_dict.ckpt\"   checkpoint_urls = geostudio_client.get_fileshare_links(object_name)  checkpoint_upload_url = checkpoint_urls[\"upload_url\"] checkpoint_download_url = checkpoint_urls[\"download_url\"]   !curl --progress-bar -O -X PUT -T \"$checkpoint_file\" \"$checkpoint_upload_url\" In\u00a0[\u00a0]: Copied! <pre>object_name = \"test-config.yaml\" #Must be a valid string, not a path\n\nconfig_file = \"../sample_files/config_deploy.yaml\"\n\nconfig_urls = geostudio_client.get_fileshare_links(object_name)\n\nconfig_upload_url = config_urls[\"upload_url\"]\nconfig_download_url = config_urls[\"download_url\"]\n\n\n\n!curl --progress-bar -O -X PUT -T \"$config_file\" \"$config_upload_url\"\n</pre> object_name = \"test-config.yaml\" #Must be a valid string, not a path  config_file = \"../sample_files/config_deploy.yaml\"  config_urls = geostudio_client.get_fileshare_links(object_name)  config_upload_url = config_urls[\"upload_url\"] config_download_url = config_urls[\"download_url\"]    !curl --progress-bar -O -X PUT -T \"$config_file\" \"$config_upload_url\" In\u00a0[\u00a0]: Copied! <pre>tune = geostudio_client.upload_completed_tunes(\n    data={\n        \"name\": \"flood-test-001\",\n        \"description\": \"Fine-tuned model for flooding detection\",\n        \"tune_checkpoint_url\": checkpoint_download_url,\n        \"tune_config_url\": config_download_url,\n    }\n)\nJSON(tune)\n</pre> tune = geostudio_client.upload_completed_tunes(     data={         \"name\": \"flood-test-001\",         \"description\": \"Fine-tuned model for flooding detection\",         \"tune_checkpoint_url\": checkpoint_download_url,         \"tune_config_url\": config_download_url,     } ) JSON(tune) In\u00a0[\u00a0]: Copied! <pre>created_tune = geostudio_client.get_tune(tune_id=tune[\"tune_id\"])\nJSON(created_tune)\n</pre> created_tune = geostudio_client.get_tune(tune_id=tune[\"tune_id\"]) JSON(created_tune) <p>When running the inference and you require to download the inference data from data sources like sentinelhub, update the payload with <code>model_input_data_spec</code> and <code>geoserver_push</code></p> <p><code>model_input_data_spec</code> section specificies which collection, connector and bands to be downloaded.</p> <pre>    \"model_input_data_spec\": [\n        {\n        \"bands\": [\n            {\n                \"band_name\": \"B01\",\n                \"resolution\": \"60m\",\n                \"description\": \"Coastal aerosol, 442.7 nm (S2A), 442.3 nm (S2B)\"\n            },\n            ....  # List all your bands here\n        ],\n            \"connector\": \"sentinelhub\",\n            \"collection\": \"s2_l2a\",\n            \"file_suffix\": \"S2L2A.tif\",\n            \"modality_tag\": \"S2L2A\",\n        }\n    \n    ]\n</pre> <p><code>geoserver_push</code> section specifies which layers to push after running inference. By default, we push rgb and model_output</p> <pre>    \"geoserver_push\": [\n        {\n            \"z_index\": 0,\n            \"workspace\": \"geofm\",\n            \"layer_name\": \"input_rgb\",\n            \"file_suffix\": \"\",\n            \"display_name\": \"Input image (RGB)\",\n            \"filepath_key\": \"model_input_original_image_rgb\",\n            \"geoserver_style\": {\n                \"rgb\": [\n                    {\n                        \"label\": \"RedChannel\",\n                        \"channel\": 1,\n                        \"maxValue\": 2000,\n                        \"minValue\": 0\n                    },\n                    {\n                        \"label\": \"GreenChannel\",\n                        \"channel\": 2,\n                        \"maxValue\": 2000,\n                        \"minValue\": 0\n                    },\n                    {\n                        \"label\": \"BlueChannel\",\n                        \"channel\": 3,\n                        \"maxValue\": 2000,\n                        \"minValue\": 0\n                    }\n                ]\n            },\n            \"visible_by_default\": \"True\"\n        },\n        {\n            \"z_index\": 1,\n            \"workspace\": \"geofm\",\n            \"layer_name\": \"pred\",\n            \"file_suffix\": \"\",\n            \"display_name\": \"Model prediction\",\n            \"filepath_key\": \"model_output_image\",\n            \"geoserver_style\": {\n                \"segmentation\": [\n                    {\n                        \"color\": \"#7d7247\",\n                        \"label\": \"no-ships\",\n                        \"opacity\": 0,\n                        \"quantity\": \"0\"\n                    },\n                    {\n                        \"color\": \"#c1121f\",\n                        \"label\": \"ships\",\n                        \"opacity\": 1,\n                        \"quantity\": \"1\"\n                    }\n                ]\n            },\n            \"visible_by_default\": \"True\"\n        }\n    ],\n</pre> In\u00a0[\u00a0]: Copied! <pre>inference = geostudio_client.try_out_tune(\n    tune_id=created_tune[\"id\"],\n    data={\n        \"spatial_domain\": {\n            \"bbox\": [\n                [\n                    92.5290608449473,\n                    26.185925522799945,\n                    92.80352715571134,\n                    26.419756619674683\n                ]\n            ]\n        },\n        \"temporal_domain\": [\n            \"2024-07-25_2024-07-27\"\n        ],\n        \"model_display_name\": \"geofm-sandbox-models\",\n        \"description\": \"test\",\n        \"location\": \"Jarani, Nagaon, Nagaon, Assam, India\",\n        # \"model_input_data_spec\": {[]},\n        # \"geoserver_push\":[{}],\n        \"post_processing\": {\n            \"cloud-masking\": \"False\",\n            \"cloud_masking\": \"False\",\n            \"ocean-masking\": \"False\",\n            \"ocean_masking\": \"False\",\n            \"snow_ice_masking\": \"False\",\n            \"permanent_water_masking\": \"False\"\n        },\n    }\n)\nJSON(inference)\n</pre> inference = geostudio_client.try_out_tune(     tune_id=created_tune[\"id\"],     data={         \"spatial_domain\": {             \"bbox\": [                 [                     92.5290608449473,                     26.185925522799945,                     92.80352715571134,                     26.419756619674683                 ]             ]         },         \"temporal_domain\": [             \"2024-07-25_2024-07-27\"         ],         \"model_display_name\": \"geofm-sandbox-models\",         \"description\": \"test\",         \"location\": \"Jarani, Nagaon, Nagaon, Assam, India\",         # \"model_input_data_spec\": {[]},         # \"geoserver_push\":[{}],         \"post_processing\": {             \"cloud-masking\": \"False\",             \"cloud_masking\": \"False\",             \"ocean-masking\": \"False\",             \"ocean_masking\": \"False\",             \"snow_ice_masking\": \"False\",             \"permanent_water_masking\": \"False\"         },     } ) JSON(inference) In\u00a0[\u00a0]: Copied! <pre># Check status of the submitted inference\n\ninference = geostudio_client.get_inference(inference[\"id\"])\nJSON(inference)\ndisplay(f\"Inference Status: {inference['status']}\")\n</pre> # Check status of the submitted inference  inference = geostudio_client.get_inference(inference[\"id\"]) JSON(inference) display(f\"Inference Status: {inference['status']}\") In\u00a0[\u00a0]: Copied! <pre># Get the inference tasks list\n\ninf_tasks_res = geostudio_client.get_inference_tasks(inference[\"id\"])\ninf_tasks_res\n</pre> # Get the inference tasks list  inf_tasks_res = geostudio_client.get_inference_tasks(inference[\"id\"]) inf_tasks_res In\u00a0[\u00a0]: Copied! <pre># Select a task to view\n\nselected_task = 0 \nselected_task_id = f\"{inference[\"id\"]}-task_{selected_task}\"\n</pre> # Select a task to view  selected_task = 0  selected_task_id = f\"{inference[\"id\"]}-task_{selected_task}\"  In\u00a0[\u00a0]: Copied! <pre># Visualize output files with the SDK\n\ngswidgets.inferenceTaskViewer(client=geostudio_client , task_id=selected_task_id)\n</pre> # Visualize output files with the SDK  gswidgets.inferenceTaskViewer(client=geostudio_client , task_id=selected_task_id)"},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#002-upload-completed-tune-artifacts","title":"002-Upload-Completed-Tune-Artifacts\u00b6","text":"<p>\ud83d\udce5 Download 002-Upload-Complete-Tune-Artifacts.ipynb and try it out</p>"},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#introduction","title":"Introduction\u00b6","text":"<p>This notebook guides you through the complete workflow of deploying a fine-tuned TerraTorch model to the GeoStudio platform. The process involves three steps:</p> <ul> <li>Preparation: Setting up checkpoint and configuration files from your fine-tuned TerraTorch model</li> <li>Upload Process: Utilizing the GeoStudio SDK to transfer your model artifacts to the GeoStudio cloud infrastructure.</li> <li>Inference: Running geospatial inference tasks using your uploaded model/tune.</li> </ul>"},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before proceeding with this notebook, ensure you have:</p> <ul> <li>Active GeoStudio Service Access: Valid credentials and permissions for the GeoStudio inference service</li> <li>SDK Installation: The GeoStudio SDK installed in your environment</li> <li>Authentication Setup: API keys configured (either via environment variables or key files)</li> <li>Model Artifacts: A completed fine-tuned TerraTorch model with both checkpoint (.ckpt) and configuration (.yaml) files</li> <li>Cloud Storage Access: Valid credentials for your object storage bucket (AWS S3, IBM Cloud Object Storage, etc.)</li> </ul> <p>Note: This workflow assumes you have already completed the model training process and possess both the trained checkpoint file and its corresponding configuration file. If you need guidance on fine-tuning TerraTorch models, refer to the TerraTorch documentation first.</p>"},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#imports-setup","title":"Imports &amp; Setup\u00b6","text":""},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#connecting-to-geospatial-studio","title":"Connecting to Geospatial Studio\u00b6","text":""},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#connecting-to-the-platform","title":"Connecting to the platform\u00b6","text":"<p>First, we set up the connection to the platform backend.  To do this we need the base url for the studio UI and an API key.</p> <p>To get an API Key:</p> <ol> <li>Go to the Geospatial Studio UI page and navigate to the Manage your API keys link.</li> <li>This should pop-up a window where you can generate, access and delete your api keys. NB: every user is limited to a maximum of two activate api keys at any one time.</li> </ol> <p>Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:</p> <pre>echo \"GEOSTUDIO_API_KEY=&lt;paste_api_key_here&gt;\" &gt; .geostudio_config_file\necho \"BASE_STUDIO_UI_URL=&lt;paste_ui_base_url_here&gt;\" &gt;&gt; .geostudio_config_file\n</pre> <p>Copy and paste the file path to this credentials file in call below.</p>"},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#prepare-your-tune-artifacts-with-presigned-urls","title":"Prepare your tune artifacts with Presigned URLs\u00b6","text":"<p>Presigned URLs are temporary, signed links that let you securely access objects in your storage bucket without exposing your credentials.</p> <p>There are two main kinds of presigned URLs:</p> <ul> <li><p>PUT presigned URL \u2192 lets you upload a file to your bucket. Think of it as a one-time signed permission slip that says: \u201cFor the next 1 hour, anyone with this link may PUT (write) an object here.\u201d</p> </li> <li><p>GET presigned URL \u2192 lets you download or allow another service to fetch the file. This is what you\u2019ll hand off to the geospatial studio service, since it needs to read the checkpoint and config.</p> </li> </ul> <p>Workflow: Whe you use the sdk <code>upload_file</code> function, it:</p> <ol> <li>Generates both a PUT and a GET URL for each file.</li> <li>Uses the PUT URL locally to upload the files.</li> <li>Passes the GET URL to the service so it can later retrieve them.</li> </ol>"},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#upload-tune-artifacts-from-your-local-environment","title":"Upload tune artifacts from your local environment\u00b6","text":""},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#upload-your-checkpoint-file","title":"Upload your checkpoint file\u00b6","text":""},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#upload-your-config-file","title":"Upload your config file\u00b6","text":""},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#register-your-tuning-artifacts","title":"Register your tuning artifacts\u00b6","text":"<p>Once your model artifacts are successfully uploaded to cloud storage, register them with the GeoStudio platform. This registration process:</p> <ul> <li>Creates Platform Records: Establishes your tune as a recognized model within the system</li> <li>Validates Artifacts: Confirms that uploaded files are accessible and properly formatted</li> <li>Establishes Metadata: Associates descriptive information with your model for easy identification</li> </ul>"},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#run-inference-with-uploaded-tune","title":"Run inference with uploaded tune\u00b6","text":"<p>Once your tune is successfully registered and available, you can execute inference tasks by trying out the uploaded tune with payload that includes spatial and temporal domains for your inference.</p>"},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#fetch-and-view-inference-results","title":"Fetch and View inference results\u00b6","text":""},{"location":"examples/fine-tuning/002-Upload-Complete-Tune-Artifacts/#accessing-inference-outputs","title":"Accessing inference outputs\u00b6","text":"<p>Once an inference run status is completed, the inputs and outputs of each task within an inference are packaged up into a zip file which is uploaded to a url you can use to download the files.</p> <p>To access and view the inference task files:</p> <ol> <li>Get the inference tasks list</li> <li>Identify the specific inference task you want to view</li> <li>Use the sdk to visualize the inference task results</li> </ol>"},{"location":"examples/fine-tuning/003-Terratorch-Iterate/","title":"FineTuning: HPO With Terratorch Iterate","text":"In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>from geostudio import Client\n</pre> from geostudio import Client <p>First, we set up the connection to the platform backend.  To do this we need the base url for the studio UI and an API key.</p> <p>To get an API Key:</p> <ol> <li>Go to the Geospatial Studio UI page and navigate to the Manage your API keys link.</li> <li>This should pop-up a window where you can generate, access and delete your api keys. NB: every user is limited to a maximum of two activate api keys at any one time.</li> </ol> <p>Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:</p> <pre>echo \"GEOSTUDIO_API_KEY=&lt;paste_api_key_here&gt;\" &gt; .geostudio_config_file\necho \"BASE_STUDIO_UI_URL=&lt;paste_ui_base_url_here&gt;\" &gt;&gt; .geostudio_config_file\n</pre> <p>Copy and paste the file path to this credentials file in call below.</p> In\u00a0[\u00a0]: Copied! <pre>#############################################################\n# 3. Initialize clients using the key?\n#############################################################\ngeostudio_client = Client(geostudio_config_file=\".geostudio_config_file\")\n</pre> ############################################################# # 3. Initialize clients using the key? ############################################################# geostudio_client = Client(geostudio_config_file=\".geostudio_config_file\") In\u00a0[\u00a0]: Copied! <pre>geostudio_client.api_url\n</pre> geostudio_client.api_url In\u00a0[\u00a0]: Copied! <pre>datasets = geostudio_client.list_datasets(output=\"df\")\ndisplay(datasets[['id','dataset_name', 'purpose', 'status','size','description', 'created_by']])\n</pre> datasets = geostudio_client.list_datasets(output=\"df\") display(datasets[['id','dataset_name', 'purpose', 'status','size','description', 'created_by']]) <p>Prepare the fine-tuning payload and submit your tuning job.</p> In\u00a0[\u00a0]: Copied! <pre>tunes = geostudio_client.list_tunes(output=\"df\")\n# tunes\ndisplay(tunes[['id','active', 'created_by', 'name', 'description', 'dataset_id', 'status', 'metrics']])\n</pre> tunes = geostudio_client.list_tunes(output=\"df\") # tunes display(tunes[['id','active', 'created_by', 'name', 'description', 'dataset_id', 'status', 'metrics']]) In\u00a0[\u00a0]: Copied! <pre>tune = geostudio_client.submit_hpo_tune(\n    data={\n        \"tune_metadata\": {\n            \"name\": \"fire-scars-hpo-tune-016\",\n            \"description\": \"Fine-tuned TerraTorch model for fire scar detection\",\n            \"dataset_id\": \"geodata-gdctf3vb3znbbtgptqvuku\",\n        },\n        \"config_file\": \"../sample_files/burnscars-iterate-hpo.yaml\"\n    }\n)\ndisplay(tune)\n</pre> tune = geostudio_client.submit_hpo_tune(     data={         \"tune_metadata\": {             \"name\": \"fire-scars-hpo-tune-016\",             \"description\": \"Fine-tuned TerraTorch model for fire scar detection\",             \"dataset_id\": \"geodata-gdctf3vb3znbbtgptqvuku\",         },         \"config_file\": \"../sample_files/burnscars-iterate-hpo.yaml\"     } ) display(tune) In\u00a0[\u00a0]: Copied! <pre>tune_resp = geostudio_client.get_tune(tune[\"tune_id\"])\ntune_resp\n</pre> tune_resp = geostudio_client.get_tune(tune[\"tune_id\"]) tune_resp In\u00a0[\u00a0]: Copied! <pre>import mlflow\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nmlflow_tracking_uri = \"&lt;studio-mlflow-tracking-uri&gt;\"\nmlflow.set_tracking_uri(mlflow_tracking_uri)\nexperiment_name = \"geotune-qokd9fqyuhxbgyyiuurpxu\"\nclient = mlflow.tracking.MlflowClient()\n</pre> import mlflow import matplotlib.pyplot as plt import pandas as pd  mlflow_tracking_uri = \"\" mlflow.set_tracking_uri(mlflow_tracking_uri) experiment_name = \"geotune-qokd9fqyuhxbgyyiuurpxu\" client = mlflow.tracking.MlflowClient() In\u00a0[\u00a0]: Copied! <pre>try:\n    experiment = client.get_experiment_by_name(experiment_name)\n    if not experiment:\n        raise ValueError(f\"Experiment '{experiment_name}' not found.\")\n    experiment_id = experiment.experiment_id\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    exit()\n</pre> try:     experiment = client.get_experiment_by_name(experiment_name)     if not experiment:         raise ValueError(f\"Experiment '{experiment_name}' not found.\")     experiment_id = experiment.experiment_id except Exception as e:     print(f\"Error: {e}\")     exit() In\u00a0[\u00a0]: Copied! <pre>runs = client.search_runs(experiment_ids=[experiment_id])\n\nif not runs:\n    print(\"No runs found for this experiment.\")\n    exit()\n\n# Create a DataFrame from the runs for easier data manipulation\ndef create_runs_dataframe(runs):\n    run_data = []\n    for run in runs:\n        # Extract desired metrics and parameters\n        metrics = run.data.metrics\n        params = run.data.params\n        \n        # You may want to flatten the run data into a single dictionary\n        row = {**metrics, **params, 'run_id': run.info.run_id}\n        run_data.append(row)\n    \n    return pd.DataFrame(run_data)\n\n# Create the dataframe and fill any missing values with NaN\nruns_df = create_runs_dataframe(runs)\nruns_df = runs_df.fillna(value=pd.NA)\n\n# Print a summary of the runs to check the data\ndisplay(\"Summary of MLflow Runs:\")\ndisplay(runs_df.head())\n</pre> runs = client.search_runs(experiment_ids=[experiment_id])  if not runs:     print(\"No runs found for this experiment.\")     exit()  # Create a DataFrame from the runs for easier data manipulation def create_runs_dataframe(runs):     run_data = []     for run in runs:         # Extract desired metrics and parameters         metrics = run.data.metrics         params = run.data.params                  # You may want to flatten the run data into a single dictionary         row = {**metrics, **params, 'run_id': run.info.run_id}         run_data.append(row)          return pd.DataFrame(run_data)  # Create the dataframe and fill any missing values with NaN runs_df = create_runs_dataframe(runs) runs_df = runs_df.fillna(value=pd.NA)  # Print a summary of the runs to check the data display(\"Summary of MLflow Runs:\") display(runs_df.head()) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nmetrics_to_plot = [\"train/loss\", \"epoch\"]\nstep_interval = 30  # only keep points every 100 steps\n\n# run = client.get_run(runs[0].info.run_id)\n# metrics_keys = run.data.metrics.keys()\n# metrics_to_plot = [metric for metric in metrics_keys if not metric.startswith(\"System\")]\n\nfor metric_name in metrics_to_plot:\n    plt.figure(figsize=(6, 4))\n    plt.title(f\"{metric_name.replace('_', ' ').title()} Over Steps\", fontsize=10)\n    plt.xlabel(\"Step\", fontsize=8)\n    plt.ylabel(metric_name.replace('_', ' ').title(), fontsize=8)\n    plt.grid(True)\n\n    for run in runs:\n        metric_history = client.get_metric_history(run.info.run_id, metric_name)\n        if metric_history:\n            steps = [m.step for m in metric_history]\n            values = [m.value for m in metric_history]\n\n            # Downsample: group by step_interval and take average\n            smoothed_steps = []\n            smoothed_values = []\n\n            for i in range(0, len(steps), step_interval):\n                chunk_steps = steps[i:i+step_interval]\n                chunk_values = values[i:i+step_interval]\n\n                smoothed_steps.append(np.mean(chunk_steps))\n                smoothed_values.append(np.mean(chunk_values))\n\n            run_params = run.data.params\n            legend_label = f\"Run {run.info.run_id[:8]}\"\n            if \"lr\" in run_params:\n                legend_label += f\" (lr={float(run_params['lr']):.18f})\"\n            if \"batch_size\" in run_params:\n                legend_label += f\" (bs={run_params['batch_size']})\"\n            # print(metric_name, run_params)\n\n            plt.plot(smoothed_steps, smoothed_values, label=legend_label)\n        else:\n            print(f\"No history for metric '{metric_name}' in run {run.info.run_id}\")\n\n    # plt.legend(loc=\"lower center\", bbox_to_anchor=(0.5, -0.5), ncol=2)\n    plt.legend(\n        loc=\"upper center\",\n        bbox_to_anchor=(0.5, -0.20),\n        ncol=1,  # number of columns (adjust based on #runs)\n        frameon=False,\n        fontsize=8,\n    )\n    plt.tight_layout()\n    plt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  metrics_to_plot = [\"train/loss\", \"epoch\"] step_interval = 30  # only keep points every 100 steps  # run = client.get_run(runs[0].info.run_id) # metrics_keys = run.data.metrics.keys() # metrics_to_plot = [metric for metric in metrics_keys if not metric.startswith(\"System\")]  for metric_name in metrics_to_plot:     plt.figure(figsize=(6, 4))     plt.title(f\"{metric_name.replace('_', ' ').title()} Over Steps\", fontsize=10)     plt.xlabel(\"Step\", fontsize=8)     plt.ylabel(metric_name.replace('_', ' ').title(), fontsize=8)     plt.grid(True)      for run in runs:         metric_history = client.get_metric_history(run.info.run_id, metric_name)         if metric_history:             steps = [m.step for m in metric_history]             values = [m.value for m in metric_history]              # Downsample: group by step_interval and take average             smoothed_steps = []             smoothed_values = []              for i in range(0, len(steps), step_interval):                 chunk_steps = steps[i:i+step_interval]                 chunk_values = values[i:i+step_interval]                  smoothed_steps.append(np.mean(chunk_steps))                 smoothed_values.append(np.mean(chunk_values))              run_params = run.data.params             legend_label = f\"Run {run.info.run_id[:8]}\"             if \"lr\" in run_params:                 legend_label += f\" (lr={float(run_params['lr']):.18f})\"             if \"batch_size\" in run_params:                 legend_label += f\" (bs={run_params['batch_size']})\"             # print(metric_name, run_params)              plt.plot(smoothed_steps, smoothed_values, label=legend_label)         else:             print(f\"No history for metric '{metric_name}' in run {run.info.run_id}\")      # plt.legend(loc=\"lower center\", bbox_to_anchor=(0.5, -0.5), ncol=2)     plt.legend(         loc=\"upper center\",         bbox_to_anchor=(0.5, -0.20),         ncol=1,  # number of columns (adjust based on #runs)         frameon=False,         fontsize=8,     )     plt.tight_layout()     plt.show()"},{"location":"examples/fine-tuning/003-Terratorch-Iterate/#finetuning-hpo-with-terratorch-iterate","title":"FineTuning: HPO With Terratorch Iterate\u00b6","text":"<p>\ud83d\udce5 Download 003-Terratorch-Iterate.ipynb and try it out</p>"},{"location":"examples/fine-tuning/003-Terratorch-Iterate/#introduction","title":"Introduction\u00b6","text":"<p>This notebook demonstrates how to use the FineTuning SDK to submit an HPO (Hyperparameter Optimization) job to the FineTuning service.</p>"},{"location":"examples/fine-tuning/003-Terratorch-Iterate/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before proceeding with this notebook, ensure you have:</p> <ul> <li>Active GeoStudio Service Access: Valid credentials and permissions for the GeoStudio inference service</li> <li>SDK Installation: The GeoStudio SDK installed in your environment</li> <li>Authentication Setup: API keys configured (either via environment variables or key files)</li> <li>TerraTorch Iterate Config File: A prepared configuration file (.yaml) for running fine-tuning.</li> </ul> <p>Note: This workflow assumes you have already prepared a configuration file for TerraTorch Iterate. If you need guidance on fine-tuning TerraTorch models with HPO enabled, refer to the TerraTorch-Iterate documentation first.</p>"},{"location":"examples/fine-tuning/003-Terratorch-Iterate/#imports-setup","title":"Imports &amp; Setup\u00b6","text":""},{"location":"examples/fine-tuning/003-Terratorch-Iterate/#connecting-to-geospatial-studio","title":"Connecting to Geospatial Studio\u00b6","text":""},{"location":"examples/fine-tuning/003-Terratorch-Iterate/#preparing-and-onboarding-data-required-for-finetuning","title":"Preparing and Onboarding Data Required for FineTuning\u00b6","text":"<p>In order to onboard your dataset to the Geospatial Studio, you need to have a direct download URL pointing to a zip file of the dataset. Review the notebooks on data <code>../dataset-onboarding</code> to onboard new datasets into the studio.</p> <p>If you already have some onboarded datasets, look them up and select the one you need for finetuning.</p>"},{"location":"examples/fine-tuning/003-Terratorch-Iterate/#submitting-the-tune","title":"Submitting the tune\u00b6","text":"<p>Once the data is onboarded and you have a valid terratorch-iterate config yaml, you are ready to kick off your hpo tuning task. In order to run a fine-tuning task, you need to select the following items:</p> <ul> <li><code>tune_metadata</code>: Identifying info about your tune such as:<ul> <li><code>name</code>: A name to identify your tune</li> <li><code>description</code>: Some detailed description about your experiment.</li> <li><code>dataset_id</code>: A dataset id for a dataset that should have been onboarded in the Studio.</li> </ul> </li> </ul>"},{"location":"examples/fine-tuning/003-Terratorch-Iterate/#fetch-tune-results","title":"Fetch tune results\u00b6","text":""},{"location":"examples/fine-tuning/003-Terratorch-Iterate/#mlflow-experiment-visualization","title":"MLFlow Experiment Visualization\u00b6","text":""},{"location":"examples/fine-tuning/003-Terratorch-Iterate/#done","title":"DONE\u00b6","text":""},{"location":"examples/fine-tuning/004-Create-User-Defined-Tuning-Templates/","title":"004-Create-User-Defined-Tuning-Templates","text":"In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre># first import the required packages\nimport json\nimport uuid\nimport yaml\nimport base64\n\nfrom IPython.display import display, Markdown\n\nimport urllib3\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\nfrom geostudio import Client\nfrom geostudio import gswidgets\n</pre> # first import the required packages import json import uuid import yaml import base64  from IPython.display import display, Markdown  import urllib3 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  from geostudio import Client from geostudio import gswidgets In\u00a0[\u00a0]: Copied! <pre>#############################################################\n# Initialize Geostudio client using a geostudio config file\n#############################################################\ngfm_client = Client(geostudio_config_file=\".geostudio_config_file\")\n</pre>  ############################################################# # Initialize Geostudio client using a geostudio config file ############################################################# gfm_client = Client(geostudio_config_file=\".geostudio_config_file\")  In\u00a0[\u00a0]: Copied! <pre>tasks = gfm_client.list_tune_templates(output=\"df\")\ndisplay(tasks[[\"id\",\"description\",\"created_by\",\"updated_at\"]])\n</pre> tasks = gfm_client.list_tune_templates(output=\"df\") display(tasks[[\"id\",\"description\",\"created_by\",\"updated_at\"]]) In\u00a0[\u00a0]: Copied! <pre>datasets = gfm_client.list_datasets(output=\"df\")\ndisplay(datasets[['id','description','created_by','updated_at']])\n</pre> datasets = gfm_client.list_datasets(output=\"df\") display(datasets[['id','description','created_by','updated_at']]) In\u00a0[\u00a0]: Copied! <pre>selected_dataset = \"geodata-gdctf3vb3znbbtgptqvuku\"\n# selected_dataset = datasets[\"dataset_id\"][0]\n</pre> selected_dataset = \"geodata-gdctf3vb3znbbtgptqvuku\" # selected_dataset = datasets[\"dataset_id\"][0] <p>A new task is expected to have these params in the payload:</p> <pre>{\n    \"name\": \"\",\n    \"description\": \"\",\n    \"purpose\": \"Other\", # Must be Other\n    \"content\": \"\", # base64 encoding of the template\n    \"extra_info\":{\"runtime_image\": \"us.icr.io/gfmaas/geostudio-ft-deploy:v0.99.9.post1-117\", \"model_framework\": \"terratorch-v2\"}, \n    \"model_params\":{},\n    \"dataset_id\": selected_dataset # dataset_id\n}\n</pre> <p>Generate the base64 encoding of your template below and add it to the payload.</p> In\u00a0[\u00a0]: Copied! <pre>def encode_file_to_base64(file_path):\n    with open(file_path, \"rb\") as file:\n        # Read the file in binary mode\n        file_content = file.read()\n        \n        # Encode the content to base64\n        base64_encoded = base64.b64encode(file_content)\n        \n        # Decode the Base64 bytes into a string (if needed)\n        base64_string = base64_encoded.decode('utf-8')\n        \n    return base64_string\n\nencoded_content = encode_file_to_base64(\"../sample_files/sample-convnext-config.yaml\")\nencoded_content\n</pre> def encode_file_to_base64(file_path):     with open(file_path, \"rb\") as file:         # Read the file in binary mode         file_content = file.read()                  # Encode the content to base64         base64_encoded = base64.b64encode(file_content)                  # Decode the Base64 bytes into a string (if needed)         base64_string = base64_encoded.decode('utf-8')              return base64_string  encoded_content = encode_file_to_base64(\"../sample_files/sample-convnext-config.yaml\") encoded_content In\u00a0[\u00a0]: Copied! <pre>created_task = gfm_client.create_task(\n    data={\n        \"name\": \"user-new-task\",\n        \"description\": \"user new task\",\n        \"purpose\": \"Other\",  # DO NOT CHANGE THIS\n        \"content\": encoded_content,  # base64 encoding of the template\n        \"extra_info\": {\n            \"runtime_image\": \"us.icr.io/gfmaas/geostudio-ft-deploy:feat-update_tt_version-142\",\n            \"model_framework\": \"terratorch-v2\",\n        },\n        \"model_params\": {},\n        \"dataset_id\": selected_dataset # dataset id\n    }\n)\ncreated_task\n</pre> created_task = gfm_client.create_task(     data={         \"name\": \"user-new-task\",         \"description\": \"user new task\",         \"purpose\": \"Other\",  # DO NOT CHANGE THIS         \"content\": encoded_content,  # base64 encoding of the template         \"extra_info\": {             \"runtime_image\": \"us.icr.io/gfmaas/geostudio-ft-deploy:feat-update_tt_version-142\",             \"model_framework\": \"terratorch-v2\",         },         \"model_params\": {},         \"dataset_id\": selected_dataset # dataset id     } ) created_task In\u00a0[\u00a0]: Copied! <pre># Now we can get the task template yaml for the selected task.  This can\n# be returned as a string in a new cell which can be updated and edited in the\n# notebook, as a file (by setting output='file') or a text string ('text').\ntt = gfm_client.get_task_template(created_task[\"id\"], output='cell')\n</pre> # Now we can get the task template yaml for the selected task.  This can # be returned as a string in a new cell which can be updated and edited in the # notebook, as a file (by setting output='file') or a text string ('text'). tt = gfm_client.get_task_template(created_task[\"id\"], output='cell') In\u00a0[\u00a0]: Copied! <pre>print(selected_dataset)\nprint(created_task['id'])\n</pre> print(selected_dataset) print(created_task['id']) In\u00a0[\u00a0]: Copied! <pre>rendered_template = gfm_client.render_template(task_id = created_task['id'], dataset_id=selected_dataset, output=\"cell\")\n</pre> rendered_template = gfm_client.render_template(task_id = created_task['id'], dataset_id=selected_dataset, output=\"cell\") In\u00a0[\u00a0]: Copied! <pre>display(Markdown(f\"```yaml\\n{rendered_template}\\n```\"))\n</pre>  display(Markdown(f\"```yaml\\n{rendered_template}\\n```\")) In\u00a0[\u00a0]: Copied! <pre># updated_file_path = \"../sample_files/sample-convnext-config.yaml\"\n# gfm_client.update_task(task_id=\"37382f53-44dc-4667-9270-634757a33c67\",\n#                           file_path=updated_file_path)\n</pre> # updated_file_path = \"../sample_files/sample-convnext-config.yaml\" # gfm_client.update_task(task_id=\"37382f53-44dc-4667-9270-634757a33c67\", #                           file_path=updated_file_path) In\u00a0[\u00a0]: Copied! <pre># rendered_template = gfm_client.render_template(task_id = created_task['id'], dataset_id = selected_dataset, output=\"text\")\n</pre> # rendered_template = gfm_client.render_template(task_id = created_task['id'], dataset_id = selected_dataset, output=\"text\") In\u00a0[\u00a0]: Copied! <pre># display(Markdown(f\"```yaml\\n{rendered_template}\\n```\"))\n</pre> # display(Markdown(f\"```yaml\\n{rendered_template}\\n```\")) In\u00a0[\u00a0]: Copied! <pre>tune_payload= {\n  \"name\": \"burns-user-defined\",\n  \"description\": \"test-user-defined-config with burnscars data\",\n  \"dataset_id\": selected_dataset,\n  \"tune_template_id\": created_task['id'],\n  \"model_parameters\": {\n      \"runner\": {\n            \"max_epochs\": \"10\"\n        }\n  },\n  \"train_options\": {\n      \"tune_type\":\"user-defined\", # Required.\n      \"image\": \"us.icr.io/gfmaas/geostudio-ft-deploy:feat-update_tt_version-142\", # Specific terratorch version to override\n  }\n}\n</pre> tune_payload= {   \"name\": \"burns-user-defined\",   \"description\": \"test-user-defined-config with burnscars data\",   \"dataset_id\": selected_dataset,   \"tune_template_id\": created_task['id'],   \"model_parameters\": {       \"runner\": {             \"max_epochs\": \"10\"         }   },   \"train_options\": {       \"tune_type\":\"user-defined\", # Required.       \"image\": \"us.icr.io/gfmaas/geostudio-ft-deploy:feat-update_tt_version-142\", # Specific terratorch version to override   } }  In\u00a0[\u00a0]: Copied! <pre>submitted_tune = gfm_client.submit_tune(data=tune_payload)\n</pre> submitted_tune = gfm_client.submit_tune(data=tune_payload) In\u00a0[\u00a0]: Copied! <pre>submitted_tune\n</pre> submitted_tune In\u00a0[\u00a0]: Copied! <pre># If you wish to you can keep polling the tuning task to monitor its progress. Note that this operation is very expensive\n# r = gfm_client.poll_finetuning_until_finished(tune_id=submitted_tune['tune_id'])\n</pre> # If you wish to you can keep polling the tuning task to monitor its progress. Note that this operation is very expensive # r = gfm_client.poll_finetuning_until_finished(tune_id=submitted_tune['tune_id']) In\u00a0[\u00a0]: Copied! <pre># Get Mlflow urls for the tune\ngfm_client.get_mlflow_metrics(submitted_tune['tune_id'])\n</pre> # Get Mlflow urls for the tune gfm_client.get_mlflow_metrics(submitted_tune['tune_id']) <p>Now that you have created and updated your task, you can proceed to carry out fine-tuning following the instructions in the fine-tuning tutorial.</p> In\u00a0[\u00a0]: Copied! <pre>gfm_client.delete_task(task_id = created_task['id'])\n</pre> gfm_client.delete_task(task_id = created_task['id']) In\u00a0[\u00a0]: Copied! <pre>rendered_template = gfm_client.render_template(task_id = created_task['id'], dataset_id = selected_dataset)\nrendered_template\n</pre> rendered_template = gfm_client.render_template(task_id = created_task['id'], dataset_id = selected_dataset) rendered_template In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/fine-tuning/004-Create-User-Defined-Tuning-Templates/#004-create-user-defined-tuning-templates","title":"004-Create-User-Defined-Tuning-Templates\u00b6","text":"<p>\ud83d\udce5 Download 004-Create-User-Defined-Tuning-Templates.ipynb and try it out</p>"},{"location":"examples/fine-tuning/004-Create-User-Defined-Tuning-Templates/#introduction","title":"Introduction\u00b6","text":"<p>This notebook is intended to be an introduction to creating and managing Tuning templates using the Geospatial Studio SDK.  The tutorial assumes the user wishes to use an existing template and data in the studio.</p> <p>For more information about the Geospatial Studio see the docs page: Geospatial Studio Docs.</p>"},{"location":"examples/fine-tuning/004-Create-User-Defined-Tuning-Templates/#prerequisites","title":"Prerequisites\u00b6","text":"<p>It is assumed that you have installed the Geospatial Studio SDK and have a network connection to the platform. Instructions for both can be found here:  Geospatial Studio SDK Docs</p>"},{"location":"examples/fine-tuning/004-Create-User-Defined-Tuning-Templates/#connecting-to-the-platform","title":"Connecting to the platform\u00b6","text":"<p>First, we set up the connection to the platform backend.  To do this we need the base url for the studio UI and an API key.</p> <p>To get an API Key:</p> <ol> <li>Go to the Geospatial Studio UI page and navigate to the Manage your API keys link.</li> <li>This should pop-up a window where you can generate, access and delete your api keys. NB: every user is limited to a maximum of two activate api keys at any one time.</li> </ol> <p>Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:</p> <pre>echo \"GEOSTUDIO_API_KEY=&lt;paste_api_key_here&gt;\" &gt; .geostudio_config_file\necho \"BASE_STUDIO_UI_URL=&lt;paste_ui_base_url_here&gt;\" &gt;&gt; .geostudio_config_file\n</pre> <p>Copy and paste the file path to this credentials file in call below.</p>"},{"location":"examples/fine-tuning/004-Create-User-Defined-Tuning-Templates/#browse-existing-task-templates","title":"Browse existing task templates\u00b6","text":""},{"location":"examples/fine-tuning/004-Create-User-Defined-Tuning-Templates/#browse-existing-datasets","title":"Browse existing datasets\u00b6","text":""},{"location":"examples/fine-tuning/004-Create-User-Defined-Tuning-Templates/#create-your-own-task","title":"Create your own task\u00b6","text":""},{"location":"examples/fine-tuning/004-Create-User-Defined-Tuning-Templates/#review-the-created-task-with-the-original-template","title":"Review the created task with the original template\u00b6","text":""},{"location":"examples/fine-tuning/004-Create-User-Defined-Tuning-Templates/#review-the-created-task-with-the-replaced-template-with-the-provided-dataet","title":"Review the created task with the replaced template with the provided dataet\u00b6","text":""},{"location":"examples/fine-tuning/004-Create-User-Defined-Tuning-Templates/#update-the-created-task-if-the-the-above-blocks-are-not-correct","title":"Update the created task if the the above blocks are not correct\u00b6","text":"<p>If the template is not formated as expected, uncomment the cells below to update it</p>"},{"location":"examples/fine-tuning/004-Create-User-Defined-Tuning-Templates/#now-you-can-use-this-task-to-run-a-fine-tuning","title":"Now you can use this task to run a Fine tuning\u00b6","text":"<p>You can now submit a tune. Change the name and description</p>"},{"location":"examples/fine-tuning/004-Create-User-Defined-Tuning-Templates/#delete-a-task","title":"Delete a task\u00b6","text":""},{"location":"examples/inference/001-Introduction-to-Inferencing/","title":"001-Introduction-to-Inferencing","text":"In\u00a0[\u00a0]: Copied! <pre># Install extra requirements\n! pip install boto3\n</pre> # Install extra requirements ! pip install boto3 In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre># import the required packages\nimport json\nimport uuid\nimport pandas as pd\nimport wget\nimport rasterio\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML\n# import seaborn as sns\nimport getpass # For use in Colab as well\n\nimport urllib3\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\nfrom geostudio import Client\nfrom geostudio import gswidgets\n</pre> # import the required packages import json import uuid import pandas as pd import wget import rasterio import matplotlib.pyplot as plt from IPython.display import display, HTML # import seaborn as sns import getpass # For use in Colab as well  import urllib3 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  from geostudio import Client from geostudio import gswidgets In\u00a0[\u00a0]: Copied! <pre>#############################################################\n# Initialize Geostudio client using a geostudio config file\n#############################################################\ngfm_client = Client(geostudio_config_file=\".geostudio_config_file\")\n</pre> ############################################################# # Initialize Geostudio client using a geostudio config file ############################################################# gfm_client = Client(geostudio_config_file=\".geostudio_config_file\")  <p>Now we have connected to the Geospatial Studio backend, we are ready to set up an inference run.  To run inference you need to choose a model to run, and define the spatial and temporal domain over which to run the model.  This is done with a json payload sent to the inference gateway.</p> <pre>request_payload = {\n\t\"model_display_name\": \"prithvi-eo-flood-blair\",\n\t\"description\": \"Jarani, Nagaon, Nagaon, Assam, India\",\n\t\"location\": \"Jarani, Nagaon, Nagaon, Assam, India\",\n\t\"spatial_domain\": {\n\t\t\"bbox\": [[92.40665153547121, 26.1051042015407,92.92535070071905,26.498933088370826]],\n\t\t\"polygons\": [],\n\t\t\"tiles\": [],\n\t\t\"urls\": []\n\t},\n\t\"temporal_domain\": [\"2024-07-25_2024-07-27\"]\n}\n</pre> <p>You can then submit the request using:</p> <pre>response = gfm_client.submit_inference(\n    data=request_payload,\n    output=\"json\"\n)\n</pre> <p>The SDK also includes some widgets which can help you to browse the available models, define bounding boxes etc.</p> In\u00a0[\u00a0]: Copied! <pre># To list the available models deployed on the inference service you are connected to you can use the function:\n# [Optional] - use output=\"json\" to view full details about the models\nmodels = gfm_client.list_models(output=\"df\")\nmodels\n</pre> # To list the available models deployed on the inference service you are connected to you can use the function: # [Optional] - use output=\"json\" to view full details about the models models = gfm_client.list_models(output=\"df\") models In\u00a0[\u00a0]: Copied! <pre># if you want help choosing a bounding box, you can use the widget below and copy and paste the bbox i.e. [....] to the payload below.\n\ngswidgets.bboxSelector()\n</pre> # if you want help choosing a bounding box, you can use the widget below and copy and paste the bbox i.e. [....] to the payload below.  gswidgets.bboxSelector() <p>Now we put that information into the payload below and send the request to the cluster.</p> In\u00a0[\u00a0]: Copied! <pre>bbox = [-51.33225, -30.08903, -51.19011, -29.97489]\n\n# Choose a model by copying and pasting its display name here\nrequest_payload = {\n\t\"model_display_name\": \"prithvi-eo-flood\",\n\t\"description\": \"Porto Alegre, Brazil SDK flooding demo\",\n\t\"location\": \"Porto Alegre, Brazil\",\n\t\"spatial_domain\": {\n\t\t\t\"bbox\": [bbox],\n\t\t\t\"polygons\": [],\n\t\t\t\"tiles\": [],\n\t\t\t\"urls\": []\n\t},\n\t\"temporal_domain\": [\n\t\t\t\"2024-05-06_2024-05-07\"\n\t]\n}\n\nresponse = gfm_client.submit_inference(data=request_payload)\nresponse\n</pre> bbox = [-51.33225, -30.08903, -51.19011, -29.97489]  # Choose a model by copying and pasting its display name here request_payload = { \t\"model_display_name\": \"prithvi-eo-flood\", \t\"description\": \"Porto Alegre, Brazil SDK flooding demo\", \t\"location\": \"Porto Alegre, Brazil\", \t\"spatial_domain\": { \t\t\t\"bbox\": [bbox], \t\t\t\"polygons\": [], \t\t\t\"tiles\": [], \t\t\t\"urls\": [] \t}, \t\"temporal_domain\": [ \t\t\t\"2024-05-06_2024-05-07\" \t] }  response = gfm_client.submit_inference(data=request_payload) response <p>If you have your image locally and would like to pre-sign the image using S3.</p> In\u00a0[\u00a0]: Copied! <pre>upload_url = gfm_client.create_upload_presigned_url(\n    bucket_name=\"bucket_name\", # bucket name\n    object_key=\"data/train/austin1_sdk_upload.tiff\", # file path to upload in the bucket\n    endpoint_url=\"https://s3.us-east.cloud-object-storage.appdomain.cloud\", # s3 endpoint url\n    service_name= \"s3\", # service to use\n    region_name=\"us-east\", # cloud region \n    expiration=3600 # expiration\n    # Add any other args to pass to the s3 client\n)\nupload_url\n</pre> upload_url = gfm_client.create_upload_presigned_url(     bucket_name=\"bucket_name\", # bucket name     object_key=\"data/train/austin1_sdk_upload.tiff\", # file path to upload in the bucket     endpoint_url=\"https://s3.us-east.cloud-object-storage.appdomain.cloud\", # s3 endpoint url     service_name= \"s3\", # service to use     region_name=\"us-east\", # cloud region      expiration=3600 # expiration     # Add any other args to pass to the s3 client ) upload_url In\u00a0[\u00a0]: Copied! <pre># Push your file to the bucket using the url generated.\n!curl -X PUT -T **your_file.zip or your_file.tiff or your_file.tif** \"**upload_url**\"\n</pre> # Push your file to the bucket using the url generated. !curl -X PUT -T **your_file.zip or your_file.tiff or your_file.tif** \"**upload_url**\" <p>Once the image is uploaded to your s3 bucket, create a download link to use in the inference request.</p> In\u00a0[\u00a0]: Copied! <pre>download_url = gfm_client.create_download_presigned_url(\n    bucket_name=\"geospatial-studio-example-data\", # bucket name\n    object_key=\"data/train/austin1_sdk_upload.tiff\", # file path to upload in the bucket\n    endpoint_url=\"https://s3.us-east.cloud-object-storage.appdomain.cloud\", # s3 endpoint url\n    service_name= \"s3\", # service to use\n    region_name=\"us-east\", # cloud region \n    expiration=7200 # expiration\n    # Add any other args to pass to the s3 client\n\n)\ndownload_url\n</pre> download_url = gfm_client.create_download_presigned_url(     bucket_name=\"geospatial-studio-example-data\", # bucket name     object_key=\"data/train/austin1_sdk_upload.tiff\", # file path to upload in the bucket     endpoint_url=\"https://s3.us-east.cloud-object-storage.appdomain.cloud\", # s3 endpoint url     service_name= \"s3\", # service to use     region_name=\"us-east\", # cloud region      expiration=7200 # expiration     # Add any other args to pass to the s3 client  ) download_url <p>If you would like to upload to a geostudio temporary bucket, use this function <code>get_fileshare_links</code> function.</p> In\u00a0[\u00a0]: Copied! <pre># Unique object name to be used in temporary COS for each layer you want to upload\nobject_name = \"austin1_sdk_upload.tiff\"\ngfm_client.get_fileshare_links(object_name)\n</pre>  # Unique object name to be used in temporary COS for each layer you want to upload object_name = \"austin1_sdk_upload.tiff\" gfm_client.get_fileshare_links(object_name)  In\u00a0[\u00a0]: Copied! <pre># Push your file to the bucket using the url generated.\n!curl -X PUT -T **your_file.zip or your_file.tiff or your_file.tif** \"**upload_url**\"\n</pre> # Push your file to the bucket using the url generated. !curl -X PUT -T **your_file.zip or your_file.tiff or your_file.tif** \"**upload_url**\" In\u00a0[\u00a0]: Copied! <pre># grab the download url for use in inference. \ndownload_url_tiff = download_url\n# Choose a model by copying and pasting its display name here\nrequest_payload_with_url = {\n\t\"model_display_name\": \"prithvi-eo-flood\",\n\t\"description\": \"Your inference description\",\n\t\"location\": \"Your tiff location\",\n\t\"spatial_domain\": {\n\t\t\t\"bbox\": [],\n\t\t\t\"polygons\": [],\n\t\t\t\"tiles\": [],\n\t\t\t\"urls\": [download_url_tiff]\n\t},\n\t\"temporal_domain\": [\n\t\t\t\"2024-05-06_2024-05-07\"\n\t]\n}\n\nresponse = gfm_client.submit_inference(data=request_payload_with_url)\nresponse\n</pre> # grab the download url for use in inference.  download_url_tiff = download_url # Choose a model by copying and pasting its display name here request_payload_with_url = { \t\"model_display_name\": \"prithvi-eo-flood\", \t\"description\": \"Your inference description\", \t\"location\": \"Your tiff location\", \t\"spatial_domain\": { \t\t\t\"bbox\": [], \t\t\t\"polygons\": [], \t\t\t\"tiles\": [], \t\t\t\"urls\": [download_url_tiff] \t}, \t\"temporal_domain\": [ \t\t\t\"2024-05-06_2024-05-07\" \t] }  response = gfm_client.submit_inference(data=request_payload_with_url) response In\u00a0[\u00a0]: Copied! <pre># Poll inference status\ngfm_client.poll_inference_until_finished(inference_id=response['id'])\n</pre> # Poll inference status gfm_client.poll_inference_until_finished(inference_id=response['id'])  In\u00a0[\u00a0]: Copied! <pre>gfm_client.get_inference(inference_id=response['id'])\n</pre> gfm_client.get_inference(inference_id=response['id']) In\u00a0[\u00a0]: Copied! <pre># Get the inference tasks list\n\ninf_tasks_res = gfm_client.get_inference_tasks(response[\"id\"])\ninf_tasks_res\n</pre> # Get the inference tasks list  inf_tasks_res = gfm_client.get_inference_tasks(response[\"id\"]) inf_tasks_res <p>Next, Identify the task you want to view from the response above, ensure status of the task is FINISHED and set <code>selected_task</code> variable below to the task number at the end of the task id string. For example, if <code>task_id</code> is \"6d1149fa-302d-4612-82dd-5879fc06081d-task_0\", selected_task would be 0</p> In\u00a0[\u00a0]: Copied! <pre># Select a task to view\n\nselected_task = 0 \nselected_task_id = f\"{inf_tasks_res['inference_id']}-task_{selected_task}\"\n</pre> # Select a task to view  selected_task = 0  selected_task_id = f\"{inf_tasks_res['inference_id']}-task_{selected_task}\" In\u00a0[\u00a0]: Copied! <pre># Download task output files\n\ngswidgets.fileDownloaderTasks(client=gfm_client, task_id=selected_task_id)\n</pre> # Download task output files  gswidgets.fileDownloaderTasks(client=gfm_client, task_id=selected_task_id) In\u00a0[\u00a0]: Copied! <pre># Paste the name (+path) to one of the files you downloaded and select the band you want to load+plot\nfilename = '6d1149fa-302d-4612-82dd-5879fc06081d-task_0_HLS_L30_2024-08-12_imputed__merged_pred_masked.tif'\nband_number = 1\n\n# open the file and read the band and metadata with rasterio\nwith rasterio.open(filename) as fp:\n    data = fp.read(band_number)\n    bounds = fp.bounds\n\n\nprint(\"Image dimensions: \" + str(data.shape))\n\nplt.imshow(data, extent=[bounds.left, bounds.right, bounds.bottom, bounds.top])\nplt.xlabel('Longitude'); plt.xlabel('Latitude')\n</pre> # Paste the name (+path) to one of the files you downloaded and select the band you want to load+plot filename = '6d1149fa-302d-4612-82dd-5879fc06081d-task_0_HLS_L30_2024-08-12_imputed__merged_pred_masked.tif' band_number = 1  # open the file and read the band and metadata with rasterio with rasterio.open(filename) as fp:     data = fp.read(band_number)     bounds = fp.bounds   print(\"Image dimensions: \" + str(data.shape))  plt.imshow(data, extent=[bounds.left, bounds.right, bounds.bottom, bounds.top]) plt.xlabel('Longitude'); plt.xlabel('Latitude')  In\u00a0[\u00a0]: Copied! <pre># Visualize output files with the SDK\n\ngswidgets.inferenceTaskViewer(client=gfm_client , task_id=selected_task_id)\n</pre> # Visualize output files with the SDK  gswidgets.inferenceTaskViewer(client=gfm_client , task_id=selected_task_id) In\u00a0[\u00a0]: Copied! <pre>gfm_client.list_inferences()\n</pre> gfm_client.list_inferences()"},{"location":"examples/inference/001-Introduction-to-Inferencing/#001-introduction-to-inferencing","title":"001-Introduction-to-Inferencing\u00b6","text":"<p>\ud83d\udce5 Download 001-Introduction-to-Inferencing.ipynb and try it out</p>"},{"location":"examples/inference/001-Introduction-to-Inferencing/#introduction","title":"Introduction\u00b6","text":"<p>This notebook is meant for someone with minimal knowledge of Geospatial, to be able to meaningfully use the most important functions of the Geospatial Studio SDK to run a model.</p> <p>For more information about the Geospatial Studio see the docs page: Geospatial Studio Docs</p> <p>For more information about the Geospatial Studio SDK and all the functions available through it, see the SDK docs page: Geospatial Studio SDK Docs</p>"},{"location":"examples/inference/001-Introduction-to-Inferencing/#connecting-to-the-platform","title":"Connecting to the platform\u00b6","text":"<p>First, we set up the connection to the platform backend.  To do this we need the base url for the studio UI and an API key.</p> <p>To get an API Key:</p> <ol> <li>Go to the Geospatial Studio UI page and navigate to the Manage your API keys link.</li> <li>This should pop-up a window where you can generate, access and delete your api keys. NB: every user is limited to a maximum of two activate api keys at any one time.</li> </ol> <p>Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:</p> <pre>echo \"GEOSTUDIO_API_KEY=&lt;paste_api_key_here&gt;\" &gt; .geostudio_config_file\necho \"BASE_STUDIO_UI_URL=&lt;paste_ui_base_url_here&gt;\" &gt;&gt; .geostudio_config_file\n</pre> <p>Copy and paste the file path to this credentials file in call below.</p>"},{"location":"examples/inference/001-Introduction-to-Inferencing/#defining-the-inference-query","title":"Defining the Inference query\u00b6","text":"<p>Studio allows inference requests by either providing the bounding box or a downloadabe link to the files.</p>"},{"location":"examples/inference/001-Introduction-to-Inferencing/#using-bounding-box","title":"Using bounding box\u00b6","text":""},{"location":"examples/inference/001-Introduction-to-Inferencing/#using-an-s3-pre-signed-link","title":"Using an S3 pre-signed link\u00b6","text":""},{"location":"examples/inference/001-Introduction-to-Inferencing/#personal-buckets","title":"Personal buckets\u00b6","text":"<p>Use the <code>create_upload_presigned_url</code> to generate an upload link that you can use to upload the file to the dataset.</p> <p>This function assumes you have your own storage bucket to upload to.</p>"},{"location":"examples/inference/001-Introduction-to-Inferencing/#geostudio-temporary-buckets","title":"Geostudio temporary buckets\u00b6","text":""},{"location":"examples/inference/001-Introduction-to-Inferencing/#submit-inference","title":"Submit Inference\u00b6","text":"<p>Now you can create the inference payload using the download link.</p>"},{"location":"examples/inference/001-Introduction-to-Inferencing/#monitor-tuning-status-and-progress","title":"Monitor tuning status and progress\u00b6","text":"<p>After submitting the request, we can poll the inference service to check the progress as well as get the output details once its complete (this could take a few minutes depending on the request size and the current service load).</p>"},{"location":"examples/inference/001-Introduction-to-Inferencing/#accessing-inference-outputs","title":"Accessing inference outputs\u00b6","text":"<p>Once an inference run is completed, the inputs and outputs of each task within an inference are packaged up into a zip file which is uploaded to a url you can use to download the files.</p> <p>To access the inference task files:</p> <ol> <li>Get the inference tasks list</li> <li>Identify the specific inference task you want to view</li> <li>Download task output files</li> </ol>"},{"location":"examples/inference/001-Introduction-to-Inferencing/#visualizing-the-output-of-the-inference-runs","title":"Visualizing the output of the inference runs\u00b6","text":"<p>You can check out the results visually in the Studio UI, or with the quick widget below.  You can alternatively use the SDK to download selected files for further analysis see documentation.</p> <p>We have several options for visualising the data:</p> <ul> <li>we can load the data with a package like rasterio and plot the images, and/or access the values.</li> <li>we could use the widget from the SDK to visualise the chosen files for a inference run. (shown below)</li> <li>view the data in the Geospatial Studio Inference lab UI.</li> <li>load the files in an external software, such as QGIS.</li> </ul>"},{"location":"examples/inference/001-Introduction-to-Inferencing/#load-the-data-with-a-package-rasterio-and-plot-the-images-andor-access-the-values","title":"Load the data with a package rasterio and plot the images, and/or access the values.\u00b6","text":""},{"location":"examples/inference/001-Introduction-to-Inferencing/#visualize-through-the-sdk-widgets","title":"Visualize through the SDK widgets\u00b6","text":""},{"location":"examples/inference/001-Introduction-to-Inferencing/#list-past-inference-runs","title":"List past inference runs\u00b6","text":"<p>All past inference runs from a user are stored in the Studio database and the user can access this historical record, with the ability to retrieve past output data.  This can be done using the simple sdk function:</p> <pre><code>gfm_client.list_inferences()\n</code></pre> <p>or similarly using the sdk widget:</p>"},{"location":"examples/inference/002-Add-Precomputed-Examples/","title":"002-Add-Precomputed-Examples","text":"In\u00a0[\u00a0]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre># import the required packages\nfrom geostudio import Client\nfrom geostudio import gswidgets\n</pre> # import the required packages from geostudio import Client from geostudio import gswidgets In\u00a0[\u00a0]: Copied! <pre>#############################################################\n# Initialize Geostudio client using a geostudio config file\n#############################################################\ngfm_client = Client(geostudio_config_file=\".geostudio_config_file\")\n</pre>  ############################################################# # Initialize Geostudio client using a geostudio config file ############################################################# gfm_client = Client(geostudio_config_file=\".geostudio_config_file\")  In\u00a0[\u00a0]: Copied! <pre># Unique object name to be used in temporary COS for each layer you want to upload\nobject_name = \"my-test-layer.zip\"\ngfm_client.get_fileshare_links(object_name)\n</pre> # Unique object name to be used in temporary COS for each layer you want to upload object_name = \"my-test-layer.zip\" gfm_client.get_fileshare_links(object_name) <p>use the <code>upload_url</code> to upload each file you want to add.</p> <pre>curl -X PUT -T **your_file.zip or your_file.gpkg or your_file.nc** \"**upload_url**\"\n</pre> <p>use the equivalent <code>download_url</code> for the section below.</p> <p>Now we put that information into the payload below and send the request to the cluster.</p> In\u00a0[\u00a0]: Copied! <pre>request_payload = {\n  \"fine_tuning_id\": \"sandbox\", // DO NOT CHANGE\n  \"spatial_domain\": {\n    \"urls\": [\n      \"https://download_url.zip\" // PRESIGNED URL WITH IMAGES TO BE ONBOARDED\n    ]\n  },\n  \"temporal_domain\": [],\n  \"geoserver_push\": [\n    {\n      \"workspace\": \"geofm\", // DO NOT CHANGE\n      \"layer_name\": \"layer_name\", // LAYER NAME TO BE USED TO PUSH TO GEOSERVER. USE LOWER CASE ALPHANUMERIC JOINED WITH UNDERSCORE WITHOUT ANY OTHER SPECIAL CHARACTERS\n      \"display_name\": \"My Layer\", // THE NAME TO APPEAR IN THE UI\n      \"filepath_key\": \"original_input_image\", // DO NOT CHANGE\n      \"file_suffix\": \"\", // DO NOT CHANGE\n      \"z_index\": 1, // Z INDEX OF THE CURRENT LAYER ON THE UI MAP (HIGHEST NUMBER WILL APPEAR ON TOP OF ALL OTHER LAYERS)\n\t  \"visible_by_default\": \"True\", // WHETHER THE LAYER WILL BE SHOWN BY DEFAULT WHEN LOADED IN THE UI\n      \"coverage_name\": \"\", // ONLY USED FOR NETCDF TO POINT TO A PROPERTY OF INTEREST IN THE NETCDF\n      \"geoserver_style\": {\n        \"regression\": [ // RASTER REGRESSION STYLING\n          {\n            \"opacity\": 1,\n            \"quantity\": \"0\",\n            \"color\": \"#000dff\",\n            \"label\": \"Min\"\n          },\n          {\n            \"opacity\": 1,\n            \"quantity\": \"300\",\n            \"color\": \"#ff00d9\",\n            \"label\": \"MAX\"\n          }\n        ],\n        \"segmentation\": [ // RASTER SEGMENTATION STYLING\n          {\n            \"opacity\": 0,\n            \"quantity\": \"0\",\n            \"color\": \"#000dff\",\n            \"label\": \"No flood\"\n          },\n          {\n            \"opacity\": 1,\n            \"quantity\": \"1\",\n            \"color\": \"#ff00d9\",\n            \"label\": \"Flood\"\n          }\n        ],\n         \"rgb\": [ // RASTER RGB STYLING\n          {\n            \"minValue\": 0,\n            \"maxValue\": 255,\n            \"channel\": 1,\n            \"label\": \"RedChannel\"\n          },\n          {\n            \"minValue\": 0,\n            \"maxValue\": 255,\n            \"channel\": 2,\n            \"label\": \"GreenChannel\"\n          },\n          {\n            \"minValue\": 0,\n            \"maxValue\": 255,\n            \"channel\": 3,\n            \"label\": \"BlueChannel\"\n          }\n        ],\n        \"polygon_style\": { // VECTOR POLYGON STYLING EXAMPLE\n          \"fill\": \"#fffcfd\",\n          \"fill_opacity\": 0.5\n        },\n        \"point_style\": { // VECTOR POINT STYLING EXAMPLE\n          \"well_known_name\": \"circle\",\n          \"fill\": \"#b3b3b3\",\n          \"stroke\": \"#253c99\",\n          \"stroke_width\": 1,\n          \"size\": 6\n        },\n        \"line_style\": { // VECTOR LINE STYLING EXAMPLE\n          \"stroke\": \"#b0d6ff\",\n          \"stroke_width\": 1\n        }\n      }\n    }\n  ],\n  \"model_display_name\": \"add-layer-sandbox-model\", // DO NOT CHANGE &lt;IF YOU GET 404 ERROR TRY WITH \"add-layer-sandbox-models\"&gt;\n  \"description\": \"Descriptions for my layers\", // TEXTUAL DESCRIPTION OF THE ONBOARDED LAYERS\n  \"location\": \"Layers location\", // LOCATION NAME FOR THE LAYERS\n  \"demo\": {\n    \"demo\": true, // DO NOT CHANGE\n    \"section_name\": \"My Examples\" // DO NOT CHANGE\n  }\n}\n\nresponse = gfm_client.submit_inference(data=request_payload)\nresponse\n</pre> request_payload = {   \"fine_tuning_id\": \"sandbox\", // DO NOT CHANGE   \"spatial_domain\": {     \"urls\": [       \"https://download_url.zip\" // PRESIGNED URL WITH IMAGES TO BE ONBOARDED     ]   },   \"temporal_domain\": [],   \"geoserver_push\": [     {       \"workspace\": \"geofm\", // DO NOT CHANGE       \"layer_name\": \"layer_name\", // LAYER NAME TO BE USED TO PUSH TO GEOSERVER. USE LOWER CASE ALPHANUMERIC JOINED WITH UNDERSCORE WITHOUT ANY OTHER SPECIAL CHARACTERS       \"display_name\": \"My Layer\", // THE NAME TO APPEAR IN THE UI       \"filepath_key\": \"original_input_image\", // DO NOT CHANGE       \"file_suffix\": \"\", // DO NOT CHANGE       \"z_index\": 1, // Z INDEX OF THE CURRENT LAYER ON THE UI MAP (HIGHEST NUMBER WILL APPEAR ON TOP OF ALL OTHER LAYERS) \t  \"visible_by_default\": \"True\", // WHETHER THE LAYER WILL BE SHOWN BY DEFAULT WHEN LOADED IN THE UI       \"coverage_name\": \"\", // ONLY USED FOR NETCDF TO POINT TO A PROPERTY OF INTEREST IN THE NETCDF       \"geoserver_style\": {         \"regression\": [ // RASTER REGRESSION STYLING           {             \"opacity\": 1,             \"quantity\": \"0\",             \"color\": \"#000dff\",             \"label\": \"Min\"           },           {             \"opacity\": 1,             \"quantity\": \"300\",             \"color\": \"#ff00d9\",             \"label\": \"MAX\"           }         ],         \"segmentation\": [ // RASTER SEGMENTATION STYLING           {             \"opacity\": 0,             \"quantity\": \"0\",             \"color\": \"#000dff\",             \"label\": \"No flood\"           },           {             \"opacity\": 1,             \"quantity\": \"1\",             \"color\": \"#ff00d9\",             \"label\": \"Flood\"           }         ],          \"rgb\": [ // RASTER RGB STYLING           {             \"minValue\": 0,             \"maxValue\": 255,             \"channel\": 1,             \"label\": \"RedChannel\"           },           {             \"minValue\": 0,             \"maxValue\": 255,             \"channel\": 2,             \"label\": \"GreenChannel\"           },           {             \"minValue\": 0,             \"maxValue\": 255,             \"channel\": 3,             \"label\": \"BlueChannel\"           }         ],         \"polygon_style\": { // VECTOR POLYGON STYLING EXAMPLE           \"fill\": \"#fffcfd\",           \"fill_opacity\": 0.5         },         \"point_style\": { // VECTOR POINT STYLING EXAMPLE           \"well_known_name\": \"circle\",           \"fill\": \"#b3b3b3\",           \"stroke\": \"#253c99\",           \"stroke_width\": 1,           \"size\": 6         },         \"line_style\": { // VECTOR LINE STYLING EXAMPLE           \"stroke\": \"#b0d6ff\",           \"stroke_width\": 1         }       }     }   ],   \"model_display_name\": \"add-layer-sandbox-model\", // DO NOT CHANGE    \"description\": \"Descriptions for my layers\", // TEXTUAL DESCRIPTION OF THE ONBOARDED LAYERS   \"location\": \"Layers location\", // LOCATION NAME FOR THE LAYERS   \"demo\": {     \"demo\": true, // DO NOT CHANGE     \"section_name\": \"My Examples\" // DO NOT CHANGE   } }  response = gfm_client.submit_inference(data=request_payload) response In\u00a0[\u00a0]: Copied! <pre># Poll inference status\ngfm_client.poll_inference_until_finished(inference_id=response['id'])\n</pre> # Poll inference status gfm_client.poll_inference_until_finished(inference_id=response['id'])  In\u00a0[\u00a0]: Copied! <pre>gfm_client.get_inference(inference_id=response['id'])\n</pre> gfm_client.get_inference(inference_id=response['id']) In\u00a0[\u00a0]: Copied! <pre># Get the inference tasks list\n\ninf_tasks_res = gfm_client.get_inference_tasks(response[\"id\"])\ninf_tasks_res\n</pre> # Get the inference tasks list  inf_tasks_res = gfm_client.get_inference_tasks(response[\"id\"]) inf_tasks_res In\u00a0[\u00a0]: Copied! <pre>df = gfm_client.inference_task_status_df(response[\"id\"])\n\n\ndisplay(df.style.map(gswidgets.color_inference_tasks_by_status))\n</pre> df = gfm_client.inference_task_status_df(response[\"id\"])   display(df.style.map(gswidgets.color_inference_tasks_by_status)) In\u00a0[\u00a0]: Copied! <pre>gswidgets.view_inference_process_timeline(gfm_client, inference_id = response[\"id\"])\n</pre> gswidgets.view_inference_process_timeline(gfm_client, inference_id = response[\"id\"])"},{"location":"examples/inference/002-Add-Precomputed-Examples/#002-add-precomputed-examples","title":"002-Add-Precomputed-Examples\u00b6","text":"<p>\ud83d\udce5 Download 002-Add-Precomputed-Examples.ipynb and try it out</p>"},{"location":"examples/inference/002-Add-Precomputed-Examples/#introduction","title":"Introduction\u00b6","text":"<p>This notebook is meant for someone with minimal knowledge, to be able to meaningfully use the most important functions of the Geospatial SDK.</p> <p>For more information about the Geospatial Studio see the docs page: Geospatial Studio Docs</p> <p>For more information about the Geospatial Studio SDK and all the functions available through it, see the SDK docs page: Geospatial Studio SDK Docs</p>"},{"location":"examples/inference/002-Add-Precomputed-Examples/#connecting-to-the-platform","title":"Connecting to the platform\u00b6","text":"<p>First, we set up the connection to the platform backend.  To do this we need the base url for the studio UI and an API key.</p> <p>To get an API Key:</p> <ol> <li>Go to the Geospatial Studio UI page and navigate to the Manage your API keys link.</li> <li>This should pop-up a window where you can generate, access and delete your api keys. NB: every user is limited to a maximum of two activate api keys at any one time.</li> </ol> <p>Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:</p> <pre>echo \"GEOSTUDIO_API_KEY=&lt;paste_api_key_here&gt;\" &gt; .geostudio_config_file\necho \"BASE_STUDIO_UI_URL=&lt;paste_ui_base_url_here&gt;\" &gt;&gt; .geostudio_config_file\n</pre> <p>Copy and paste the file path to this credentials file in call below.</p>"},{"location":"examples/inference/002-Add-Precomputed-Examples/#preparing-layers","title":"Preparing layers\u00b6","text":"<p>Now we have connected to the Geospatial Studio, we are ready to start preparing layers to be onboarded. To add a layer to the studio, a presigned link is used and you have to upload the files to the link outside of the studio.</p>"},{"location":"examples/inference/002-Add-Precomputed-Examples/#prerequisites","title":"Prerequisites\u00b6","text":"<p>We support onboarding of;</p> <ol> <li>Raster Data<ol> <li>GeoTIFF</li> <li>NetCDF</li> </ol> </li> <li>Vector Data<ol> <li>Shapefile</li> <li>GeoPackage</li> </ol> </li> </ol> <p>Below are the requirements for each file type.</p>"},{"location":"examples/inference/002-Add-Precomputed-Examples/#geotiff","title":"GeoTIFF\u00b6","text":"<p>For GeoTIFFs, especially where you have the same spatial domain over different temporal domain, you need to have each of individual tif files have the date in the file name in the format in this example <code>somename_2024-04-27_whatever.tif</code> and all the tif files zipped in a <code>.zip</code> file.</p>"},{"location":"examples/inference/002-Add-Precomputed-Examples/#netcdf","title":"NetCDF\u00b6","text":"<p>Onboard a single NetCDF file with the extension <code>.nc</code></p>"},{"location":"examples/inference/002-Add-Precomputed-Examples/#shapefile","title":"Shapefile\u00b6","text":"<p>For shapefiles, zip all the mandatory files for a shp file in a <code>.zip</code> files. In the zip include only one shapefile.</p>"},{"location":"examples/inference/002-Add-Precomputed-Examples/#geopackage","title":"GeoPackage\u00b6","text":"<p>Onboard a single GeoPackage file with the extension <code>.gpkg</code></p>"},{"location":"examples/inference/002-Add-Precomputed-Examples/#pushing-the-layers-to-a-link","title":"Pushing the layers to a link\u00b6","text":"<p>If you have your layers already in an environment like S3 or BOX etc, you can just get presigned links to them and ignore this section. However, ensure the links include the extension of the file being onboarded, e.g. .zip (for both tiff and shapefile), .nc, .gpkg</p> <p>If you do not have an environment to upload your files, you can leverage a temporary COS storage that we provide. Generate the upload and download links using below code for each layer you want to add.</p>"},{"location":"examples/inference/002-Add-Precomputed-Examples/#defining-the-add-layer-query","title":"Defining the Add Layer query\u00b6","text":"<p>Now we have our layers ready, we are ready to set up an add layer run.  To add layer you need to define the spatial (urls) and temporal domain for the layers to be added.  This is done with a json payload sent to the studio.</p> <p>Below is a template json with comments on the different fields that need to be defined.</p> <pre>request_payload = {\n  \"fine_tuning_id\": \"sandbox\", // DO NOT CHANGE\n  \"spatial_domain\": {\n    \"urls\": [\n      \"https://download_url.zip\" // PRESIGNED URL WITH IMAGES TO BE ONBOARDED\n    ]\n  },\n  \"temporal_domain\": [],\n  \"geoserver_push\": [\n    {\n      \"workspace\": \"geofm\", // DO NOT CHANGE\n      \"layer_name\": \"layer_name\", // LAYER NAME TO BE USED TO PUSH TO GEOSERVER. USE LOWER CASE ALPHANUMERIC JOINED WITH UNDERSCORE WITHOUT ANY OTHER SPECIAL CHARACTERS\n      \"display_name\": \"My Layer\", // THE NAME TO APPEAR IN THE UI\n      \"filepath_key\": \"original_input_image\", // DO NOT CHANGE\n      \"file_suffix\": \"\", // DO NOT CHANGE\n      \"z_index\": 1, // Z INDEX OF THE CURRENT LAYER ON THE UI MAP (HIGHEST NUMBER WILL APPEAR ON TOP OF ALL OTHER LAYERS)\n\t  \"visible_by_default\": \"True\", // WHETHER THE LAYER WILL BE SHOWN BY DEFAULT WHEN LOADED IN THE UI\n      \"coverage_name\": \"\", // ONLY USED FOR NETCDF TO POINT TO A PROPERTY OF INTEREST IN THE NETCDF\n      \"geoserver_style\": {\n        \"regression\": [ // RASTER REGRESSION STYLING\n          {\n            \"opacity\": 1,\n            \"quantity\": \"0\",\n            \"color\": \"#000dff\",\n            \"label\": \"Min\"\n          },\n          {\n            \"opacity\": 1,\n            \"quantity\": \"300\",\n            \"color\": \"#ff00d9\",\n            \"label\": \"MAX\"\n          }\n        ],\n        \"segmentation\": [ // RASTER SEGMENTATION STYLING\n          {\n            \"opacity\": 0,\n            \"quantity\": \"0\",\n            \"color\": \"#000dff\",\n            \"label\": \"No flood\"\n          },\n          {\n            \"opacity\": 1,\n            \"quantity\": \"1\",\n            \"color\": \"#ff00d9\",\n            \"label\": \"Flood\"\n          }\n        ],\n         \"rgb\": [ // RASTER RGB STYLING\n          {\n            \"minValue\": 0,\n            \"maxValue\": 255,\n            \"channel\": 1,\n            \"label\": \"RedChannel\"\n          },\n          {\n            \"minValue\": 0,\n            \"maxValue\": 255,\n            \"channel\": 2,\n            \"label\": \"GreenChannel\"\n          },\n          {\n            \"minValue\": 0,\n            \"maxValue\": 255,\n            \"channel\": 3,\n            \"label\": \"BlueChannel\"\n          }\n        ],\n        \"polygon_style\": { // VECTOR POLYGON STYLING EXAMPLE\n          \"fill\": \"#fffcfd\",\n          \"fill_opacity\": 0.5\n        },\n        \"point_style\": { // VECTOR POINT STYLING EXAMPLE\n          \"well_known_name\": \"circle\",\n          \"fill\": \"#b3b3b3\",\n          \"stroke\": \"#253c99\",\n          \"stroke_width\": 1,\n          \"size\": 6\n        },\n        \"line_style\": { // VECTOR LINE STYLING EXAMPLE\n          \"stroke\": \"#b0d6ff\",\n          \"stroke_width\": 1\n        }\n      }\n    }\n  ],\n  \"model_display_name\": \"add-layer-sandbox-model\", // DO NOT CHANGE\n  \"description\": \"Descriptions for my layers\", // TEXTUAL DESCRIPTION OF THE ONBOARDED LAYERS\n  \"location\": \"Layers location\", // LOCATION NAME FOR THE LAYERS\n  \"demo\": {\n    \"demo\": true, // DO NOT CHANGE\n    \"section_name\": \"My Examples\" // DO NOT CHANGE\n  }\n}\n</pre> <p>You can find example jsons for GeoTIFF, NetCDF, and Vector to guide you.</p> <p>You can then submit the request using:</p> <pre>response = gfm_client.submit_inference(\n    data=request_payload,\n    output=\"json\"\n)\n</pre> <p>The SDK also includes some widgets which can help you to browse the available models, define bounding boxes etc.</p>"},{"location":"examples/inference/002-Add-Precomputed-Examples/#monitor-tuning-status-and-progress","title":"Monitor tuning status and progress\u00b6","text":"<p>After submitting the request, we can poll the inference service to check the progress as well as get the output details once its complete (this could take a few minutes depending on the request size and the current service load).</p>"},{"location":"examples/inference/002-Add-Precomputed-Examples/#accessing-inference-outputs","title":"Accessing inference outputs\u00b6","text":"<p>Once an inference run is completed, the inputs and outputs of each task within an inference are packaged up into a zip file which is uploaded to a url you can use to download the files.</p> <p>To access the inference task files:</p> <ol> <li>Get the inference tasks list</li> <li>Identify the specific inference task you want to view</li> <li>Download task output files</li> </ol>"},{"location":"examples/inference/003-Running-inferences-with-extra-pipeline-steps/","title":"003-Running-inferences-with-extra-pipeline-steps","text":"In\u00a0[1]: Copied! <pre># Install extra requirements\n! pip install boto3\n</pre> # Install extra requirements ! pip install boto3 <pre>Requirement already satisfied: boto3 in /Users/catherinewanjiru/projects/climate/geospatial-studio-toolkit/.toolkit/lib/python3.12/site-packages (1.40.21)\nRequirement already satisfied: botocore&lt;1.41.0,&gt;=1.40.21 in /Users/catherinewanjiru/projects/climate/geospatial-studio-toolkit/.toolkit/lib/python3.12/site-packages (from boto3) (1.40.40)\nRequirement already satisfied: jmespath&lt;2.0.0,&gt;=0.7.1 in /Users/catherinewanjiru/projects/climate/geospatial-studio-toolkit/.toolkit/lib/python3.12/site-packages (from boto3) (1.0.1)\nRequirement already satisfied: s3transfer&lt;0.14.0,&gt;=0.13.0 in /Users/catherinewanjiru/projects/climate/geospatial-studio-toolkit/.toolkit/lib/python3.12/site-packages (from boto3) (0.13.1)\nRequirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /Users/catherinewanjiru/projects/climate/geospatial-studio-toolkit/.toolkit/lib/python3.12/site-packages (from botocore&lt;1.41.0,&gt;=1.40.21-&gt;boto3) (2.9.0.post0)\nRequirement already satisfied: urllib3!=2.2.0,&lt;3,&gt;=1.25.4 in /Users/catherinewanjiru/projects/climate/geospatial-studio-toolkit/.toolkit/lib/python3.12/site-packages (from botocore&lt;1.41.0,&gt;=1.40.21-&gt;boto3) (2.5.0)\nRequirement already satisfied: six&gt;=1.5 in /Users/catherinewanjiru/projects/climate/geospatial-studio-toolkit/.toolkit/lib/python3.12/site-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.41.0,&gt;=1.40.21-&gt;boto3) (1.17.0)\n\n[notice] A new release of pip is available: 24.3.1 -&gt; 25.3\n[notice] To update, run: pip install --upgrade pip\n</pre> In\u00a0[2]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre># import the required packages\n\nimport urllib3\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\nfrom geostudio import Client\nfrom geostudio import gswidgets\n</pre> # import the required packages  import urllib3 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  from geostudio import Client from geostudio import gswidgets In\u00a0[3]: Copied! <pre>#############################################################\n# Initialize Geostudio client using a geostudio config file\n#############################################################\ngfm_client = Client(geostudio_config_file=\".geostudio_config_file\")\n</pre> ############################################################# # Initialize Geostudio client using a geostudio config file ############################################################# gfm_client = Client(geostudio_config_file=\".geostudio_config_file\")  <pre>Using api key and base urls from geostudio config file\nUsing api key and base urls from geostudio config file\nUsing api key and base urls from geostudio config file\n</pre> <p>This notebook is used to run inference with extra custom post-processing steps to be applied after model prediction.</p> In\u00a0[5]: Copied! <pre># Replace with your tune_id\n\ntune_id = \"geotune-4kyeuubgnuk8kjdlnjdrsl\"\n</pre> # Replace with your tune_id  tune_id = \"geotune-4kyeuubgnuk8kjdlnjdrsl\" In\u00a0[6]: Copied! <pre># define payload\nrequest_prebuilt_steps_payload = {\n    \"description\": \"prebuilt steps inference\",\n    \"location\": \"austin\",\n    \"fine_tuning_id\": tune_id,  # replace this with the ID of your tune\n    \"model_display_name\": \"geofm-sandbox-models\",  # Keep as is\n    \"spatial_domain\": {  # provide downloadable link to the file(s) to run inference on.\n        \"urls\": [\n            \"https://ibm.box.com/shared/static/kpvuc75q0j29rvf3w2qiamgczuvuymmd.tif\"\n        ]\n    },\n    \"temporal_domain\": [\"2025-11-24\"],\n    # Provide this\n    \"geoserver_push\": [\n        # Keep these 2 layers as is.\n        {\n            \"z_index\": 0,\n            \"workspace\": \"geofm\",\n            \"layer_name\": \"input_rgb\",\n            \"file_suffix\": \"\",\n            \"display_name\": \"Input image (RGB)\",\n            \"filepath_key\": \"model_input_original_image_rgb\",\n            \"geoserver_style\": {\n                \"rgb\": [\n                    {\n                        \"label\": \"RedChannel\",\n                        \"channel\": 1,\n                        \"maxValue\": 255,\n                        \"minValue\": 0,\n                    },\n                    {\n                        \"label\": \"GreenChannel\",\n                        \"channel\": 2,\n                        \"maxValue\": 255,\n                        \"minValue\": 0,\n                    },\n                    {\n                        \"label\": \"BlueChannel\",\n                        \"channel\": 3,\n                        \"maxValue\": 255,\n                        \"minValue\": 0,\n                    },\n                ]\n            },\n            \"visible_by_default\": \"True\",\n        },\n        {\n            \"z_index\": 1,\n            \"workspace\": \"geofm\",\n            \"layer_name\": \"pred\",\n            \"file_suffix\": \"\",\n            \"display_name\": \"Model prediction\",\n            \"filepath_key\": \"model_output_image\",\n            \"geoserver_style\": {\n                \"segmentation\": [\n                    {\n                        \"color\": \"#7d7247\",\n                        \"label\": \"no-buildings\",\n                        \"opacity\": 0,\n                        \"quantity\": \"0\",\n                    },\n                    {\n                        \"color\": \"#390c8c\",\n                        \"label\": \"buildings\",\n                        \"opacity\": 1,\n                        \"quantity\": \"1\",\n                    },\n                ]\n            },\n            \"visible_by_default\": \"True\",\n        },\n        # Add all layers to push created by your step(s).\n        {\n            \"z_index\": 1,\n            \"workspace\": \"geofm\",\n            \"layer_name\": \"im2poly\",  # Name of the registered step\n            \"file_suffix\": \"gpkg\",  # Suffix of the generated file\n            \"display_name\": \"im2poly_regularize_image\",  # Display name of the Step\n            \"filepath_key\": \"model_output_im2poly_regularize_image\",  # Should be &lt;model_output_{step_registered_name}_image&gt;\n            \"geoserver_style\": {},  # for shape files, keep this empty\n            \"visible_by_default\": \"True\",\n        },\n    ],\n    \"post_processing\": {\n        \"cloud_masking\": \"False\",\n        \"regularization\": \"False\",\n        # Defining the extra post-processing steps\n        \"regularization_custom\": [\n            {\n                \"name\": \"im2poly_regularize\",  # step_registered_name\n                \"params\": {  # params of the function\n                    \"geoserver_suffix_extension\": \"gpkg\",  # Expected\n                    \"params\": {\n                        \"simplify_tolerance\": 2,\n                        \"parallel_threshold\": 2.0,\n                        \"allow_45_degree\": \"True\",\n                    },\n                },\n            }\n        ],\n    },\n}\n</pre> # define payload request_prebuilt_steps_payload = {     \"description\": \"prebuilt steps inference\",     \"location\": \"austin\",     \"fine_tuning_id\": tune_id,  # replace this with the ID of your tune     \"model_display_name\": \"geofm-sandbox-models\",  # Keep as is     \"spatial_domain\": {  # provide downloadable link to the file(s) to run inference on.         \"urls\": [             \"https://ibm.box.com/shared/static/kpvuc75q0j29rvf3w2qiamgczuvuymmd.tif\"         ]     },     \"temporal_domain\": [\"2025-11-24\"],     # Provide this     \"geoserver_push\": [         # Keep these 2 layers as is.         {             \"z_index\": 0,             \"workspace\": \"geofm\",             \"layer_name\": \"input_rgb\",             \"file_suffix\": \"\",             \"display_name\": \"Input image (RGB)\",             \"filepath_key\": \"model_input_original_image_rgb\",             \"geoserver_style\": {                 \"rgb\": [                     {                         \"label\": \"RedChannel\",                         \"channel\": 1,                         \"maxValue\": 255,                         \"minValue\": 0,                     },                     {                         \"label\": \"GreenChannel\",                         \"channel\": 2,                         \"maxValue\": 255,                         \"minValue\": 0,                     },                     {                         \"label\": \"BlueChannel\",                         \"channel\": 3,                         \"maxValue\": 255,                         \"minValue\": 0,                     },                 ]             },             \"visible_by_default\": \"True\",         },         {             \"z_index\": 1,             \"workspace\": \"geofm\",             \"layer_name\": \"pred\",             \"file_suffix\": \"\",             \"display_name\": \"Model prediction\",             \"filepath_key\": \"model_output_image\",             \"geoserver_style\": {                 \"segmentation\": [                     {                         \"color\": \"#7d7247\",                         \"label\": \"no-buildings\",                         \"opacity\": 0,                         \"quantity\": \"0\",                     },                     {                         \"color\": \"#390c8c\",                         \"label\": \"buildings\",                         \"opacity\": 1,                         \"quantity\": \"1\",                     },                 ]             },             \"visible_by_default\": \"True\",         },         # Add all layers to push created by your step(s).         {             \"z_index\": 1,             \"workspace\": \"geofm\",             \"layer_name\": \"im2poly\",  # Name of the registered step             \"file_suffix\": \"gpkg\",  # Suffix of the generated file             \"display_name\": \"im2poly_regularize_image\",  # Display name of the Step             \"filepath_key\": \"model_output_im2poly_regularize_image\",  # Should be              \"geoserver_style\": {},  # for shape files, keep this empty             \"visible_by_default\": \"True\",         },     ],     \"post_processing\": {         \"cloud_masking\": \"False\",         \"regularization\": \"False\",         # Defining the extra post-processing steps         \"regularization_custom\": [             {                 \"name\": \"im2poly_regularize\",  # step_registered_name                 \"params\": {  # params of the function                     \"geoserver_suffix_extension\": \"gpkg\",  # Expected                     \"params\": {                         \"simplify_tolerance\": 2,                         \"parallel_threshold\": 2.0,                         \"allow_45_degree\": \"True\",                     },                 },             }         ],     }, } In\u00a0[7]: Copied! <pre>inference_response = gfm_client.try_out_tune(tune_id=tune_id, data=request_prebuilt_steps_payload)\ninference_response\n</pre> inference_response = gfm_client.try_out_tune(tune_id=tune_id, data=request_prebuilt_steps_payload) inference_response Out[7]: <pre>{'spatial_domain': {'bbox': [],\n  'polygons': [],\n  'tiles': [],\n  'urls': ['https://ibm.box.com/shared/static/kpvuc75q0j29rvf3w2qiamgczuvuymmd.tif']},\n 'temporal_domain': ['2025-11-24'],\n 'fine_tuning_id': 'geotune-4kyeuubgnuk8kjdlnjdrsl',\n 'maxcc': 100,\n 'model_display_name': 'geofm-sandbox-models',\n 'description': 'prebuilt steps inference',\n 'location': 'austin',\n 'geoserver_layers': None,\n 'demo': None,\n 'model_id': '6eeed629-5206-4c1a-8188-58ad754fd235',\n 'inference_output': None,\n 'id': '84e8b661-d568-4314-8b31-1827352215c0',\n 'active': True,\n 'created_by': 'Catherine.Wanjiru@ibm.com',\n 'created_at': '2025-12-04T09:29:17.137150Z',\n 'updated_at': '2025-12-04T09:29:17.457513Z',\n 'status': 'PENDING',\n 'tasks_count_total': 1,\n 'tasks_count_success': 0,\n 'tasks_count_failed': 0,\n 'tasks_count_stopped': 0,\n 'tasks_count_waiting': 1}</pre> In\u00a0[8]: Copied! <pre># define payload\nrequest_download_scripts_payload = {\n    \"description\": \"download scripts\",\n    \"location\": \"austin\",\n    \"fine_tuning_id\": tune_id,  # replace this with the ID of your tune\n    \"model_display_name\": \"geofm-sandbox-models\",  # Keep as is\n    \"spatial_domain\": {  # provide downloadable link to the file(s) to run inference on.\n        \"urls\": [\n            \"https://ibm.box.com/shared/static/kpvuc75q0j29rvf3w2qiamgczuvuymmd.tif\"\n        ]\n    },\n    \"temporal_domain\": [\"2025-11-24\"],\n    # Provide this\n    \"geoserver_push\": [\n        # Keep these 2 layers as is.\n        {\n            \"z_index\": 0,\n            \"workspace\": \"geofm\",\n            \"layer_name\": \"input_rgb\",\n            \"file_suffix\": \"\",\n            \"display_name\": \"Input image (RGB)\",\n            \"filepath_key\": \"model_input_original_image_rgb\",\n            \"geoserver_style\": {\n                \"rgb\": [\n                    {\n                        \"label\": \"RedChannel\",\n                        \"channel\": 1,\n                        \"maxValue\": 255,\n                        \"minValue\": 0,\n                    },\n                    {\n                        \"label\": \"GreenChannel\",\n                        \"channel\": 2,\n                        \"maxValue\": 255,\n                        \"minValue\": 0,\n                    },\n                    {\n                        \"label\": \"BlueChannel\",\n                        \"channel\": 3,\n                        \"maxValue\": 255,\n                        \"minValue\": 0,\n                    },\n                ]\n            },\n            \"visible_by_default\": \"True\",\n        },\n        {\n            \"z_index\": 1,\n            \"workspace\": \"geofm\",\n            \"layer_name\": \"pred\",\n            \"file_suffix\": \"\",\n            \"display_name\": \"Model prediction\",\n            \"filepath_key\": \"model_output_image\",\n            \"geoserver_style\": {\n                \"segmentation\": [\n                    {\n                        \"color\": \"#7d7247\",\n                        \"label\": \"no-buildings\",\n                        \"opacity\": 0,\n                        \"quantity\": \"0\",\n                    },\n                    {\n                        \"color\": \"#390c8c\",\n                        \"label\": \"buildings\",\n                        \"opacity\": 1,\n                        \"quantity\": \"1\",\n                    },\n                ]\n            },\n            \"visible_by_default\": \"True\",\n        },\n        # Add all layers to push created by your step(s).\n        {\n            \"z_index\": 1,\n            \"workspace\": \"geofm\",\n            \"layer_name\": \"im2poly\",  # Name of the registered step\n            \"file_suffix\": \"gpkg\",  # Suffix of the generated file\n            \"display_name\": \"im2poly_regularize_image\",  # Display name of the Step\n            \"filepath_key\": \"model_output_im2poly_regularize_image\",  # Should be &lt;model_output_{step_registered_name}_image&gt;\n            \"geoserver_style\": {},  # for shape files, keep this empty\n            \"visible_by_default\": \"True\",\n        },\n    ],\n    \"post_processing\": {\n        \"cloud_masking\": \"False\",\n        \"regularization\": \"False\",\n        \"download_scripts\": {\n            \"activated\": \"True\",\n            \"verify_ssl\": \"False\",\n            \"plugins_list\": [\n                {\n                    \"url\": \"https://s3.us-east.cloud-object-storage.appdomain.cloud/geospatial-studio-example-data/example_post_processing_scripts/user_masking_testing.py?response-content-type=application%2Foctet-stream&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=1c6317aac0054b9890797f09f217b54e%2F20251203%2Fus-east%2Fs3%2Faws4_request&amp;X-Amz-Date=20251203T103901Z&amp;X-Amz-Expires=259200&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=ca85cef43562cf6bc88d20a8f7463a311a0cdde8d53288ef552acc38b86a3849\",\n                    \"filename\": \"user_masking_testing.py\",\n                },\n                {\n                    \"url\": \"https://s3.us-east.cloud-object-storage.appdomain.cloud/geospatial-studio-example-data/example_post_processing_scripts/cloud_masking_testing.py?response-content-type=application%2Foctet-stream&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=1c6317aac0054b9890797f09f217b54e%2F20251203%2Fus-east%2Fs3%2Faws4_request&amp;X-Amz-Date=20251203T104802Z&amp;X-Amz-Expires=259200&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=4f702cb5424c1a8438acc5a0865142e95c58240ca9d122cc322016d1878ba1b8\",\n                    \"filename\": \"cloud_masking_testing.py\",\n                },\n            ],\n        },\n        # Defining the extra post-processing steps\n        \"regularization_custom\": [\n            {\n                \"name\": \"im2poly_regularize\",  # step_registered_name\n                \"params\": {  # params of the function\n                    \"geoserver_suffix_extension\": \"gpkg\",  # Expected\n                    \"params\": {\n                        \"simplify_tolerance\": 2,\n                        \"parallel_threshold\": 2.0,\n                        \"allow_45_degree\": \"True\",\n                    },\n                },\n            }\n        ],\n    },\n}\n</pre> # define payload request_download_scripts_payload = {     \"description\": \"download scripts\",     \"location\": \"austin\",     \"fine_tuning_id\": tune_id,  # replace this with the ID of your tune     \"model_display_name\": \"geofm-sandbox-models\",  # Keep as is     \"spatial_domain\": {  # provide downloadable link to the file(s) to run inference on.         \"urls\": [             \"https://ibm.box.com/shared/static/kpvuc75q0j29rvf3w2qiamgczuvuymmd.tif\"         ]     },     \"temporal_domain\": [\"2025-11-24\"],     # Provide this     \"geoserver_push\": [         # Keep these 2 layers as is.         {             \"z_index\": 0,             \"workspace\": \"geofm\",             \"layer_name\": \"input_rgb\",             \"file_suffix\": \"\",             \"display_name\": \"Input image (RGB)\",             \"filepath_key\": \"model_input_original_image_rgb\",             \"geoserver_style\": {                 \"rgb\": [                     {                         \"label\": \"RedChannel\",                         \"channel\": 1,                         \"maxValue\": 255,                         \"minValue\": 0,                     },                     {                         \"label\": \"GreenChannel\",                         \"channel\": 2,                         \"maxValue\": 255,                         \"minValue\": 0,                     },                     {                         \"label\": \"BlueChannel\",                         \"channel\": 3,                         \"maxValue\": 255,                         \"minValue\": 0,                     },                 ]             },             \"visible_by_default\": \"True\",         },         {             \"z_index\": 1,             \"workspace\": \"geofm\",             \"layer_name\": \"pred\",             \"file_suffix\": \"\",             \"display_name\": \"Model prediction\",             \"filepath_key\": \"model_output_image\",             \"geoserver_style\": {                 \"segmentation\": [                     {                         \"color\": \"#7d7247\",                         \"label\": \"no-buildings\",                         \"opacity\": 0,                         \"quantity\": \"0\",                     },                     {                         \"color\": \"#390c8c\",                         \"label\": \"buildings\",                         \"opacity\": 1,                         \"quantity\": \"1\",                     },                 ]             },             \"visible_by_default\": \"True\",         },         # Add all layers to push created by your step(s).         {             \"z_index\": 1,             \"workspace\": \"geofm\",             \"layer_name\": \"im2poly\",  # Name of the registered step             \"file_suffix\": \"gpkg\",  # Suffix of the generated file             \"display_name\": \"im2poly_regularize_image\",  # Display name of the Step             \"filepath_key\": \"model_output_im2poly_regularize_image\",  # Should be              \"geoserver_style\": {},  # for shape files, keep this empty             \"visible_by_default\": \"True\",         },     ],     \"post_processing\": {         \"cloud_masking\": \"False\",         \"regularization\": \"False\",         \"download_scripts\": {             \"activated\": \"True\",             \"verify_ssl\": \"False\",             \"plugins_list\": [                 {                     \"url\": \"https://s3.us-east.cloud-object-storage.appdomain.cloud/geospatial-studio-example-data/example_post_processing_scripts/user_masking_testing.py?response-content-type=application%2Foctet-stream&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=1c6317aac0054b9890797f09f217b54e%2F20251203%2Fus-east%2Fs3%2Faws4_request&amp;X-Amz-Date=20251203T103901Z&amp;X-Amz-Expires=259200&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=ca85cef43562cf6bc88d20a8f7463a311a0cdde8d53288ef552acc38b86a3849\",                     \"filename\": \"user_masking_testing.py\",                 },                 {                     \"url\": \"https://s3.us-east.cloud-object-storage.appdomain.cloud/geospatial-studio-example-data/example_post_processing_scripts/cloud_masking_testing.py?response-content-type=application%2Foctet-stream&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=1c6317aac0054b9890797f09f217b54e%2F20251203%2Fus-east%2Fs3%2Faws4_request&amp;X-Amz-Date=20251203T104802Z&amp;X-Amz-Expires=259200&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=4f702cb5424c1a8438acc5a0865142e95c58240ca9d122cc322016d1878ba1b8\",                     \"filename\": \"cloud_masking_testing.py\",                 },             ],         },         # Defining the extra post-processing steps         \"regularization_custom\": [             {                 \"name\": \"im2poly_regularize\",  # step_registered_name                 \"params\": {  # params of the function                     \"geoserver_suffix_extension\": \"gpkg\",  # Expected                     \"params\": {                         \"simplify_tolerance\": 2,                         \"parallel_threshold\": 2.0,                         \"allow_45_degree\": \"True\",                     },                 },             }         ],     }, } In\u00a0[9]: Copied! <pre>inference_response = gfm_client.try_out_tune(tune_id=tune_id, data=request_download_scripts_payload)\ninference_response\n</pre> inference_response = gfm_client.try_out_tune(tune_id=tune_id, data=request_download_scripts_payload) inference_response  Out[9]: <pre>{'spatial_domain': {'bbox': [],\n  'polygons': [],\n  'tiles': [],\n  'urls': ['https://ibm.box.com/shared/static/kpvuc75q0j29rvf3w2qiamgczuvuymmd.tif']},\n 'temporal_domain': ['2025-11-24'],\n 'fine_tuning_id': 'geotune-4kyeuubgnuk8kjdlnjdrsl',\n 'maxcc': 100,\n 'model_display_name': 'geofm-sandbox-models',\n 'description': 'download scripts',\n 'location': 'austin',\n 'geoserver_layers': None,\n 'demo': None,\n 'model_id': '6eeed629-5206-4c1a-8188-58ad754fd235',\n 'inference_output': None,\n 'id': 'ee82608d-39cf-4101-b39f-653579b1b314',\n 'active': True,\n 'created_by': 'Catherine.Wanjiru@ibm.com',\n 'created_at': '2025-12-04T09:29:19.158507Z',\n 'updated_at': '2025-12-04T09:29:19.495407Z',\n 'status': 'PENDING',\n 'tasks_count_total': 1,\n 'tasks_count_success': 0,\n 'tasks_count_failed': 0,\n 'tasks_count_stopped': 0,\n 'tasks_count_waiting': 1}</pre> In\u00a0[10]: Copied! <pre># Poll inference status\nr = gfm_client.poll_inference_until_finished(inference_id=inference_response['id'])\n</pre> # Poll inference status r = gfm_client.poll_inference_until_finished(inference_id=inference_response['id'])  <pre>COMPLETED - 289 seconds\n</pre> In\u00a0[11]: Copied! <pre>response = gfm_client.get_inference(inference_id=inference_response['id'])\nresponse\n</pre> response = gfm_client.get_inference(inference_id=inference_response['id']) response Out[11]: <pre>{'spatial_domain': {'bbox': [],\n  'polygons': [],\n  'tiles': [],\n  'urls': ['https://ibm.box.com/shared/static/kpvuc75q0j29rvf3w2qiamgczuvuymmd.tif']},\n 'temporal_domain': ['2025-11-24'],\n 'fine_tuning_id': 'geotune-4kyeuubgnuk8kjdlnjdrsl',\n 'maxcc': 100,\n 'model_display_name': 'geofm-sandbox-models',\n 'description': 'download scripts',\n 'location': 'austin',\n 'geoserver_layers': {'predicted_layers': [{'uri': 'geofm:ee82608d-39cf-4101-b39f-653579b1b314-input_rgb',\n    'display_name': 'Input image (RGB)',\n    'sld_body': '&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n    &lt;StyledLayerDescriptor xmlns=\"http://www.opengis.net/sld\" xmlns:ogc=\"http://www.opengis.net/ogc\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.opengis.net/sld\\n    http://schemas.opengis.net/sld/1.0.0/StyledLayerDescriptor.xsd\" version=\"1.0.0\"&gt;\\n        &lt;NamedLayer&gt;\\n            &lt;Name&gt;geofm:ee82608d-39cf-4101-b39f-653579b1b314-input_rgb&lt;/Name&gt;\\n            &lt;UserStyle&gt;\\n                &lt;Title&gt;geofm:ee82608d-39cf-4101-b39f-653579b1b314-input_rgb&lt;/Title&gt;\\n                &lt;FeatureTypeStyle&gt;\\n                    &lt;Rule&gt;\\n                        &lt;RasterSymbolizer&gt;\\n                            &lt;ChannelSelection&gt;\\n                                &lt;RedChannel&gt;\\n        &lt;SourceChannelName&gt;1&lt;/SourceChannelName&gt;\\n        &lt;ContrastEnhancement&gt;\\n        &lt;Normalize&gt;\\n            &lt;VendorOption name=\"algorithm\"&gt;StretchToMinimumMaximum&lt;/VendorOption&gt;\\n            &lt;VendorOption name=\"minValue\"&gt;0&lt;/VendorOption&gt;\\n            &lt;VendorOption name=\"maxValue\"&gt;255&lt;/VendorOption&gt;\\n        &lt;/Normalize&gt;\\n        &lt;/ContrastEnhancement&gt;\\n    &lt;/RedChannel&gt;&lt;GreenChannel&gt;\\n        &lt;SourceChannelName&gt;2&lt;/SourceChannelName&gt;\\n        &lt;ContrastEnhancement&gt;\\n        &lt;Normalize&gt;\\n            &lt;VendorOption name=\"algorithm\"&gt;StretchToMinimumMaximum&lt;/VendorOption&gt;\\n            &lt;VendorOption name=\"minValue\"&gt;0&lt;/VendorOption&gt;\\n            &lt;VendorOption name=\"maxValue\"&gt;255&lt;/VendorOption&gt;\\n        &lt;/Normalize&gt;\\n        &lt;/ContrastEnhancement&gt;\\n    &lt;/GreenChannel&gt;&lt;BlueChannel&gt;\\n        &lt;SourceChannelName&gt;3&lt;/SourceChannelName&gt;\\n        &lt;ContrastEnhancement&gt;\\n        &lt;Normalize&gt;\\n            &lt;VendorOption name=\"algorithm\"&gt;StretchToMinimumMaximum&lt;/VendorOption&gt;\\n            &lt;VendorOption name=\"minValue\"&gt;0&lt;/VendorOption&gt;\\n            &lt;VendorOption name=\"maxValue\"&gt;255&lt;/VendorOption&gt;\\n        &lt;/Normalize&gt;\\n        &lt;/ContrastEnhancement&gt;\\n    &lt;/BlueChannel&gt;            \\n                            &lt;/ChannelSelection&gt;\\n                        &lt;/RasterSymbolizer&gt;\\n                    &lt;/Rule&gt;\\n                &lt;/FeatureTypeStyle&gt;\\n            &lt;/UserStyle&gt;\\n        &lt;/NamedLayer&gt;\\n    &lt;/StyledLayerDescriptor&gt;\\n    ',\n    'z_index': 0,\n    'visible_by_default': 'True'},\n   {'uri': 'geofm:ee82608d-39cf-4101-b39f-653579b1b314-pred',\n    'display_name': 'Model prediction',\n    'sld_body': '&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n    &lt;StyledLayerDescriptor xmlns=\"http://www.opengis.net/sld\" xmlns:ogc=\"http://www.opengis.net/ogc\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.opengis.net/sld\\n    http://schemas.opengis.net/sld/1.0.0/StyledLayerDescriptor.xsd\" version=\"1.0.0\"&gt;\\n    &lt;NamedLayer&gt;\\n        &lt;Name&gt;geofm:ee82608d-39cf-4101-b39f-653579b1b314-pred&lt;/Name&gt;\\n        &lt;UserStyle&gt;\\n        &lt;Title&gt;geofm:ee82608d-39cf-4101-b39f-653579b1b314-pred&lt;/Title&gt;\\n        &lt;IsDefault&gt;1&lt;/IsDefault&gt;\\n        &lt;FeatureTypeStyle&gt;\\n            &lt;Rule&gt;\\n            &lt;RasterSymbolizer&gt;\\n                &lt;Opacity&gt;1.0&lt;/Opacity&gt;\\n                &lt;ColorMap&gt;\\n                    &lt;ColorMapEntry color=\"#7d7247\"  opacity=\"0\" quantity=\"0\" label=\"no-buildings\" /&gt;&lt;ColorMapEntry color=\"#390c8c\"  opacity=\"1\" quantity=\"1\" label=\"buildings\" /&gt;\\n                &lt;/ColorMap&gt;\\n            &lt;/RasterSymbolizer&gt;\\n            &lt;/Rule&gt;\\n        &lt;/FeatureTypeStyle&gt;\\n        &lt;/UserStyle&gt;\\n    &lt;/NamedLayer&gt;\\n    &lt;/StyledLayerDescriptor&gt;',\n    'z_index': 1,\n    'visible_by_default': 'True'},\n   {'uri': 'geofm:ee82608d-39cf-4101-b39f-653579b1b314-im2poly',\n    'display_name': 'im2poly_regularize_image',\n    'sld_body': None,\n    'z_index': 1,\n    'visible_by_default': 'True'}],\n  'bbox_pred': [[16.302191847671576,\n    48.224583386883076,\n    16.322389984936816,\n    48.2380774323371],\n   [16.302188413198312,\n    48.224583386883076,\n    16.32239341941008,\n    48.2380774323371],\n   [16.30217764259808,\n    48.22456748526141,\n    16.32240522602398,\n    48.23809283932311]]},\n 'demo': None,\n 'model_id': '6eeed629-5206-4c1a-8188-58ad754fd235',\n 'inference_output': {'output_url': 'None'},\n 'id': 'ee82608d-39cf-4101-b39f-653579b1b314',\n 'active': True,\n 'created_by': 'Catherine.Wanjiru@ibm.com',\n 'created_at': '2025-12-04T09:29:19.158507Z',\n 'updated_at': '2025-12-04T09:34:08.441348Z',\n 'status': 'COMPLETED',\n 'tasks_count_total': 5,\n 'tasks_count_success': 5,\n 'tasks_count_failed': 0,\n 'tasks_count_stopped': 0,\n 'tasks_count_waiting': 0}</pre> <p>You can view the finished inference in the UI now</p> In\u00a0[12]: Copied! <pre># Get the inference tasks list\ninf_tasks_res = gfm_client.get_inference_tasks(response[\"id\"])\ninf_tasks_res\n</pre> # Get the inference tasks list inf_tasks_res = gfm_client.get_inference_tasks(response[\"id\"]) inf_tasks_res Out[12]: <pre>{'inference_id': 'ee82608d-39cf-4101-b39f-653579b1b314',\n 'status': 'COMPLETED',\n 'tasks': [{'inference_id': 'ee82608d-39cf-4101-b39f-653579b1b314',\n   'task_id': 'ee82608d-39cf-4101-b39f-653579b1b314-task_0',\n   'inference_folder': '/data/ee82608d-39cf-4101-b39f-653579b1b314',\n   'status': 'FINISHED',\n   'pipeline_steps': [{'status': 'FINISHED',\n     'end_time': '2025-12-04T09:31:18',\n     'process_id': 'url-connector',\n     'start_time': '2025-12-04T09:30:31',\n     'step_number': 0},\n    {'status': 'FINISHED',\n     'end_time': '2025-12-04T09:32:17',\n     'process_id': 'terratorch-inference',\n     'start_time': '2025-12-04T09:31:31',\n     'step_number': 1},\n    {'status': 'FINISHED',\n     'end_time': '2025-12-04T09:32:55',\n     'process_id': 'postprocess-generic',\n     'start_time': '2025-12-04T09:32:20',\n     'step_number': 2},\n    {'status': 'FINISHED',\n     'end_time': '2025-12-04T09:34:07',\n     'process_id': 'push-to-geoserver',\n     'start_time': '2025-12-04T09:33:00',\n     'step_number': 3}]},\n  {'inference_id': 'ee82608d-39cf-4101-b39f-653579b1b314',\n   'task_id': 'ee82608d-39cf-4101-b39f-653579b1b314-task_planning',\n   'inference_folder': '/data/ee82608d-39cf-4101-b39f-653579b1b314',\n   'status': 'FINISHED',\n   'pipeline_steps': [{'status': 'FINISHED',\n     'end_time': '2025-12-04T09:29:48',\n     'process_id': 'inference-planner',\n     'start_time': '2025-12-04T09:29:41',\n     'step_number': 0}]}]}</pre> In\u00a0[13]: Copied! <pre>df = gfm_client.inference_task_status_df(response[\"id\"])\n\n\ndisplay(df.style.map(gswidgets.color_inference_tasks_by_status))\n</pre> df = gfm_client.inference_task_status_df(response[\"id\"])   display(df.style.map(gswidgets.color_inference_tasks_by_status)) <pre>0\n</pre> <pre>/Users/catherinewanjiru/projects/climate/geospatial-studio-toolkit/geospatial-studio-sdk/geostudio/backends/v2/ginference/client.py:262: FutureWarning:\n\nChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n</pre> task_id terratorch-inference url-connector postprocess-generic push-to-geoserver 0 ee82608d-39cf-4101-b39f-653579b1b314-task_0 FINISHED FINISHED FINISHED FINISHED In\u00a0[14]: Copied! <pre>gswidgets.view_inference_process_timeline(gfm_client, inference_id = response[\"id\"])\n</pre> gswidgets.view_inference_process_timeline(gfm_client, inference_id = response[\"id\"]) In\u00a0[15]: Copied! <pre># Get the inference tasks list\n\ninf_tasks_res = gfm_client.get_inference_tasks(response[\"id\"])\ninf_tasks_res\n</pre> # Get the inference tasks list  inf_tasks_res = gfm_client.get_inference_tasks(response[\"id\"]) inf_tasks_res Out[15]: <pre>{'inference_id': 'ee82608d-39cf-4101-b39f-653579b1b314',\n 'status': 'COMPLETED',\n 'tasks': [{'inference_id': 'ee82608d-39cf-4101-b39f-653579b1b314',\n   'task_id': 'ee82608d-39cf-4101-b39f-653579b1b314-task_0',\n   'inference_folder': '/data/ee82608d-39cf-4101-b39f-653579b1b314',\n   'status': 'FINISHED',\n   'pipeline_steps': [{'status': 'FINISHED',\n     'end_time': '2025-12-04T09:31:18',\n     'process_id': 'url-connector',\n     'start_time': '2025-12-04T09:30:31',\n     'step_number': 0},\n    {'status': 'FINISHED',\n     'end_time': '2025-12-04T09:32:17',\n     'process_id': 'terratorch-inference',\n     'start_time': '2025-12-04T09:31:31',\n     'step_number': 1},\n    {'status': 'FINISHED',\n     'end_time': '2025-12-04T09:32:55',\n     'process_id': 'postprocess-generic',\n     'start_time': '2025-12-04T09:32:20',\n     'step_number': 2},\n    {'status': 'FINISHED',\n     'end_time': '2025-12-04T09:34:07',\n     'process_id': 'push-to-geoserver',\n     'start_time': '2025-12-04T09:33:00',\n     'step_number': 3}]},\n  {'inference_id': 'ee82608d-39cf-4101-b39f-653579b1b314',\n   'task_id': 'ee82608d-39cf-4101-b39f-653579b1b314-task_planning',\n   'inference_folder': '/data/ee82608d-39cf-4101-b39f-653579b1b314',\n   'status': 'FINISHED',\n   'pipeline_steps': [{'status': 'FINISHED',\n     'end_time': '2025-12-04T09:29:48',\n     'process_id': 'inference-planner',\n     'start_time': '2025-12-04T09:29:41',\n     'step_number': 0}]}]}</pre> <p>Next, Identify the task you want to view from the response above, ensure status of the task is FINISHED and set <code>selected_task</code> variable below to the task number at the end of the task id string. For example, if <code>task_id</code> is \"6d1149fa-302d-4612-82dd-5879fc06081d-task_0\", selected_task would be 0</p> In\u00a0[16]: Copied! <pre># Select a task to view\n\nselected_task = 0 \nselected_task_id = f\"{inf_tasks_res['inference_id']}-task_{selected_task}\"\n</pre> # Select a task to view  selected_task = 0  selected_task_id = f\"{inf_tasks_res['inference_id']}-task_{selected_task}\" In\u00a0[17]: Copied! <pre>selected_task_id\n</pre> selected_task_id Out[17]: <pre>'ee82608d-39cf-4101-b39f-653579b1b314-task_0'</pre> In\u00a0[\u00a0]: Copied! <pre># Download task output files\n\ngswidgets.fileDownloaderTasks(client=gfm_client, task_id=selected_task_id,just_tifs=False)\n</pre> # Download task output files  gswidgets.fileDownloaderTasks(client=gfm_client, task_id=selected_task_id,just_tifs=False) <pre>Just tiffs: False\n</pre> <pre>HTML(value='&lt;h1&gt;Inference Task output downloader&lt;/h1&gt; &lt;/p&gt;Select the files and the download path and hit downl\u2026</pre> <pre>SelectMultiple(description='Files:', layout=Layout(width='1000px'), options=('ee82608d-39cf-4101-b39f-653579b1\u2026</pre> <pre>Text(value='./', description='Dl path:')</pre> <pre>Button(description='Download', icon='check', layout=Layout(height='auto', width='800px'), style=ButtonStyle(),\u2026</pre> <pre>Output()</pre>"},{"location":"examples/inference/003-Running-inferences-with-extra-pipeline-steps/#003-running-inferences-with-extra-pipeline-steps","title":"003-Running-inferences-with-extra-pipeline-steps\u00b6","text":"<p>\ud83d\udce5 Download 003-Running-inferences-with-extra-pipeline-steps and try it out</p>"},{"location":"examples/inference/003-Running-inferences-with-extra-pipeline-steps/#introduction","title":"Introduction\u00b6","text":"<p>This notebook is meant for someone with minimal knowledge, to be able to meaningfully use the most important functions of the Geospatial SDK.</p> <p>For more information about the Geospatial Studio see the docs page: Geospatial Studio Docs</p> <p>For more information about the Geospatial Studio SDK and all the functions available through it, see the SDK docs page: Geospatial Studio SDK Docs</p>"},{"location":"examples/inference/003-Running-inferences-with-extra-pipeline-steps/#connecting-to-the-platform","title":"Connecting to the platform\u00b6","text":"<p>First, we set up the connection to the platform backend.  To do this we need the base url for the studio UI and an API key.</p> <p>To get an API Key:</p> <ol> <li>Go to the Geospatial Studio UI page and navigate to the Manage your API keys link.</li> <li>This should pop-up a window where you can generate, access and delete your api keys. NB: every user is limited to a maximum of two activate api keys at any one time.</li> </ol> <p>Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:</p> <pre>echo \"GEOSTUDIO_API_KEY=&lt;paste_api_key_here&gt;\" &gt; .geostudio_config_file\necho \"BASE_STUDIO_UI_URL=&lt;paste_ui_base_url_here&gt;\" &gt;&gt; .geostudio_config_file\n</pre> <p>Copy and paste the file path to this credentials file in call below.</p>"},{"location":"examples/inference/003-Running-inferences-with-extra-pipeline-steps/#running-inference","title":"Running inference\u00b6","text":""},{"location":"examples/inference/003-Running-inferences-with-extra-pipeline-steps/#monitor-inference-status-and-progress","title":"Monitor inference status and progress\u00b6","text":"<p>After submitting the request, we can poll the inference service to check the progress as well as get the output details once its complete (this could take a few minutes depending on the request size and the current service load).</p>"},{"location":"examples/inference/003-Running-inferences-with-extra-pipeline-steps/#accessing-inference-outputs","title":"Accessing inference outputs\u00b6","text":"<p>Once an inference run is completed, the inputs and outputs of each task within an inference are packaged up into a zip file which is uploaded to a url you can use to download the files.</p> <p>To access the inference task files:</p> <ol> <li>Get the inference tasks list</li> <li>Identify the specific inference task you want to view</li> <li>Download task output files</li> </ol>"},{"location":"examples/inference/003-Running-inferences-with-extra-pipeline-steps/#accessing-inference-outputs","title":"Accessing inference outputs\u00b6","text":"<p>Once an inference run is completed, the inputs and outputs of each task within an inference are packaged up into a zip file which is uploaded to a url you can use to download the files.</p> <p>To access the inference task files:</p> <ol> <li>Get the inference tasks list</li> <li>Identify the specific inference task you want to view</li> <li>Download task output files</li> </ol>"}]}