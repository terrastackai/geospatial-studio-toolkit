{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fedb38d1",
   "metadata": {},
   "source": [
    "# FineTuning: HPO With Terratorch Iterate\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to use the FineTuning SDK to submit an HPO (Hyperparameter Optimization) job to the FineTuning service.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before proceeding with this notebook, ensure you have:\n",
    "\n",
    "- Active GeoStudio Service Access: Valid credentials and permissions for the GeoStudio inference service\n",
    "- SDK Installation: The GeoStudio SDK installed in your environment\n",
    "- Authentication Setup: API keys configured (either via environment variables or key files)\n",
    "- TerraTorch Iterate Config File: A prepared configuration file (.yaml) for running fine-tuning.\n",
    "\n",
    "\n",
    "> **Note:** This workflow assumes you have already prepared a configuration file for TerraTorch Iterate. If you need guidance on fine-tuning TerraTorch models with HPO enabled, refer to the TerraTorch-Iterate documentation first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2466d4",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee6ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geostudio import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28a8f0",
   "metadata": {},
   "source": [
    "## Connecting to Geospatial Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903ce65",
   "metadata": {},
   "source": [
    "First, we set up the connection to the platform backend.  To do this we need the base url for the studio UI and an API key.\n",
    "\n",
    "To get an API Key:\n",
    "1. Go to the Geospatial Studio UI page and navigate to the Manage your API keys link.\n",
    "2.  This should pop-up a window where you can generate, access and delete your api keys. NB: every user is limited to a maximum of two activate api keys at any one time.\n",
    "\n",
    "Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:\n",
    "\n",
    "```bash\n",
    "echo \"GEOSTUDIO_API_KEY=<paste_api_key_here>\" > .geostudio_config_file\n",
    "echo \"BASE_STUDIO_UI_URL=<paste_ui_base_url_here>\" >> .geostudio_config_file\n",
    "```\n",
    "\n",
    "Copy and paste the file path to this credentials file in call below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06611a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 3. Initialize clients using the key?\n",
    "#############################################################\n",
    "geostudio_client = Client(geostudio_config_file=\".geostudio_config_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e52cb",
   "metadata": {},
   "source": [
    "## Preparing and Onboarding Data Required for FineTuning\n",
    "\n",
    "In order to onboard your dataset to the Geospatial Studio, you need to have a direct download URL pointing to a zip file of the dataset.\n",
    "Review the notebooks on data `../dataset-onboarding` to onboard new datasets into the studio.\n",
    "\n",
    "If you already have some onboarded datasets, look them up and select the one you need for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e186abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "geostudio_client.api_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86831257",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = geostudio_client.list_datasets(output=\"df\")\n",
    "display(datasets[['id','dataset_name', 'purpose', 'status','size','description', 'created_by']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b67bd6d",
   "metadata": {},
   "source": [
    "## Submitting the tune\n",
    "\n",
    "Once the data is onboarded and you have a valid terratorch-iterate config yaml, you are ready to kick off your hpo tuning task. In order to run a fine-tuning task, you need to select the following items:\n",
    "\n",
    "- `tune_metadata`: Identifying info about your tune such as:\n",
    "    - `name`: A name to identify your tune\n",
    "    - `description`: Some detailed description about your experiment.\n",
    "    - `dataset_id`: A dataset id for a dataset that should have been onboarded in the Studio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f413c48",
   "metadata": {},
   "source": [
    "Prepare the fine-tuning payload and submit your tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunes = geostudio_client.list_tunes(output=\"df\")\n",
    "# tunes\n",
    "display(tunes[['id','active', 'created_by', 'name', 'description', 'dataset_id', 'status', 'metrics']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune = geostudio_client.submit_hpo_tune(\n",
    "    data={\n",
    "        \"tune_metadata\": {\n",
    "            \"name\": \"fire-scars-hpo-tune-016\",\n",
    "            \"description\": \"Fine-tuned TerraTorch model for fire scar detection\",\n",
    "            \"dataset_id\": \"geodata-gdctf3vb3znbbtgptqvuku\",\n",
    "        },\n",
    "        \"config_file\": \"../sample_files/burnscars-iterate-hpo.yaml\"\n",
    "    }\n",
    ")\n",
    "display(tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d216352b",
   "metadata": {},
   "source": [
    "## Fetch tune results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43429a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_resp = geostudio_client.get_tune(tune[\"tune_id\"])\n",
    "tune_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea072857",
   "metadata": {},
   "source": [
    "## MLFlow Experiment Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "mlflow_tracking_uri = \"<studio-mlflow-tracking-uri>\"\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "experiment_name = \"geotune-qokd9fqyuhxbgyyiuurpxu\"\n",
    "client = mlflow.tracking.MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e207ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    experiment = client.get_experiment_by_name(experiment_name)\n",
    "    if not experiment:\n",
    "        raise ValueError(f\"Experiment '{experiment_name}' not found.\")\n",
    "    experiment_id = experiment.experiment_id\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = client.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "if not runs:\n",
    "    print(\"No runs found for this experiment.\")\n",
    "    exit()\n",
    "\n",
    "# Create a DataFrame from the runs for easier data manipulation\n",
    "def create_runs_dataframe(runs):\n",
    "    run_data = []\n",
    "    for run in runs:\n",
    "        # Extract desired metrics and parameters\n",
    "        metrics = run.data.metrics\n",
    "        params = run.data.params\n",
    "        \n",
    "        # You may want to flatten the run data into a single dictionary\n",
    "        row = {**metrics, **params, 'run_id': run.info.run_id}\n",
    "        run_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(run_data)\n",
    "\n",
    "# Create the dataframe and fill any missing values with NaN\n",
    "runs_df = create_runs_dataframe(runs)\n",
    "runs_df = runs_df.fillna(value=pd.NA)\n",
    "\n",
    "# Print a summary of the runs to check the data\n",
    "display(\"Summary of MLflow Runs:\")\n",
    "display(runs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35816309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "metrics_to_plot = [\"train/loss\", \"epoch\"]\n",
    "step_interval = 30  # only keep points every 100 steps\n",
    "\n",
    "# run = client.get_run(runs[0].info.run_id)\n",
    "# metrics_keys = run.data.metrics.keys()\n",
    "# metrics_to_plot = [metric for metric in metrics_keys if not metric.startswith(\"System\")]\n",
    "\n",
    "for metric_name in metrics_to_plot:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"{metric_name.replace('_', ' ').title()} Over Steps\", fontsize=10)\n",
    "    plt.xlabel(\"Step\", fontsize=8)\n",
    "    plt.ylabel(metric_name.replace('_', ' ').title(), fontsize=8)\n",
    "    plt.grid(True)\n",
    "\n",
    "    for run in runs:\n",
    "        metric_history = client.get_metric_history(run.info.run_id, metric_name)\n",
    "        if metric_history:\n",
    "            steps = [m.step for m in metric_history]\n",
    "            values = [m.value for m in metric_history]\n",
    "\n",
    "            # Downsample: group by step_interval and take average\n",
    "            smoothed_steps = []\n",
    "            smoothed_values = []\n",
    "\n",
    "            for i in range(0, len(steps), step_interval):\n",
    "                chunk_steps = steps[i:i+step_interval]\n",
    "                chunk_values = values[i:i+step_interval]\n",
    "\n",
    "                smoothed_steps.append(np.mean(chunk_steps))\n",
    "                smoothed_values.append(np.mean(chunk_values))\n",
    "\n",
    "            run_params = run.data.params\n",
    "            legend_label = f\"Run {run.info.run_id[:8]}\"\n",
    "            if \"lr\" in run_params:\n",
    "                legend_label += f\" (lr={float(run_params['lr']):.18f})\"\n",
    "            if \"batch_size\" in run_params:\n",
    "                legend_label += f\" (bs={run_params['batch_size']})\"\n",
    "            # print(metric_name, run_params)\n",
    "\n",
    "            plt.plot(smoothed_steps, smoothed_values, label=legend_label)\n",
    "        else:\n",
    "            print(f\"No history for metric '{metric_name}' in run {run.info.run_id}\")\n",
    "\n",
    "    # plt.legend(loc=\"lower center\", bbox_to_anchor=(0.5, -0.5), ncol=2)\n",
    "    plt.legend(\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, -0.20),\n",
    "        ncol=1,  # number of columns (adjust based on #runs)\n",
    "        frameon=False,\n",
    "        fontsize=8,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95788eed",
   "metadata": {},
   "source": [
    "# DONE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
