{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e650e6f4-f890-49c6-904b-cd2644aecb8f",
   "metadata": {},
   "source": [
    "# 001-Introduction-to-Finetuning\n",
    "\n",
    "Download [001-Introduction-to-Finetuning.ipynb](001-Introduction-to-Finetuning.ipynb) notebook and try it out\n",
    "\n",
    "## Introduction\n",
    "This notebook is intended to be an introduction to using the python SDK to fine-tune a new model from a geospatial foundation model backbone using the Geospatial Studio.\n",
    "\n",
    "For more information about the Geospatial Studio see the docs page: [Geospatial Studio Docs](https://terrastackai.github.io/geospatial-studio)\n",
    "\n",
    "For more information about the Geospatial Studio SDK and all the functions available through it, see the SDK docs page: [Geospatial Studio SDK Docs](https://terrastackai.github.io/geospatial-studio-toolkit)\n",
    "\n",
    "## Prerequisites\n",
    "1. Access to a deploy instance of the Geospatial Studio.\n",
    "1. Ability to run and edit a copy of this notebook.\n",
    "   \n",
    "# Install SDK:\n",
    "\n",
    "1. Prepare a python 3.9+ environment, however you normally do that (e.g. conda, pyenv, poetry, etc.) and activate this new environment.\n",
    "\n",
    "1. Install Jupyter into that environment: `python -m pip install --upgrade pip` then `pip install notebook`\n",
    "\n",
    "1. Install the SDK with: `python -m pip install geostudio`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62527a36",
   "metadata": {},
   "source": [
    "### Install notebook dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f895a6c-100a-4a7a-820a-2ad4a7e76bcc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first import the required packages\n",
    "import json\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import wget\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "import getpass # For use in Colab as well\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "from geostudio import Client\n",
    "from geostudio import gswidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2492ef9-9c20-433d-9f4e-81bdf462be51",
   "metadata": {},
   "source": [
    "## Connecting to the platform\n",
    "First, we set up the connection to the platform backend.  To do this we need the base url for the studio UI and an API key.\n",
    "\n",
    "To get an API Key:\n",
    "1. Go to the Geospatial Studio UI page and navigate to the Manage your API keys link.\n",
    "2.  This should pop-up a window where you can generate, access and delete your api keys. NB: every user is limited to a maximum of two activate api keys at any one time.\n",
    "\n",
    "Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:\n",
    "\n",
    "```bash\n",
    "echo \"GEOSTUDIO_API_KEY=<paste_api_key_here>\" > .geostudio_config_file\n",
    "echo \"BASE_STUDIO_UI_URL=<paste_ui_base_url_here>\" >> .geostudio_config_file\n",
    "```\n",
    "\n",
    "Copy and paste the file path to this credentials file in call below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cbe638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# Initialize Geostudio client using a geostudio config file\n",
    "#############################################################\n",
    "gfm_client = Client(geostudio_config_file=\".geostudio_config_file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c60b5-aacc-428e-9444-684c06dd036f",
   "metadata": {},
   "source": [
    "## Setting up a fine-tuning task\n",
    "\n",
    "Now we are all set to prepare our fine-tuning task.  *This assumes that the tuning dataset to be used is already present in the platform (if it is not, please see the dataset factory examples and return here once the dataset is onboarded).*\n",
    "\n",
    "In order to run a fine-tuning task, you need to select the following items:\n",
    "\n",
    "* **tuning task type** - what type of learning task are you attempting?  segmentation, regression etc\n",
    "* **fine-tuning dataset** - what dataset will you use to train the model for your particular application?\n",
    "* **base foundation model** - which geospatial foundation model will you use as the starting point for your tuning task?\n",
    "\n",
    "Below we walk you through how to use the Geospatial Studio SDK to see what options are available in the platform for each of these, then once you have made your selection, how we configure our task and submit it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b967c193-5774-44f9-968b-fc8e0a2d4c51",
   "metadata": {},
   "source": [
    "### Tuning task selection\n",
    "\n",
    "The tuning task tells the model what type of task it is (segmentation, regression etc), and exposes a range of optional hyperparameters which the user can set.  These all have reasonable defaults, but it gives uses the possibility to configure the model training how they wish.  Below, we will check what task templates are available to us, and then update some parameters.\n",
    "\n",
    "Advanced users can create and upload new task templates to the platform, and instructions are found in the relevant notebook and documentation.  The templates are for Terratorch (the backend tuning library), and more details of Terratroch and configuration options can be found here: https://terrastackai.github.io/terratorch/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80f8de-1fc6-4747-bc17-59e0c5a82559",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = gfm_client.list_tune_templates(output=\"df\")\n",
    "display(tasks[['name','description', 'id','created_by','updated_at']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba21dea-eac0-4b7d-8ca6-4d8eec4c89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a task from the options above.  Copy and paste the id into the variable, task_id, below.\n",
    "task_id = 'e4791b2c-bb17-4a5e-9f05-1be5411a4fa6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfcb2e8-9685-4d67-b33b-a4b8c6195654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we can view the full meta-data and details of the selected task\n",
    "task_meta = gfm_client.get_task(task_id=task_id)\n",
    "task_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335165b-919b-4cc8-a153-5ce52fa44875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T23:07:38.184631Z",
     "iopub.status.busy": "2024-09-06T23:07:38.184247Z",
     "iopub.status.idle": "2024-09-06T23:07:38.812283Z",
     "shell.execute_reply": "2024-09-06T23:07:38.811534Z",
     "shell.execute_reply.started": "2024-09-06T23:07:38.184606Z"
    }
   },
   "source": [
    "If you are happy with your choice, you can decide which (if any) hyperparameters you want to set (otherwise defaults will be used).\n",
    "\n",
    "Here we can see the available parameters and their associated defaults.  To update a parameter you can just set values in the dictionary (as shown below for `max_epochs`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d7e12e-0166-4cec-a47a-01dd80654851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_params = gfm_client.get_task_param_defaults(task_id)\n",
    "task_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ac182-4e0a-41f0-a32d-9b7ceebf3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params['runner']['max_epochs'] = 5\n",
    "task_params['optimizer']['type'] = 'AdamW'\n",
    "task_params['data']['batch_size'] = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63acd50b-11b2-499c-968f-315b54078639",
   "metadata": {},
   "source": [
    "### Dataset selection\n",
    "Now we have chosen the type of tuning task we wish to carry out, we need to decide on the tuning dataset.  There are two options available:\n",
    "* use a dataset already registered in the Studio\n",
    "* create a new dataset by uploading or curating a dataset\n",
    "\n",
    "In this notebook, we use a already existing dataset.  For a walkthrough of how to create new datasets see the relevant example and documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c5d97-62a1-483e-8641-ef09f7862d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = gfm_client.list_datasets(output='df')\n",
    "display(datasets[['dataset_name','description','id','status','created_by','updated_at']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0870c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "\n",
    "gfm_client.get_dataset(\"geodata-ferctkm2brxpkbqz9apa6z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01c702-0cec-49fe-bdf2-c13203928cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste the id of the dataset into the variable below\n",
    "dataset_id = 'geodata-ferctkm2brxpkbqz9apa6z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2faf6-2740-43e1-9906-3d4f58f106aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T09:58:38.139929Z",
     "iopub.status.busy": "2024-09-09T09:58:38.139263Z",
     "iopub.status.idle": "2024-09-09T09:58:38.156718Z",
     "shell.execute_reply": "2024-09-09T09:58:38.155931Z",
     "shell.execute_reply.started": "2024-09-09T09:58:38.139906Z"
    }
   },
   "source": [
    "### Foundation model selection\n",
    "The final selection we need to make before kicking off our tuning task is to select the backbone/base model we wish to start from.  Again, we can first view the available options in the studio, then make our selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a955e-9f10-48ca-8dbb-e3c7965ca311",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = gfm_client.list_base_models(output='df')\n",
    "display(base[['name','description','id','updated_at']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda5e76-5394-4225-842e-ad23e7f28cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste the id of the base model you selected into the variable below\n",
    "base_model_id = '71c82e28-c0ee-44b8-aba9-7facd94e08ec'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf421e-ae44-41b0-96be-11445ba5f9a8",
   "metadata": {},
   "source": [
    "## Submitting the tuning task\n",
    "\n",
    "Now we put that information into the payload below and send the request to the cluster.  In this case we will use the asynchronous submission (avoids issues with timeouts for large areas and time windows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5ab3a-cf77-4b07-abb2-0c8120781617",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_payload = {\n",
    "  \"name\": \"test-fine-tuning\",\n",
    "  \"description\": \"testing\",\n",
    "  \"dataset_id\": dataset_id,\n",
    "  \"base_model_id\": base_model_id,\n",
    "  \"tune_template_id\": task_id,\n",
    "  # \"model_parameters\": task_params # uncomment this line if you customised task_params in the cells above otherwise, defaults will be used\n",
    "}\n",
    "\n",
    "print(json.dumps(tune_payload, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f119af6-4310-4a79-add7-03b1bba30dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted = gfm_client.submit_tune(\n",
    "        data = tune_payload,\n",
    "        output = 'json'\n",
    ")\n",
    "\n",
    "print(submitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dcaad0-b1e5-4ea7-8c6a-502b012c82ec",
   "metadata": {},
   "source": [
    "## Monitor tuning status and progress\n",
    "After submitting the request, we can poll the inference service to check the progress and get the output details once its complete (this could take a few minutes depending on the request size and the current service load)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you wish to you can keep polling the tuning task to monitor its progress.\n",
    "r = gfm_client.poll_finetuning_until_finished(tune_id=submitted['tune_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7d68c-9a71-4a49-a2b4-83588e3a002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_id = submitted[\"tune_id\"]\n",
    "\n",
    "tune_info = gfm_client.get_tune(tune_id, output='json')\n",
    "tune_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2757a465-47df-489e-8921-707100c6ca0f",
   "metadata": {},
   "source": [
    "## Check the training metrics from the tune\n",
    "The metrics from the model training are logged in a backend MLflow service and can be accessed through the APIs, SDK and UI.\n",
    "\n",
    "You can get access the training metrics either in full as a json using:\n",
    "```python\n",
    "gfm_client.get_tune_metrics(tune_id)\n",
    "```\n",
    "\n",
    "Or directly to a pandas dataframe for ready analysis using the function below `get_tune_metrics_df`.  In addition, the SDK provides functionality to quickly plot some top level metrics for training and validation.\n",
    "\n",
    "In addition to that, you can simply plot the training and validation loss and multi-class accuracy using the `plot_tune_metrics` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47bfc9a-e811-4faa-8191-12b86587706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = gfm_client.get_tune_metrics_df(tune_id)\n",
    "mdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74969432",
   "metadata": {},
   "outputs": [],
   "source": [
    "gswidgets.plot_tune_metrics(client=gfm_client, tune_id=tune_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa7ff7-dd53-437d-ac21-4bcf210367e6",
   "metadata": {},
   "source": [
    "## Try out the model for inference\n",
    "Once your model has finished tuning, if you want to run inference as a test you can do by passing either a location (bbox) or a url to a pre-prepared files.  The steps to test the model are:\n",
    "1. Define the inference payload\n",
    "2. Try out the tune temporarily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d690d",
   "metadata": {},
   "source": [
    "### Using an S3 pre-signed link\n",
    "If you have your image locally and would like to pre-sign the image using S3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccbdab8",
   "metadata": {},
   "source": [
    "#### Personal buckets\n",
    "Use the `create_upload_presigned_url` to generate an upload link that you can use to upload the file to the dataset. \n",
    "\n",
    "This function assumes you have your own storage bucket to upload to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d57d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_url = gfm_client.create_upload_presigned_url(\n",
    "    bucket_name=\"bucket_name\", # bucket name\n",
    "    object_key=\"data/train/austin1_sdk_upload.tiff\", # file path to upload in the bucket\n",
    "    endpoint_url=\"https://s3.us-east.cloud-object-storage.appdomain.cloud\", # s3 endpoint url\n",
    "    service_name= \"s3\", # service to use\n",
    "    region_name=\"us-east\", # cloud region \n",
    "    expiration=3600 # expiration\n",
    "    # Add any other args to pass to the s3 client\n",
    ")\n",
    "upload_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push your file to the bucket using the url generated.\n",
    "!curl -X PUT -T **your_file.zip or your_file.tiff or your_file.tif** \"**upload_url**\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd80385",
   "metadata": {},
   "source": [
    "Once the image is uploaded to your s3 bucket, create a download link to use in the inference request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = gfm_client.create_download_presigned_url(\n",
    "    bucket_name=\"geospatial-studio-example-data\", # bucket name\n",
    "    object_key=\"data/train/austin1_sdk_upload.tiff\", # file path to upload in the bucket\n",
    "    endpoint_url=\"https://s3.us-east.cloud-object-storage.appdomain.cloud\", # s3 endpoint url\n",
    "    service_name= \"s3\", # service to use\n",
    "    region_name=\"us-east\", # cloud region \n",
    "    expiration=7200 # expiration\n",
    "    # Add any other args to pass to the s3 client\n",
    "\n",
    ")\n",
    "download_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8b718a",
   "metadata": {},
   "source": [
    "#### Geostudio temporary buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f89461",
   "metadata": {},
   "source": [
    "\n",
    "If you would like to upload to a geostudio temporary bucket, use this function `get_fileshare_links` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8852b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Unique object name to be used in temporary COS for each layer you want to upload\n",
    "object_name = \"austin1_sdk_upload.tiff\"\n",
    "gfm_client.get_fileshare_links(object_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c55096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push your file to the bucket using the url generated.\n",
    "!curl -X PUT -T **your_file.zip or your_file.tiff or your_file.tif** \"**upload_url**\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de7838",
   "metadata": {},
   "source": [
    "#### Submit Inference\n",
    "Now you can create the inference payload using the download link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e91fd8-ce85-4d64-841b-6490804bb6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the inference payload\n",
    "\n",
    "bbox = [-121.837006,39.826468,-121.641312,40.038655]\n",
    "download_url_tiff = download_url\n",
    "\n",
    "# When using a bbox\n",
    "request_payload_with_bbox = {\n",
    "\t\"description\": \"Park Fire 2024 SDK\",\n",
    "\t\"location\": \"Red Bluff, California, United States\",\n",
    "\t\"spatial_domain\": {\n",
    "\t\t\t\"bbox\": [bbox], # When using bboxes\n",
    "\t\t\t\"polygons\": [],\n",
    "\t\t\t\"tiles\": [],\n",
    "\t\t\t\"urls\": []\n",
    "\t},\n",
    "\t\"temporal_domain\": [\n",
    "\t\t\t\"2024-08-12_2024-08-13\"\n",
    "\t]\n",
    "}\n",
    "\n",
    "# When using a presigned link\n",
    "request_payload_with_url = {\n",
    "\t\"description\": \"Park Fire 2024 SDK\",\n",
    "\t\"location\": \"Red Bluff, California, United States\",\n",
    "\t\"spatial_domain\": {\n",
    "\t\t\t\"bbox\": [],\n",
    "\t\t\t\"polygons\": [],\n",
    "\t\t\t\"tiles\": [],\n",
    "\t\t\t\"urls\": [download_url_tiff] # When using url\n",
    "\t},\n",
    "\t\"temporal_domain\": [\n",
    "\t\t\t\"2024-08-12_2024-08-13\"\n",
    "\t]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8707b-eb31-40bc-8b77-45eba095b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now submit the test inference request\n",
    "# Change the request to the correct one when using urls\n",
    "inference_response = gfm_client.try_out_tune(tune_id=tune_id, data=request_payload_with_bbox)\n",
    "inference_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a38df8f-01b2-4f5c-86ff-9cdf53896953",
   "metadata": {},
   "source": [
    "## Downloading the tuned model artefacts\n",
    "If you want to download the model artefacts (e.g. checkpoint and config) in order to run the model locally or elsewhere, you can use the following function to do it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30412e0-3d1c-4e11-88f9-3ca5dcda00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfm_client.download_tune(tune_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
