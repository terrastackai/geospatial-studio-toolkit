{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping flooding using an AI flood model\n",
    "\n",
    "Download [GeospatialStudio-Walkthrough-Flooding.ipynb](GeospatialStudio-Walkthrough-Flooding.ipynb) notebook and try it out\n",
    "\n",
    "Assume you are interested in mapping flooding, traditionally you might have either relied on on-the-ground mapping, or possibly for manual analysis of remote-sensing imagery (i.e. satellite or UAV). In order to scale up these efforts and operationalise, we need a way to automate the extraction of flood extent from satellite imagery. This is where we turn to AI models. \n",
    "\n",
    "<img src=\"./assets/s2-flood-examples.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "The model you will use in this walkthrough was fine-tuned from the Prithvi foundation model and using the Sen1Floods11 dataset [link here](hhttps://geospatial-studio-example-data.s3.us-east.cloud-object-storage.appdomain.cloud/sen2_flood_dst.zip).\n",
    "\n",
    "In this walkthrough we will assume that a model doesn't exist yet and we want to train a new model.  We will then show how to drive the model to map impact.\n",
    "\n",
    "We will walk through the following steps as part of this walkthrough:\n",
    "1. Upload and onboarding of data\n",
    "2. Configuring and submitting a tuning task\n",
    "3. Monitoring model training\n",
    "4. Testing and validation of the outputs\n",
    "\n",
    "\n",
    "## Pre-requisites\n",
    "You will require access to an instance of the Geospatial Studio.  For more information about the Geospatial Studio see the docs page: [Geospatial Studio Docs](https://terrastackai.github.io/geospatial-studio)\n",
    "\n",
    "For more information about the Geospatial Studio SDK and all the functions available through it, see the SDK docs page: [Geospatial Studio SDK Docs](https://terrastackai.github.io/geospatial-studio-toolkit)\n",
    "\n",
    "This walkthrough also requires you to have a direct download URL pointing to a zip file of the dataset you wish to use. We provide a sample dataset url (zip file) below to go through this notebook. If you have the dataset locally, you can find instructions on how to use the SDK to temporarily upload it to the cloud and create a download url link in the steps that follow.\n",
    "\n",
    "### Get the training data\n",
    "To train the AI model, we will need some training data which contains the input data and the labels (aka ground truth flooding extent).  To train our model we will use the following dataset: https://geospatial-studio-example-data.s3.us-east.cloud-object-storage.appdomain.cloud/sen2_flood_dst.zip\n",
    "\n",
    "Download and unzip the above archive and if you wish you can explore the data with QGIS (or any similar tool).\n",
    "\n",
    "*NB: If you already have the data in online you can skip this step.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "from geostudio import Client\n",
    "from geostudio import gswidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the platform\n",
    "First, we set up the connection to the platform backend.  To do this we need the base url for the studio UI and an API key.\n",
    "\n",
    "To get an API Key:\n",
    "1. Go to the Geospatial Studio UI page and navigate to the Manage your API keys link.\n",
    "2.  This should pop-up a window where you can generate, access and delete your api keys. NB: every user is limited to a maximum of two activate api keys at any one time.\n",
    "\n",
    "Store the API key and geostudio ui base url in a credentials file locally, for example in /User/bob/.geostudio_config_file. You can do this by:\n",
    "\n",
    "```bash\n",
    "echo \"GEOSTUDIO_API_KEY=<paste_api_key_here>\" > .geostudio_config_file\n",
    "echo \"BASE_STUDIO_UI_URL=<paste_ui_base_url_here>\" >> .geostudio_config_file\n",
    "```\n",
    "\n",
    "Copy and paste the file path to this credentials file in call below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# Initialize Geostudio client using a geostudio config file\n",
    "#############################################################\n",
    "gfm_client = Client(geostudio_config_file=\".geostudio_config_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data onboarding\n",
    "\n",
    "In order to onboard your dataset to the Geospatial Studio, you need to have a direct download URL pointing to a zip file of the dataset. You can use [this dataset url](https://geospatial-studio-example-data.s3.us-east.cloud-object-storage.appdomain.cloud/sen1floods11-studio.zip) as an example to go through this notebook.\n",
    "\n",
    "If you have the dataset locally, you can use Box, OneDrive or any other cloud storage you are used to, but in addition, to make this easier for you, there is a function which will upload your data to a temporary location in the cloud (with in Studio object storage) and provide you with a url which can be used to pass to the onboarding process.  *NB: the same upload function can be useful for pushing files for inferecnce or to processing pipelines.*\n",
    "\n",
    "If needed you can package a set of files for upload, you can use a command like:\n",
    "```bash\n",
    "zip -j flooding-dataset-upload.zip /Users/beldinemoturi/Downloads/flooding-dataset-upload/*\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) If you wish to upload the data archive through the studio, you can uncomment and use this function. \n",
    "uploaded_links = gfm_client.upload_file('/Users/beldinemoturi/Downloads/flooding_dataset.zip')\n",
    "uploaded_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Onboard the dataset to the dataset factory\n",
    "\n",
    "Now we use the SDK to provide the information about the dataset, including name, suffixes etc.  A more detailed description of the dataset details is provided in the UI walkthrough.  Here the SDK will do some basic sanity checks, and will (if possible) check that you have matching data and label pairs, and check that you have specified the correct number of bands.  This creates dictionary with the required details, which you can then submit to the platform using the step below.\n",
    "\n",
    "Note:\n",
    "\n",
    "* Change the value of the `dataset_url`variable below to the url of your zip file or the `download_url` link you got from using the SDK upload_file function above\n",
    "* Change the values of `training_data_suffix` and `label_suffix` to the suffixes of your training and label data files respectively if using a different dataset (aside from the one provided)\n",
    "* Change the `label_categories`, `custom_bands` and descriptions to those that match your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_links[\"download_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the dict below to suit your dataset details.\n",
    "\n",
    "dataset_dict = {\n",
    "    \"dataset_name\": \"Sentinel Flood Multimodal\",\n",
    "    \"data_sources\": [\n",
    "        {\n",
    "            \"bands\": [\n",
    "                {\"index\":\"0\", \"band_name\": \"Coastal_aerosol\", \"RGB_band\": \"R\", \"description\": \"\"},\n",
    "                {\"index\":\"1\", \"band_name\": \"Blue\", \"RGB_band\": \"G\", \"description\": \"\"},\n",
    "                {\"index\":\"2\", \"band_name\": \"Green\", \"RGB_band\": \"B\", \"description\": \"\"},\n",
    "                {\"index\":\"3\", \"band_name\": \"Red\", \"description\": \"\"},\n",
    "                {\"index\":\"4\", \"band_name\": \"05_-_Vegetation_Red_Edge\", \"description\": \"\"},\n",
    "                {\"index\":\"5\", \"band_name\": \"06_-_Vegetation_Red_Edge\", \"description\": \"\"},\n",
    "                {\"index\":\"6\", \"band_name\": \"07_-_Vegetation_Red_Edge\", \"description\": \"\"},\n",
    "                {\"index\":\"7\", \"band_name\": \"08_-_NIR\", \"description\": \"\"},\n",
    "                {\"index\":\"8\", \"band_name\": \"08A_-_Vegetation_Red_Edge\", \"description\": \"\"},\n",
    "                {\"index\":\"9\", \"band_name\": \"09_-_Water_vapour\", \"description\": \"\"},\n",
    "                {\"index\":\"10\", \"band_name\": \"11_-_SWIR\", \"description\": \"\"},\n",
    "                {\"index\":\"11\", \"band_name\": \"12_-_SWIR\", \"description\": \"\"},\n",
    "                {\"index\":\"12\", \"band_name\": \"Cloud_Probability\", \"description\": \"\"}\n",
    "            ],\n",
    "            \"connector\": \"sentinelhub\",\n",
    "            \"collection\": \"s2_l2a\",\n",
    "            \"modality_tag\": \"S2L1C\",\n",
    "            \"file_suffix\": \"_S2Hand.tif\",\n",
    "            \"scaling_factor\": [1, 1, 1, 1, 1, 1]\n",
    "        },\n",
    "        {\n",
    "            \"bands\": [\n",
    "                {\"index\":\"0\", \"band_name\": \"VV (Gray)\", \"description\": \"\"},\n",
    "                {\"index\":\"1\", \"band_name\": \"VH\", \"description\": \"\"}\n",
    "            ],\n",
    "            \"connector\": \"sentinelhub\",\n",
    "            \"collection\": \"s1_grd\",\n",
    "            \"modality_tag\": \"S1GRD\",\n",
    "            \"align_dates\": \"true\",\n",
    "            \"file_suffix\": \"_S1Hand.tif\",\n",
    "            \"scaling_factor\": [1, 1]\n",
    "        }\n",
    "    ],\n",
    "    \"label_categories\": [\n",
    "        {\"id\": \"0\", \"name\": \"No Floods\", \"description\": \"Flooding assets\"},\n",
    "        {\"id\": \"1\", \"name\": \"Floods\", \"description\": \"Flooding assets\"}\n",
    "    ],\n",
    "    \"dataset_url\": uploaded_links[\"download_url\"],\n",
    "    \"description\": \"Flood data from places\",\n",
    "    \"label_suffix\": \"_LabelHand.tif\",\n",
    "    \"purpose\": \"Segmentation\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-scan the dataset\n",
    "Pre-scan the dataset to check the accessibility of the dataset URL, ensure corresponding data and label files are present, and extract bands and their descriptions from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional]\n",
    "\n",
    "gfm_client.pre_scan_dataset({\n",
    "  \"dataset_url\": uploaded_links[\"download_url\"],\n",
    "  \"label_suffix\": \"_LabelHand.tif\",\n",
    "  \"training_data_suffixes\": \n",
    "    [\"_S2Hand.tif\", \"_S1Hand.tif\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start onboarding process\n",
    "\n",
    "onboard_response = gfm_client.onboard_dataset(data=dataset_dict)\n",
    "display(json.dumps(onboard_response, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning submission\n",
    "\n",
    "Once the data is onboarded, you are ready to setup your tuning task.  In order to run a fine-tuning task, you need to select the following items:\n",
    "* **tuning task type/config template** - what type of learning task are you attempting?  segmentation, regression etc\n",
    "* **fine-tuning dataset** - what dataset will you use to train the model for your particular application?\n",
    "* **base foundation model** - which geospatial foundation model will you use as the starting point for your tuning task?\n",
    "\n",
    "Below we walk you through how to use the Geospatial Studio SDK to see what options are available in the platform for each of these, then once you have made your selection, how we configure our task and submit it.\n",
    "\n",
    "### Tuning task\n",
    "The tuning task tells the model what type of task it is (segmentation, regression etc), and exposes a range of optional hyperparameters which the user can set.  These all have reasonable defaults, but it gives uses the possibility to configure the model training how they wish.  Below, we will check what task templates are available to us, and then update some parameters.\n",
    "\n",
    "Advanced users can create and upload new task templates to the platform, and instructions are found in the relevant notebook and documentation.  The templates are for Terratorch (the backend tuning library), and more details of Terratroch and configuration options can be found here: https://ibm.github.io/terratorch/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list tasks available\n",
    "\n",
    "tasks = gfm_client.list_tune_templates(output=\"df\")\n",
    "display(tasks[['name','description', 'id','created_by','updated_at']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a task from the options above.  Copy and paste the id into the variable, task_id, below. For this example, it is a segmentation task since we are classifying flooded and non-flooded areas\n",
    "task_id = '48c878d8-3b05-4ca5-bd89-89400c8790eb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the full meta-data and details of the selected task\n",
    "task_meta = gfm_client.get_task(task_id)\n",
    "task_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are happy with your choice, you can decide which (if any) hyperparameters you want to set (otherwise defaults will be used).\n",
    "\n",
    "Here we can see the available parameters and their associated defaults.  To update a parameter you can just set values in the dictionary (as shown below for `max_epochs`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the default values for parameters\n",
    "\n",
    "task_params = gfm_client.get_task_param_defaults(task_id)\n",
    "task_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the parameters you want \n",
    "\n",
    "task_params['runner']['max_epochs'] = '3'\n",
    "# task_params['optimizer']['type'] = 'AdamW'\n",
    "# task_params['data']['batch_size'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base foundation model\n",
    "The base model is the foundation model (encoder) which has been pre-trained and has the basic understanding of the data.  More information can currently be found on the different models we have open-sourced [on hugging face.](https://huggingface.co/ibm-nasa-geospatial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list foundation models available\n",
    "\n",
    "base = gfm_client.list_base_models(output='df')\n",
    "display(base[['name','description','id','updated_at']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select base foundation model\n",
    "base_model_id = '55e638d9-7a7c-4e8b-bda2-035b172922af'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting the tune\n",
    "Now we pull these choices together into a payload which we then submit to the platform.  This will then deploy the job in the backend and we will see below how we can monitor it.  First, we populate the payload so we can check it, then we simply submit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tune payload\n",
    "\n",
    "dataset_id = \"geodata-zvgkj5qqwxbhzzz25qbuxz\" # the dataset_id of the dataset you onboarded above\n",
    "\n",
    "tune_payload = {\n",
    "  \"name\": \"test-fine-tuning-multimodal\",\n",
    "  \"description\": \"Segmentation\",\n",
    "  \"dataset_id\": dataset_id,\n",
    "  \"base_model_id\": base_model_id,\n",
    "  \"tune_template_id\": task_id,\n",
    "}\n",
    "\n",
    "print(json.dumps(tune_payload, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit tune\n",
    "\n",
    "submitted = gfm_client.submit_tune(\n",
    "        data = tune_payload,\n",
    "        output = 'json'\n",
    ")\n",
    "\n",
    "print(submitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring training\n",
    "Once the tune has been submitted you can check its status and monitor tuning progress through the SDK.  You can also access the training metrics and images in MLflow.  The `get_tune` function will give you the meta-data of the tune, including the status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata about the submitted tune\n",
    "\n",
    "tune_id = submitted.get(\"tune_id\")\n",
    "\n",
    "tune_info = gfm_client.get_tune(tune_id, output='json')\n",
    "tune_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has started training, you will also be able to access the training metrics.  The `get_tune_metrics_df` function returns a dataframe containing the up-to-date training metrics, which you are free to explore and analyse.  In addition to that, you can simply plot the training and validation loss and multi-class accuracy using the `plot_tune_metrics` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training metrics\n",
    "\n",
    "mdf = gfm_client.get_mlflow_metrics(\"geotune-iskjtu463hxou9mukjct9o\")\n",
    "mdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some basic training metrics\n",
    "\n",
    "# gfm_client.plot_tune_metrics(tune_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your model is finished training and you are happy with the metrics (and images in MLflow), you can run some inference in test mode through the inference service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your model\n",
    "\n",
    "To do a test deployment and inference with the model, we need to register the model with the inference service.  To do this you need to **select a model style**(describing the visulisation style of the model output), and **define the data required to feed the model** (in the example here it is using Sentinel Hub).  For the data specification, you need to **define the data collection and bands** from sentinelhub (using the collection and band names for SH).  In addition, if the data to be fed in is returned from SH with a **scale factor** that needs to be added here too.  Data collection data for HLS are found here: https://docs.sentinel-hub.com/api/latest/data/hls/\n",
    "\n",
    "**Example flood events**\n",
    "\n",
    "|  Location            |  Date    | Bounding box      | Link |\n",
    "| :---------------------: | :--------: | :-----------------: | :-----------------: |\n",
    "|  Maiduguri, Nigeria | 2024-09-12 | [13.146418, 11.799808, 13.215874, 11.871586] | https://www.aljazeera.com/features/2024/9/19/a-disaster-homes-lost-relatives-missing-in-floods-in-northeast-nigeria |\n",
    "|  Porto Alegre, Brazil  | 2024-05-06 | [-51.33225, -30.08903, -51.19011, -29.97489] | https://www.reuters.com/pictures/stunning-images-show-extent-flooding-southern-brazil-2024-05-07/ |\n",
    "|  Ahero, Kenya  | 2024-05-05  | [34.838652, -0.231379, 34.977847, -0.131439] | |\n",
    "|  Gloucester, UK  | 2024-01-09 | [-2.311807, 51.855573, -2.17892, 51.952735]  | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out the model for inference\n",
    "Once your model has finished tuning, if you want to run inference as a test you can do by passing either a location (bbox) or a url to a pre-prepared files.  The steps to test the model are:\n",
    "1. Define the inference payload\n",
    "2. Try out the tune temporarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the inference payload\n",
    "\n",
    "bbox = [92.40665153547121, 26.1051042015407,92.92535070071905,26.498933088370826]\n",
    "\n",
    "request_payload = {\n",
    "\t\"description\": \"Jarani, Nagaon, Nagaon, Assam, India\",\n",
    "\t\"location\": \"Jarani, Nagaon, Nagaon, Assam, India\",\n",
    "\t\"spatial_domain\": {\n",
    "\t\t\t\"bbox\": [bbox],\n",
    "\t\t\t\"polygons\": [],\n",
    "\t\t\t\"tiles\": [],\n",
    "\t\t\t\"urls\": []\n",
    "\t},\n",
    "\t\"temporal_domain\": [\n",
    "\t\t\t\"2024-07-25_2024-07-27\"\n",
    "\t]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have registered the model, you can now run it with a test inference.  As with the main inference service, this is done by either supplying a bounding box (`bbox`), time range (`start_date`, `end_date`) and the `model_id`.  You can then monitor it and visualise the outputs either through the SDK, or in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now submit the test inference request\n",
    "inference_response = gfm_client.try_out_tune(tune_id=tune_id, data=request_payload)\n",
    "inference_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring your inference task\n",
    "\n",
    "Once submitted you can check on progress using the following function which will return all the metadata about the inference task, including the status.  You can optionally use the `poll_until_finished` to watch the status until it completes.  For a test inference it can take 5-10 minutes, depending on the size of the data query, the size of the model etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata about the inference task\n",
    "\n",
    "gfm_client.get_inference(inference_response['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking model outputs\n",
    "You can check out the results visually in the Studio UI, or with the quick widget below.  You can alternatively use the SDK to download selected files for further analysis [see documentation](https://github.com/terrastackai/geospatial-studio-toolkit/examples/inference/001-Introduction-to-Inferencing.ipynb).\n",
    "\n",
    "*Note:*\n",
    "\n",
    "*For now, you can check out the inference output and results visually in the Studio UI through the history tab of the inference page.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view inference results\n",
    "\n",
    "# gswidgets.inferenceViewer(gfm_client, inference_response['id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
